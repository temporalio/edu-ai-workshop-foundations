{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iy6hs6HQZY-Z"
   },
   "source": [
    "# Adding Durability with Temporal\n",
    "\n",
    "You've just built a research application that generates PDF reports. It works perfectly—until it doesn't.\n",
    "\n",
    "Imagine this: Your application conducts expensive research through an LLM call (costing time and money), but then **crashes** during PDF generation due to a network ourFW. When you restart, everything is lost. You're back to the beginning, paying for the same LLM call again, making your users wait, and burning through your API budget.\n",
    "\n",
    "As these workflows grow more complex—chaining multiple LLM calls, database queries, external APIs—the problem compounds. Every failure means starting over completely.\n",
    "\n",
    "In this section, we'll solve this problem by making your application durable. You'll learn how to build Gen-AI applications that survive failures, recover automatically, and never lose progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHV0ZA9gaV7n"
   },
   "source": [
    "## Setup Your Notebook\n",
    "\n",
    "Run the following code blocks to install various packages and tools necessary to run this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ZqFOSwtoJv1L",
    "outputId": "d6b8a8a8-5586-45a6-c412-0be392e478a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/azhou/Desktop/edu-ai-workshop-mcp/env/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# We'll first install the necessary packages for this workshop.\n",
    "\n",
    "%pip install --quiet temporalio litellm reportlab python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Znl4WVrt6T0d"
   },
   "source": [
    "## Create a `.env` File\n",
    "\n",
    "Next you'll create a `.env` file to store your API keys.\n",
    "In the file browser on the left, create a new file and name it `.env`.\n",
    "Note that this file doesn't persist across notebooks or sesions.\n",
    "\n",
    "**Note**: It may disappear as soon as you create it. This is because Google Collab hides hidden files (files that start with a `.`) by default.\n",
    "To make this file appear, click the icon that is a crossed out eye and hidden files will appear.\n",
    "\n",
    "Then double click on the `.env` file and add the following line with your API key.\n",
    "\n",
    "```\n",
    "LLM_API_KEY = YOUR_API_KEY\n",
    "LLM_MODEL = \"openai/gpt-4o\"\n",
    "```\n",
    "\n",
    "By default this notebook uses OpenAI's GPT-4o.\n",
    "If you want to use a different LLM provider, look up the appropriate model name [in their documentation](https://docs.litellm.ai/docs/providers) and change the `LLM_MODEL` field and provide your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvdK0Uw96qof"
   },
   "outputs": [],
   "source": [
    "# Create .env file\n",
    "with open(\".env\", \"w\") as fh:\n",
    "  fh.write(\"LLM_API_KEY = YOUR_API_KEY\\nLLM_MODEL = openai/gpt-4o\")\n",
    "\n",
    "# Now open the file and replace YOUR_API_KEY with your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uPJ0l0su7CWK",
    "outputId": "302f69ce-9bd1-4505-e652-68ebed1b2dbf"
   },
   "outputs": [],
   "source": [
    "# Load environment variables and configure LLM settings\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Get LLM_API_KEY environment variable and print it to make sure that your .env file is properly loaded.\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"openai/gpt-4o\")\n",
    "LLM_API_KEY = os.getenv(\"LLM_API_KEY\", None)\n",
    "print(\"LLM API Key\", LLM_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "TLHoJlR4R8Au"
   },
   "outputs": [],
   "source": [
    "# This allows us to run the Temporal Asyncio event loop within the event loop of Jupyter Notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SBeJ9pGZSVAs",
    "outputId": "c2e729a5-c85e-4675-9ef8-ada734b1c9ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mtemporal:\u001b[0m Downloading Temporal CLI latest\n",
      "\u001b[1mtemporal:\u001b[0m Temporal CLI installed at /Users/azhou/.temporalio/bin/temporal\n",
      "\u001b[1mtemporal:\u001b[0m For convenience, we recommend adding it to your PATH\n",
      "\u001b[1mtemporal:\u001b[0m If using bash, run echo export PATH=\"\\$PATH:/Users/azhou/.temporalio/bin\" >> ~/.bashrc\n"
     ]
    }
   ],
   "source": [
    "# Running this will download the Temporal CLI, which we need for this workshop.\n",
    "\n",
    "!curl -sSf https://temporal.download/cli.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckJJ2NbFgR5y"
   },
   "outputs": [],
   "source": [
    "# Mermaid renderer, run at the beginning to setup rendering of diagrams\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def render_mermaid(graph_definition):\n",
    "    \"\"\"\n",
    "    Renders a Mermaid diagram in Google Colab using mermaid.ink.\n",
    "\n",
    "    Args:\n",
    "        graph_definition (str): The Mermaid diagram code (e.g., \"graph LR; A-->B;\").\n",
    "    \"\"\"\n",
    "    graph_bytes = graph_definition.encode(\"ascii\")\n",
    "    base64_bytes = base64.b64encode(graph_bytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5lA3iJ9X8Na"
   },
   "source": [
    "# Adding Durability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kD53k7xOuL6a"
   },
   "source": [
    "## What Can Go Wrong with Gen-AI Applications?\n",
    "\n",
    "Let's brainstorm the issues you might face when running your research application from Notebook 1 in production.\n",
    "\n",
    "**Think about these categories:**\n",
    "* **Technical failures:** What external services could fail?\n",
    "* **Timing issues:** What if something takes longer than expected?\n",
    "* **Recovery challenges:** If something breaks halfway through, what happens?\n",
    "\n",
    "*Take 2 minutes to discuss with your neighbor, then we'll share answers.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKIuzLjvucLf"
   },
   "source": [
    "## Common Issues with Gen-AI Applications in Production\n",
    "\n",
    "**Common answers we typically hear:**\n",
    "* LLM API timeouts or rate limiting\n",
    "* PDF generation fails due to disk space\n",
    "* Network connectivity issues\n",
    "* Process crashes mid-execution\n",
    "* Restarting burns money"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCBVj2WX7mwN"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQOR9QLGZ_hw"
   },
   "source": [
    "## These Aren't New Problems\n",
    "\n",
    "The challenges you just identified? They're the same problems we've been solving in distributed systems for decades:\n",
    "\n",
    "**Your Research Application in Production Reality:**\n",
    "* **LLM API call** - External service that can timeout, rate limit, or be down.\n",
    "* **PDF generation** - File system operation that can fail due to disk space\n",
    "* **User input/output** - Network operations that can be interrupted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01ZkJPjC7njg"
   },
   "source": [
    "## Gen-AI Applications are Distributed Systems!\n",
    "\n",
    "**This is a distributed system!** Your \"simple\" application is actually:\n",
    "* Multiple network calls to external services\n",
    "* File system operations\n",
    "* State that needs to persist across failures\n",
    "* Coordination between different steps\n",
    "\n",
    "**The challenge:** Traditional distributed systems tools weren't designed for AI workflows. They don't understand expensive LLM calls, context windows, or long-term state management.\n",
    "\n",
    "**The good news:** You can use a platform that guarantees the _reliable execution_ of your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_1jsgtvaLJ1"
   },
   "source": [
    "## Your Report Generation Application Needs Durable Execution\n",
    "\n",
    "Recall your research application from Notebook 1? Here's what happens in production:\n",
    "\n",
    "**Scenario:** User asks for research on \"sustainable energy trends\"\n",
    "1. LLM call succeeds - generates comprehensive research content ($2.50 in API costs)\n",
    "2. PDF generation fails - disk full, permission error, or process crash\n",
    "3. **User has to start over completely** - losing expensive work and time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZ28Xb3gwUAj"
   },
   "source": [
    "## What Developers Actually Want\n",
    "\n",
    "* \"Just fix the disk issue and generate the PDF from the research you already have.\"\n",
    "* \"Don't make me pay for the same LLM call twice!\"\n",
    "* \"Don't lose my work because of a simple file system error!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_cCDze0xOZv"
   },
   "source": [
    "## What Normal Execution Gives Us\n",
    "\n",
    "* Every failure means restarting from scratch\n",
    "* Expensive LLM calls are repeated unnecessarily\n",
    "* User experience becomes frustrating and unreliable\n",
    "* No way to resume from where you left off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyO36uq0xUBF"
   },
   "source": [
    "## What We Need\n",
    "\n",
    "A way to make our AI applications resilient to these failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAlIqew1aQhp"
   },
   "source": [
    "## What Is Durable Execution?\n",
    "\n",
    "* Crash-proof execution\n",
    "* Retries upon failure\n",
    "* Maintains application state, resuming after a crash at the point of failure\n",
    "* Can run across a multitude of processes, even on different machines\n",
    "  * Virtualizes execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyhiqZsp0j5v"
   },
   "source": [
    "## Temporal Provides Durable Execution\n",
    "\n",
    "* It removes the pain of plumbing your distributed system by handling state, retries, timeouts, state preservation right out of the box\n",
    "* Open-Source MIT Licensed\n",
    "* Code based approach to Workflow design\n",
    "  - Instead of building custom orchestration systems, you write normal functions.\n",
    "  - Since it’s a general purpose programming language, there are no abstractions to get in your way. Since AI patterns will continue to evolve, general-purpose programming languages will be as well-suited to implement these new patterns.\n",
    "* Use your own tools, processes, and libraries\n",
    "* Support for 7 languages\n",
    "  * Python, TypeScript, Ruby, Java, Go, PHP, .NET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyAg5UjQwaof"
   },
   "source": [
    "## Demo (Expand for instructor notes or to run on your own)\n",
    "<!--\n",
    "Normal Execution Demo:\n",
    "1. To demonstrate the power of durable execution, we'll first show the power of running the app with no durable execution. This is the code that we showed in the first notebook.\n",
    "2. Clone this repository: `https://github.com/temporalio/edu-ai-workshop-agentic-loop`. The instructions will also be in the README.\n",
    "2. From the `src/module_one_01_ai_agentic_loop/app.py` directory, run `app.py` with `python app.py`.\n",
    "3. When prompted, provide the prompt you want to prompt OpenAI in the command line.\n",
    "4. Before the process generates a PDF, kill the process.\n",
    "5. Rerun the application again with `python app.py` and show that the process restarted and you have to have your application start the research again. Emphasize that from a cost perspective, this could be very costly, because you could have to re-run through many tokens to get to where you left off.\n",
    "\n",
    "Durable Execution Demo:\n",
    "1. Now show the durable version by switching into the ``src/module_one_02_adding_durability` directory.\n",
    "2. Run the Worker with `python worker.py`.\n",
    "3. Run the Workflow with `python workflow.py`.\n",
    "4. When prompted, provide the prompt you want to prompt OpenAI in the command line.\n",
    "5. Before the process generates a PDF, kill the Worker.\n",
    "6. Rerun the Worker and show that you continue right where you left off.\n",
    "7. Emphasize that you lost no progress or data. The Workflow will continue by generating the PDF (available in the same directory) and completing the process successfully.\n",
    "10. Show the Workflow Execution completion in the Web UI.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjVtkEtCbcGX"
   },
   "source": [
    "## Durable Execution Requirements\n",
    "\n",
    "Temporal relies on a Replay mechanism to recover from failure.\n",
    "As your program progresses, Temporal saves the input and output from function calls to the history.\n",
    "This allows a failed program to restart right where it left off.\n",
    "This can also save us a lot of money since we aren't re-burning through tokens!\n",
    "\n",
    "For example:\n",
    "\n",
    "User request: \"Research sustainable energy trends\"\n",
    "✓ Step 1: LLM research call → Output saved to history\n",
    "✓ Step 2: Generate summary → Output saved to history  \n",
    "✗ Step 3: Create PDF → CRASH!\n",
    "\n",
    "On restart:\n",
    "- Temporal replays Steps 1 & 2 from history (no actual execution)\n",
    "- Continues from Step 3 with the same inputs\n",
    "\n",
    "**Because of this, Temporal requires your workflow to be deterministic**\n",
    "\n",
    "A Workflow is deterministic if it produces the same output given the same input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhjwg4tTcEnk"
   },
   "source": [
    "## _Wait, how can AI code be deterministic?_\n",
    "\n",
    "Your **workflow** needs to be deterministic, not the entire application.\n",
    "\n",
    "The key is understanding Temporal's separation of concerns.\n",
    "\n",
    "1. **Non-deterministic parts** - Run arbitrary code that has the potential to fail due to external conditions\n",
    "  * Ex: Calling LLMs, accessing the file system, writing to a database.\n",
    "  * Take 1 minute to discuss with your neighbor any other examples, then we'll share answers\n",
    "2. **Deterministic parts** - Orchestrate the non-deterministic parts\n",
    "  * Ex: Branching, looping, mathematical operations, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJCi-5xlhMH3"
   },
   "source": [
    "## Consider the Following Example\n",
    "\n",
    "* Depending on the time of day, a different decision is made\n",
    "* If it's 5:00pm, it's dinner time\n",
    "* If it's 9:30am, it's breakfast time\n",
    "\n",
    "**What would happen if a user ran this application at 11:59am, it crashed and was replayed at 12:01pm? What would the user expect?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "o68o8I0BgHPA",
    "outputId": "c489afad-1422-47ee-b8e8-0c44bc21f1fa"
   },
   "outputs": [],
   "source": [
    "diagram = \"\"\"\n",
    "graph TD\n",
    "    A[\"Get Current Time\"] --> B[\"Is am or pm?\"]\n",
    "    B --> C[\"Time for breakfast\"]\n",
    "    B --> D[\"Time for dinner\"]\n",
    "\"\"\"\n",
    "render_mermaid(diagram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOI8Mhmhhujv"
   },
   "source": [
    "## _What Does This Have to Do with AI?_\n",
    "\n",
    "_Common Misconception_: \"Since workflows need to be deterministic, your AI applications will always behave the same way and follow identical paths.\"\n",
    "\n",
    "_Reality_: **This statement is completely wrong.**\n",
    "\n",
    "### **Determistic != predetermined**\n",
    "Deterministic means your workflow makes the same decisions when replayed with the same inputs and external responses. Your AI application can still be dynamic and adaptive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxb6hx9bymHs"
   },
   "source": [
    "## AI Application Reality Check\n",
    "\n",
    "**Common Fear:** \"If my workflow is deterministic, my AI application will always do the same thing.\"\n",
    "\n",
    "**Reality:** Your application can be completely dynamic while still being deterministic.\n",
    "\n",
    "Example: User asks \"Research best Italian restaurant in New York City.\"\n",
    "LLM returns product information → application follows restaurant research path\n",
    "\n",
    "**The deterministic guarantee**: If any of these workflows need to replay because of a network outage or an application crash, they will execute the exact same steps with the exact same LLM responses recorded in their history. But each scenario can produce completely different, contextually dynamic results.\n",
    "\n",
    "**Bottom line**: Determinism ensures reliability, not rigidity. Your AI remains as smart and adaptive as you design it to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQ0StPpCyteS"
   },
   "source": [
    "## AI Research application Examples\n",
    "\n",
    "**Each run is completely different** (dynamic), but **each individual run is reproducible** (deterministic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "Y0rH6AEijBdF",
    "outputId": "0c2dba73-454f-4635-999e-b9311e6dcef9"
   },
   "outputs": [],
   "source": [
    "diagram = \"\"\"\n",
    "graph TD\n",
    "    A[\"Ask AI for a Plan\"] --> B{\"Make Tea\"}\n",
    "    B --> C[\"Boil Water\"]\n",
    "    C --> D[\"Steep tea\"]\n",
    "    D --> E[\"Remove and enjoy\"]\n",
    "\"\"\"\n",
    "render_mermaid(diagram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "id": "-p3e4gLrjqV-",
    "outputId": "a359ed1b-d9d5-42ad-dc72-db5360f0ec1c"
   },
   "outputs": [],
   "source": [
    "diagram = \"\"\"\n",
    "graph TD\n",
    "    A[\"Ask AI for a Plan\"] --> B{\"Slay a dragon\"}\n",
    "    B --> C[\"Find the weak spot\"]\n",
    "    C --> D[\"Acquire the correct weapon\"]\n",
    "    D --> E[\"Carry out your attack\"]\n",
    "\"\"\"\n",
    "render_mermaid(diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 587
    },
    "id": "yJicN8wkjrmo",
    "outputId": "3590255c-4693-4f5e-aac5-05f7b6f23e0c"
   },
   "outputs": [],
   "source": [
    "diagram = \"\"\"\n",
    "graph TD\n",
    "    A[\"Ask AI for a Plan\"] --> B{\"Write Code\"}\n",
    "    B --> C[\"Locate files\"]\n",
    "    C --> D[\"Write code\"]\n",
    "    D --> E[\"Evaluate result\"]\n",
    "\"\"\"\n",
    "render_mermaid(diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFdMr3gjkb6y"
   },
   "source": [
    "## Deterministic Workflows Are *Essential* for AI Workflows\n",
    "\n",
    "* The applicationic Loop: AI applications follow a repeatable pattern of reasoning and action:\n",
    "  * Evaluate goal - What am I trying to accomplish?\n",
    "  * Locate tools - What capabilities do I need to use?\n",
    "  * Execute tools - Perform the actual work (API calls, file operations, etc.)\n",
    "  * Evaluate completion - Did I achieve the goal or need to continue?\n",
    "* Tools that the LLM decides to call become **dynamic**, not **non-deterministic**.\n",
    "  - Dynamic: The LLM can choose different tools based on the situation (flight search vs hotel search vs weather API)\n",
    "  - Non-Deterministic: The same input produces same outputs on replay\n",
    "* **Deterministic, not predetermined**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jE-mLgDTy_I8"
   },
   "source": [
    "## Let's Make Your application Durable\n",
    "\n",
    "We're about to transform your simple research application into a durable one. Here's what changes:\n",
    "\n",
    "* Your tools will become crash-proof\n",
    "* Automatic retries and recovery\n",
    "* State persistence\n",
    "\n",
    "This results in a process such as:\n",
    "LLM Decision → Tool A → Result X (Saved in history, then on replay, same result X will result in the same next decision) → Next Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nt8y5WgdzL1e"
   },
   "source": [
    "## What stays the same\n",
    "\n",
    "* Your core logic (LLM call → PDF generation)\n",
    "* Your inputs and outputs\n",
    "* Your business requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtDXSdICK0QV"
   },
   "source": [
    "## Package Our Inputs & Outputs for Ease of Management\n",
    "\n",
    "For ease of use, evolution of parameters, and type checking, Temporal recommends passing and returning a single object from functions. `dataclass` is the recommended structure here, but anything serializable will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtTsi1b9K86m"
   },
   "outputs": [],
   "source": [
    "# TODO: Run this code block to load it into the program\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class LLMCallInput:\n",
    "  prompt: str\n",
    "  llm_api_key: str\n",
    "  llm_model: str\n",
    "\n",
    "@dataclass\n",
    "class PDFGenerationInput:\n",
    "  content: str\n",
    "  filename: str = \"research_pdf.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whhJ6z6P1hXm"
   },
   "source": [
    "## What is an Activity?\n",
    "\n",
    "* An Activity is a function/method that is prone to failure and/or non-deterministic.\n",
    "* Temporal requires all non-deterministic code be run in an Activity\n",
    "\n",
    "Examples:\n",
    "  - External API calls - LLM requests, web scraping, database queries\n",
    "  - File system operations - Reading documents, writing reports, managing storage\n",
    "  - Network operations - HTTP requests, email sending, data transfers\n",
    "  - Resource-intensive computations - Image processing, data analysis, model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vx-Q6RTU19NU"
   },
   "source": [
    "## What Activities Give You\n",
    "\n",
    "* **Automatic retries** when external code fails\n",
    "* **Timeout handling** for slow operations and detecting failures\n",
    "* **Detailed visibility** of execution, including inputs/outputs for debugging\n",
    "* **Automatic checkpoints** - if your workflow crashes, Activities aren't re-executed. Instead, your Workflow continues from the last known good state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Duu7FpnzadyW"
   },
   "source": [
    "## Tasks/Tools become Activities\n",
    "\n",
    "To turn a function/method into an Activity, add the `@activity.defn` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doN_2Wzganj5"
   },
   "outputs": [],
   "source": [
    "# TODO: Run this code block to load it into the program\n",
    "from temporalio import activity\n",
    "from litellm import completion, ModelResponse\n",
    "\n",
    "@activity.defn\n",
    "def llm_call(input: LLMCallInput) -> ModelResponse:\n",
    "    response = completion(\n",
    "      model=input.llm_model,\n",
    "      api_key=input.llm_api_key,\n",
    "      messages=[{ \"content\": input.prompt,\"role\": \"user\"}]\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OOx3-Y4lkkW"
   },
   "outputs": [],
   "source": [
    "# Step 1: Make the code an Activity. Look at the cell below for the solution.\n",
    "# Step 2: Now run the code to load it into the program\n",
    "\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "\n",
    "def create_pdf_activity(input: PDFGenerationInput) -> str:\n",
    "    print(\"Creating PDF document...\")\n",
    "\n",
    "    doc = SimpleDocTemplate(input.filename, pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    title_style = ParagraphStyle(\n",
    "        'CustomTitle',\n",
    "        parent=styles['Heading1'],\n",
    "        fontSize=24,\n",
    "        spaceAfter=30,\n",
    "        alignment=1\n",
    "    )\n",
    "\n",
    "    story = []\n",
    "    title = Paragraph(\"Research Report\", title_style)\n",
    "    story.append(title)\n",
    "    story.append(Spacer(1, 20))\n",
    "    paragraphs = input.content.split('\\n\\n')\n",
    "    for para in paragraphs:\n",
    "        if para.strip():\n",
    "          p = Paragraph(para.strip(), styles['Normal'])\n",
    "          story.append(p)\n",
    "          story.append(Spacer(1, 12))\n",
    "\n",
    "    doc.build(story)\n",
    "\n",
    "    print(f\"SUCCESS! PDF created: {input.filename}\")\n",
    "    return input.filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load and display the solution\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "notebook_dir = Path(os.getcwd())\n",
    "solution_file = notebook_dir / \"Solutions_02_Adding_Durability_with_Temporal\" / \"create_pdf_activity_solution.py\"\n",
    "\n",
    "code = solution_file.read_text()\n",
    "\n",
    "print(\"Solution loaded:\")\n",
    "display(Markdown(f\"```python\\n{code}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gW4eZ3Q-2xF-"
   },
   "source": [
    "## Your Code\n",
    "\n",
    "**Your LLM call is now:**\n",
    "* Protected against API timeouts\n",
    "* Automatically retried with backoff\n",
    "* Observable for debugging\n",
    "\n",
    "**Your PDF generation is now:**\n",
    "* Protected against file system errors\n",
    "* Automatically retried if temporary failures\n",
    "* Tracked for completion verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KF98wlHSKnak"
   },
   "source": [
    "## Activities Are Called from Workflows\n",
    "\n",
    "- You orchestrate the execution of your Activities from within a Workflow.\n",
    "- Workflows contain the decision-making flow, but Activities perform the actual work.\n",
    "- Each Activity call is recorded in the workflow history with inputs and outputs\n",
    "- Workflows can wait for activity completion, handle failures, and make decisions based on results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763
    },
    "id": "6FB7t6qYEiN5",
    "outputId": "1d4c1492-15a0-4cb0-da39-5a641aac2cd8"
   },
   "outputs": [],
   "source": [
    "diagram = \"\"\"\n",
    "graph TD\n",
    "    A[\"Research application Workflow\"] --> B[\"Activity: LLM Call\"]\n",
    "    B --> C[\"Workflow: Process LLM Response\"]\n",
    "    C --> D[\"Activity: Generate Image\"]\n",
    "    D --> E[\"Workflow: Combine Content & Image\"]\n",
    "    E --> F[\"Activity: PDF Creation\"]\n",
    "    F --> G[\"Workflow Complete\"]\n",
    "\n",
    "    style A fill:#e1f5fe\n",
    "    style B fill:#fff3e0\n",
    "    style D fill:#fff3e0\n",
    "    style F fill:#fff3e0\n",
    "    style C fill:#e8f5e8\n",
    "    style E fill:#e8f5e8\n",
    "\"\"\"\n",
    "render_mermaid(diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlWk6DY63Ehg"
   },
   "source": [
    "## More Input/Output Packaging\n",
    "\n",
    "Just like with Activities, Temporal recommends passing a single object to the Workflow for input and returning a single object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mesIIXMMkNG"
   },
   "outputs": [],
   "source": [
    "# TODO: Run this code block to load it into the program\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GenerateReportInput:\n",
    "    prompt: str\n",
    "\n",
    "@dataclass\n",
    "class GenerateReportOutput:\n",
    "    result: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qEvPXcHOn80"
   },
   "source": [
    "## Creating the Workflow\n",
    "\n",
    "* Activities are orchestrated within a Temporal Workflow.\n",
    "* Workflows must **not** make API calls, file system calls, or anything non-deterministic. That is what Activities are for.\n",
    "* Workflows are async, and you define them as a class decorated with the `@workflow.defn` decorator.\n",
    "* Every Workflow has a **single** entry point, which is an `async` method decorated with `@workflow.run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tq0aUW3OQkS"
   },
   "outputs": [],
   "source": [
    "# Step 1: Notice how the Workflow calls the `llm_call` Activity. \n",
    "# Follow the pattern to call the `create_pdf_activity`.\n",
    "# Step 2: A Start-to-Close timeout is the maximum amount of time a single Activity Execution can take. We recommend always setting this timeout.\n",
    "# Set a Start-to-Close timeout of 10 seconds for the `create_pdf_activity`.\n",
    "# Step 3: Run this code block to load it into the program\n",
    "from datetime import timedelta\n",
    "from temporalio import workflow\n",
    "\n",
    "# sandboxed=False is a Notebook only requirement. You normally don't do this\n",
    "@workflow.defn(sandboxed=False)\n",
    "class GenerateReportWorkflow:\n",
    "\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "\n",
    "        llm_call_input = LLMCallInput(prompt=input.prompt, llm_api_key=LLM_API_KEY, llm_model=LLM_MODEL)\n",
    "\n",
    "        research_facts = await workflow.execute_activity(\n",
    "            llm_call,\n",
    "            llm_call_input,\n",
    "            start_to_close_timeout=timedelta(seconds=30), # maximum amount of time a single Activity Execution can take.\n",
    "        )\n",
    "\n",
    "        workflow.logger.info(\"Research complete!\")\n",
    "\n",
    "        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "        pdf_filename = await workflow.execute_activity(\n",
    "            # TODO: Call the create_pdf_activity here\n",
    "            pdf_generation_input,\n",
    "            # TODO: Set the Start-to-Close timeout of 10 seconds here\n",
    "        )\n",
    "\n",
    "        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load and display the solution\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "notebook_dir = Path(os.getcwd())\n",
    "solution_file = notebook_dir / \"Solutions_02_Adding_Durability_with_Temporal\" / \"generatereportworkflow_solution.py\"\n",
    "\n",
    "code = solution_file.read_text()\n",
    "\n",
    "print(\"Solution loaded:\")\n",
    "display(Markdown(f\"```python\\n{code}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0gKu3eSQxbO"
   },
   "source": [
    "## Running a Worker\n",
    "\n",
    "* Temporal Workflows are run on Workers\n",
    "* Workers wait for tasks to do, such as an Activity or Workflow Task, and execute them.\n",
    "* Workers have Workflows and Activities registered to them so the Worker knows what to execute.\n",
    "* Workers find tasks by listenting on a Task Queue\n",
    "* This makes the execution of work indirect; _any_ Worker can pick up a registered Workflow or Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t41qa6TcJjcv"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZB5mZ3HxOk6u"
   },
   "outputs": [],
   "source": [
    "# Run this code block to load it into the program\n",
    "import concurrent.futures\n",
    "from temporalio.client import Client\n",
    "from temporalio.worker import Worker\n",
    "\n",
    "async def run_worker() -> None:\n",
    "    # Create client connected to server at the given address\n",
    "    client = await Client.connect(\"localhost:7233\", namespace=\"default\")\n",
    "\n",
    "    # Run the Worker\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as activity_executor:\n",
    "        worker = Worker(\n",
    "            client,\n",
    "            task_queue=\"research\", # the task queue the Worker is polling\n",
    "            workflows=[GenerateReportWorkflow], # register the Workflow\n",
    "            activities=[llm_call, create_pdf_activity], # register the Activities\n",
    "            activity_executor=activity_executor\n",
    "        )\n",
    "\n",
    "        print(f\"Starting the worker....\")\n",
    "        await worker.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8-AKv5nOYmA"
   },
   "source": [
    "## Running a Temporal Service\n",
    "\n",
    "* The Temporal Service brings it all together\n",
    "* The Temporal Service can be run locally, self-hosted, or you can use Temporal Cloud\n",
    "* The service acts as the supervisor of your Workflows, Activities, and everything else\n",
    "\n",
    "**To run the Temporal Server in this exercise environment**:\n",
    "1. You should have the Temporal Server running in your terminal (run `temporal server start-dev` if not).\n",
    "2. Then in your `Ports` tab on the bottom of this screen, find `8233` and click on the Globe icon to open the Temporal Web UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FB4oDvu7Nbwm"
   },
   "source": [
    "## Starting the Worker\n",
    "\n",
    "A Workflow can't execute if a Worker isn't running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZiLuWj_Q0XR"
   },
   "outputs": [],
   "source": [
    "# Due to the limitation of Jupyter Notebooks and Google Collab, this is how\n",
    "# you must start the worker in a Notebook environment\n",
    "import asyncio\n",
    "\n",
    "worker = asyncio.create_task(run_worker())\n",
    "\n",
    "# If you are running this code in a typical Python environment, you can start\n",
    "# the Worker by just calling `asyncio.run`\n",
    "# if __name__ == \"__main__\":\n",
    "#    asyncio.run(run_worker())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QTOOaYd8UnBL",
    "outputId": "b23a7ac2-7329-436b-cd65-098549037771"
   },
   "outputs": [],
   "source": [
    "# Step 1: Set the Task Queue to be the Task Queue that your Worker is polling\n",
    "# Step 2: Run this code block to load it into the program\n",
    "from temporalio.client import Client\n",
    "import uuid\n",
    "\n",
    "# Create client connected to server at the given address\n",
    "client = await Client.connect(\"localhost:7233\", namespace=\"default\")\n",
    "\n",
    "print(\"Welcome to the Research Report Generator!\")\n",
    "prompt = input(\"Enter your research topic or question: \").strip()\n",
    "\n",
    "if not prompt:\n",
    "    prompt = \"Give me 5 fun and fascinating facts about tardigrades. Make them interesting and educational!\"\n",
    "    print(f\"No prompt entered. Using default: {prompt}\")\n",
    "\n",
    "# Asynchronous start of a Workflow\n",
    "handle = await client.start_workflow(\n",
    "    GenerateReportWorkflow,\n",
    "    GenerateReportInput(prompt=prompt),\n",
    "    id=f\"generate-research-report-workflow-{uuid.uuid4()}\", # user-defined Workflow identifier, which typically has some business meaning\n",
    "    task_queue=\"\", # TODO: Set the Task Queue to be the Task Queue that your Worker is polling\n",
    ")\n",
    "\n",
    "print(f\"Started workflow. Workflow ID: {handle.id}, RunID {handle.result_run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load and display the solution\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "notebook_dir = Path(os.getcwd())\n",
    "solution_file = notebook_dir / \"Solutions_02_Adding_Durability_with_Temporal\" / \"worker_solution.py\"\n",
    "\n",
    "code = solution_file.read_text()\n",
    "\n",
    "print(\"Solution loaded:\")\n",
    "display(Markdown(f\"```python\\n{code}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvM082EAtXZv"
   },
   "source": [
    "## Getting the Result\n",
    "\n",
    "The example above uses async execution. You can `await` the handle to get the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VODi5uGZAj3j",
    "outputId": "50cd4396-b6e6-42ed-8e6a-5dbdefd517cb"
   },
   "outputs": [],
   "source": [
    "# Get the result\n",
    "result = await handle.result()\n",
    "print(f\"Result: {result}\")\n",
    "\n",
    "# To download the report: right click `research_pdf.pdf` in your file explore, then click `Download`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4gwjNk6wx-L"
   },
   "source": [
    "## Temporal Web UI\n",
    "\n",
    "- Temporal provides a robust Web UI for managing Workflow Executions\n",
    "- Can gain insights like responses from Activities, execution time, and failures\n",
    "- Great for debugging and understanding what's happening during your Workflow Executions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsOEZDrgGefo"
   },
   "source": [
    "## Exploring the Web UI\n",
    "\n",
    "Can you locate the following items on the Web UI?\n",
    "\n",
    "- The name of the Task Queue\n",
    "- The name of the two Activities called\n",
    "- The inputs and outputs of the called Activities\n",
    "- Input and output of the Workflow Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZIR7OS4Nk-U"
   },
   "source": [
    "## Simulating Failure\n",
    "\n",
    "What happens if the Worker process were to crash during execution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stu--jIeNwUY"
   },
   "source": [
    "## Adding a Durable Timer\n",
    "\n",
    "- Timers introduce delays in your Workflow with guaranteed execution.\n",
    "- Durable timers will fire even if there is no Worker running, and persists despite restarts and infrastructure failures\n",
    "- Let's add one to the Workflow to give us time to kill the Worker in the middle of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbkxjA4TNvXm"
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "from temporalio import workflow\n",
    "\n",
    "# sandboxed=False is a Notebook only requirement. You normally don't do this\n",
    "@workflow.defn(sandboxed=False)\n",
    "class GenerateReportWorkflow:\n",
    "\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "\n",
    "        llm_call_input = LLMCallInput(prompt=input.prompt, llm_api_key=LLM_API_KEY, llm_model=LLM_MODEL)\n",
    "\n",
    "        research_facts = await workflow.execute_activity(\n",
    "            llm_call,\n",
    "            llm_call_input,\n",
    "            start_to_close_timeout=timedelta(seconds=30),\n",
    "        )\n",
    "\n",
    "        workflow.logger.info(\"Research complete!\")\n",
    "\n",
    "        # Adding a Timer here to pause the Workflow Execution\n",
    "        await workflow.sleep(timedelta(seconds=20))\n",
    "\n",
    "        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "        pdf_filename = await workflow.execute_activity(\n",
    "            create_pdf_activity,\n",
    "            pdf_generation_input,\n",
    "            start_to_close_timeout=timedelta(seconds=10),\n",
    "        )\n",
    "\n",
    "        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEMJDMaYOpek"
   },
   "source": [
    "## Restart the Worker\n",
    "\n",
    "- After a Workflow change, you must restart the Worker for the change to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "miEibF-kT5Ap",
    "outputId": "3b9b5f53-7102-4685-b099-1eb980b17e93"
   },
   "outputs": [],
   "source": [
    "# Run this to kill the current Worker\n",
    "x = worker.cancel()\n",
    "\n",
    "if x:\n",
    "  print(\"Worker killed\")\n",
    "else:\n",
    "  print(\"Worker was not running. Nothing to kill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2AlPpWHBOx4-",
    "outputId": "3aedc295-c157-48f5-f5d3-a0e2f970875e"
   },
   "outputs": [],
   "source": [
    "# Starting the Worker again\n",
    "import asyncio\n",
    "\n",
    "worker = asyncio.create_task(run_worker())\n",
    "\n",
    "# Check if the task is in the set of all tasks\n",
    "if worker in asyncio.all_tasks():\n",
    "    # The sleep is necessary because of the async task scheduling in Jupyter\n",
    "    print(\"Task is currently active.\") # The Worker now registers the updated Workflow changes\n",
    "else:\n",
    "    print(\"Task is not found in active tasks (might have finished or not yet scheduled).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfGUkFh_O4wI"
   },
   "source": [
    "## Start the Workflow and Simulate an Error\n",
    "\n",
    "Start the Workflow again, prompt the LLM, wait about ~5 seconds to let the Timer start, then kill the Worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9U5eLhjPFzj",
    "outputId": "4439423c-b275-4ea7-96b5-d04776f55ae1"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "client = await Client.connect(\"localhost:7233\", namespace=\"default\")\n",
    "\n",
    "print(\"Welcome to the Research Report Generator!\")\n",
    "prompt = input(\"Enter your research topic or question: \").strip()\n",
    "\n",
    "if not prompt:\n",
    "    prompt = \"Give me 5 fun and fascinating facts about tardigrades. Make them interesting and educational!\"\n",
    "    print(f\"No prompt entered. Using default: {prompt}\")\n",
    "\n",
    "# Asynchronous start of a Workflow\n",
    "handle = await client.start_workflow(\n",
    "    GenerateReportWorkflow.run,\n",
    "    GenerateReportInput(prompt=prompt),\n",
    "    id=f\"generate-research-report-workflow-{uuid.uuid4()}\",\n",
    "    task_queue=\"research\",\n",
    ")\n",
    "\n",
    "print(f\"Started workflow. Wait about ~4 seconds to let the Timer start, then kill the Worker.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hFrX4DPmxYrc",
    "outputId": "ccb72f79-f87a-4b3c-e8e8-63690832209a"
   },
   "outputs": [],
   "source": [
    "# After about 5 seconds, run this to kill the current Worker\n",
    "x = worker.cancel()\n",
    "\n",
    "if x:\n",
    "  print(\"Worker killed\")\n",
    "else:\n",
    "  print(\"Worker was not running. Nothing to kill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0xnVjylPj6k"
   },
   "source": [
    "## Watch the Progress in the Web UI\n",
    "\n",
    "Refresh your Web UI, click on the running Workflow Execution, and watch the progress. What do you observe? Does the Timer complete despite the Worker being kiled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "q-EVnqlwPrWy",
    "outputId": "940542a8-b480-4afa-b918-5a4cc9d2dd15"
   },
   "outputs": [],
   "source": [
    "# Get the Temporal Web UI URL\n",
    "# The Temporal server is running on port 8080 (configured in devcontainer.json)\n",
    "print(\"Temporal Web UI is available on port 8080\")\n",
    "print(\"Click on the 'Ports' tab and open port 8080, or navigate to http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99gwelS1P4iz"
   },
   "source": [
    "## Restart the Worker to Resume Execution\n",
    "\n",
    "- Restart the Worker and return to the WebUI.\n",
    "  * What do you think will happen?\n",
    "- You will see the Workflow pick up where it left off as if nothing happened!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6czqh6bP-u5"
   },
   "outputs": [],
   "source": [
    "# Starting the Worker again\n",
    "import asyncio\n",
    "\n",
    "worker = asyncio.create_task(run_worker())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbw85OliybvN"
   },
   "source": [
    "---\n",
    "# Exercise 2 - Adding Durability\n",
    "\n",
    "* In these exercises you will:\n",
    "  * Transform your LLM calls and your execution of tools to Activities\n",
    "  * Use a Temporal Workflow to orchestrate your Activities\n",
    "  * Observe how Temporal handles your errors\n",
    "  * Debug your error and observe your Workflow Execution successfully complete\n",
    "* Go to the **Exercise** Directory in the Google Drive and open the **Practice** Directory\n",
    "* Open _02-Adding-Durability.ipynb_ and follow the instructions\n",
    "* If you get stuck, raise your hand and someone will come by and help. You can also check the `Solution` directory for the answers\n",
    "* **You have 5 mins**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (edu-ai-workshop-mcp)",
   "language": "python",
   "name": "edu-ai-workshop-mcp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
