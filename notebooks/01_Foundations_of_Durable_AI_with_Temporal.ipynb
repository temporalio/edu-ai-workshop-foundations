{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h52a-EFoGu4x"
   },
   "source": [
    "# Foundations of Durable AI with Temporal\n",
    "\n",
    "By now, you've experienced generative AI firsthand. You've used ChatGPT and seen what LLMs can do. They excel at tasks like research, but their real power emerges when we connect them with users and other actions to build more advanced applications that go beyond simple chat interfaces.\n",
    "\n",
    "In this workshop, we'll build toward creating AI agents, but let's start with a simple chain:\n",
    "\n",
    "Use an LLM to generate research -> then produce a PDF from that research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are GenAI Applications?\n",
    "\n",
    "At their core, GenAI applications use an LLM as one component among many. The LLM isn't the application itself. Take ChatGPT as an example—it's an application that wraps an LLM, not an LLM itself. Even this seemingly simple chat interface does much more than just call an LLM:\n",
    "\n",
    "- Displays responses to the user\n",
    "- Captures user input\n",
    "- Maintains conversation history\n",
    "- Orchestrates each subsequent LLM call\n",
    "\n",
    "Applications can look like many different formats. We will start with something like a chain workflow where a series of LLM calls, actions, and user interactions are strung together:\n",
    "\n",
    "<img src=\"https://images.ctfassets.net/0uuz8ydxyd9p/70SBemKQHnqfLxoHgPovQX/33f3a0b6cfc96eae2d17d1a463079560/Screenshot_2025-07-08_at_10.26.26%C3%A2__AM.png\" />\n",
    "\n",
    "Applications can also be agentic, where the path through the business logic isn't predetermined. AI agents are GenAI applications where the LLM has agency over the functionality and flow of the application.\n",
    "\n",
    "<img src=\"https://i.postimg.cc/zv5W9VkH/Screenshot-2025-10-07-at-7-48-20-PM.png\" width=\"300\"/>\n",
    "\n",
    "We'll first look at applications that look like a chain workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Prompt an LLM\n",
    "\n",
    "APIs are used to supply an LLM with context and get back a response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyJ9MCfUkdGm"
   },
   "source": [
    "### Hands-on Moments\n",
    "\n",
    "This is a hands-on workshop!\n",
    "\n",
    "All of the instructors slides and code samples are are executable in the workshop notebooks.\n",
    "We encourage you to follow along and play with the samples!\n",
    "\n",
    "At the end of every chapter (notebook) will be a hands-on lab.\n",
    "This a self-guided experience where the instructor gives a prompt (not an llm haha) with a notebook and some starter code and the attendees solve the puzzle.\n",
    "\n",
    "We are going to create a Research Agent that makes a call to the OpenAI API, conducts research on a topic of your choice, and generates a PDF report from that research. Let's go ahead and first set up your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "mbEBnIn7H1Yg",
    "outputId": "bc16c98c-4661-4f53-849d-f84f4ff92d44"
   },
   "outputs": [],
   "source": [
    "# We'll first install the necessary packages for this workshop.\n",
    "\n",
    "%pip install --quiet litellm reportlab python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXp3VIAdZuhO"
   },
   "source": [
    "### Create an `.env` File\n",
    "\n",
    "Next you'll create a `.env` file to store your API keys.\n",
    "In the file browser on the left, create a new file and name it `.env`.\n",
    "\n",
    "**Note**: It may disappear as soon as you create it. This is because Google Collab hides hidden files (files that start with a `.`) by default.\n",
    "To make this file appear, click the icon that is a crossed out eye and hidden files will appear.\n",
    "\n",
    "Then double click on the `.env` file and add the following line with your API key.\n",
    "\n",
    "```\n",
    "LLM_API_KEY = YOUR_API_KEY\n",
    "LLM_MODEL = \"openai/gpt-4o\"\n",
    "```\n",
    "\n",
    "By default this notebook uses OpenAI's GPT-4o.\n",
    "If you want to use a different LLM provider, look up the appropriate model name [in their documentation](https://docs.litellm.ai/docs/providers) and change the `LLM_MODEL` field and provide your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSl-K8ATXLJ3"
   },
   "outputs": [],
   "source": [
    "# Create .env file\n",
    "with open(\".env\", \"w\") as fh:\n",
    "  fh.write(\"LLM_API_KEY = YOUR_API_KEY\\nLLM_MODEL = openai/gpt-4o\")\n",
    "\n",
    "# Now open the file and replace YOUR_API_KEY with your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fy4s0KTRdWxL",
    "outputId": "69af7aa5-c7fb-462e-d3e5-6bf8a4d73a01"
   },
   "outputs": [],
   "source": [
    "# Load environment variables and configure LLM settings\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Get LLM_API_KEY environment variable and print it to make sure that your .env file is properly loaded.\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"openai/gpt-4o\")\n",
    "LLM_API_KEY = os.getenv(\"LLM_API_KEY\", None)\n",
    "print(\"LLM API Key\", LLM_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting the LLM\n",
    "\n",
    "Our agent will use LLM calls to process information and decide what actions to take.\n",
    "\n",
    "We use `litellm` here, which is a unified interface for over 100+ LLM providers. This means that the same code works with different models - you only need to change the model string. All you need to do is provide an API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this codeblock to load it into the program\n",
    "from litellm import completion, ModelResponse\n",
    "\n",
    "def llm_call(prompt: str, llm_api_key: str, llm_model: str) -> ModelResponse:\n",
    "    response = completion(\n",
    "      model=llm_model,\n",
    "      api_key=llm_api_key,\n",
    "      messages=[{ \"content\": prompt,\"role\": \"user\"}]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "prompt = \"Give me 5 facts about elephants.\"\n",
    "result = llm_call(prompt, LLM_API_KEY, LLM_MODEL)[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making It Interactive\n",
    "\n",
    "Great! You've successfully made your first LLM call with a hardcoded prompt. Now, let's transform this into an interactive research assistant that will perform research on any topic of your choosing.\n",
    "\n",
    "This is the first step toward building our Gen-AI application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this codeblock to load it into the program\n",
    "# Make the API call\n",
    "print(\"Welcome to the Research Report Generator!\")\n",
    "prompt = input(\"Enter your research topic or question: \")\n",
    "result = llm_call(prompt, LLM_API_KEY, LLM_MODEL)\n",
    "\n",
    "# Extract the response content\n",
    "response_content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "print(\"Research complete!\")\n",
    "print(\"-\"*80)\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's process the LLM results.\n",
    "\n",
    "So far, we’ve only returned the response from the LLM to the user.\n",
    "\n",
    "1. Pass results into a \"next step\"\n",
    "2. Add results to conversation history\n",
    "3. Parse the results\n",
    "\n",
    "We want to do more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Prompts to Actions\n",
    "\n",
    "Our LLM can only generate text responses - it thinks and responds, but it can't *do* anything in the real world.\n",
    "\n",
    "What if we want our LLM to:\n",
    "- Search the web for the latest information?\n",
    "- Save the research report to a file?\n",
    "- Send the results via email?\n",
    "- Query a database or call an API?\n",
    "\n",
    "This is where **actions** come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an Action?\n",
    "\n",
    "An action is an external function that performs specific tasks beyond just generating text.\n",
    "\n",
    "Examples:\n",
    "- Information retrieval (web search, database query, file reading)\n",
    "- Communication tools (sending emails, post to Slack, sending text messages or notifications)\n",
    "- Data analysis tools (run calculations, generate charts and graphs)\n",
    "- Creative tools (image generation, document creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Your First Action\n",
    "\n",
    "Let's enhance our research application by adding an action that saves the results to a PDF. This will demonstrate how to move from just generating text to performing concrete actions with that text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a PDF\n",
    "\n",
    "Once you have your research data, you’ll call an action that takes the results and writes it out to a PDF.\n",
    "\n",
    "Remember, actions can interact with the outside world:\n",
    "- File operations (like this PDF generator)\n",
    "- API calls to external services\n",
    "- Database queries\n",
    "- Email sending\n",
    "- Web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's create our PDF generation action.\n",
    "## This function takes text content and formats it into a professional-looking PDF document:\n",
    "\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "\n",
    "def create_pdf(content: str, filename: str = \"research_report.pdf\") -> str:\n",
    "    doc = SimpleDocTemplate(filename, pagesize=letter)\n",
    "\n",
    "    styles = getSampleStyleSheet()\n",
    "    title_style = ParagraphStyle(\n",
    "        'CustomTitle',\n",
    "        parent=styles['Heading1'],\n",
    "        fontSize=24,\n",
    "        spaceAfter=30,\n",
    "        alignment=1\n",
    "    )\n",
    "\n",
    "    story = []\n",
    "    title = Paragraph(\"Research Report\", title_style)\n",
    "    story.append(title)\n",
    "    story.append(Spacer(1, 20))\n",
    "\n",
    "    paragraphs = content.split('\\n\\n')\n",
    "    for para in paragraphs:\n",
    "        if para.strip():\n",
    "            p = Paragraph(para.strip(), styles['Normal'])\n",
    "            story.append(p)\n",
    "            story.append(Spacer(1, 12))\n",
    "\n",
    "    doc.build(story)\n",
    "    return filename\n",
    "\n",
    "create_pdf(\"Hello PDF!\", filename=\"test.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the PDF\n",
    "\n",
    "Download the `test.pdf` PDF and open it by following these steps:\n",
    "  - Right-click the PDF file in the file explorer\n",
    "  - Select \"Download\"\n",
    "  - Open it locally on your machine\n",
    "\n",
    "You should see a title **Research Report** and the words **Hello PDF!** in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all Together\n",
    "\n",
    "You now have multiple functions that you can execute to achieve a task.\n",
    "Next, write the code to bring this all together:\n",
    "\n",
    "1. Use the LLM to respond to a prompt\n",
    "2. Call the `create_pdf` action to create a PDF with the response to your prompt.\n",
    "\n",
    "See how neat this is? Instead of just printing text to the console, your application now creates a tangible deliverable. You ask a question, get a response, and walk away with a professional PDF report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Call the `llm_call` function with `prompt`, `LLM_API_KEY`, and `LLM_MODEL` as arguments.\n",
    "# Step 2: Call the `create_pdf` function with `response_content` as the first argument.\n",
    "# Step 3: Run the code block to execute the program.\n",
    "# Step 4: # Download the `research_report.pdf` PDF and open it by following these steps:\n",
    "#  - Right-click the PDF file in the file explorer\n",
    "#  - Select \"Download\"\n",
    "#  - Open it locally on your machine\n",
    "\n",
    "# Make the API call\n",
    "print(\"Welcome to the Research Report Generator!\")\n",
    "prompt = input(\"Enter your research topic or question: \")\n",
    "result = llm_call() # TODO: Call the `llm_call` function with `prompt`, `LLM_API_KEY`, and `LLM_MODEL` as arguments.\n",
    "\n",
    "# Extract the response content\n",
    "response_content: str = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "pdf_filename = # Call the `create_pdf` function that takes in our 2 arguments: response_content and \"research_report.pdf\"\n",
    "print(f\"SUCCESS! PDF created: {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Run this cell to load and display the solution\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "notebook_dir = Path(os.getcwd())\n",
    "solution_file = notebook_dir / \"Solutions_01_Foundations_of_Durable_AI_with_Temporal\" / \"pdf_generation_solution.py\"\n",
    "\n",
    "code = solution_file.read_text()\n",
    "\n",
    "print(\"Solution loaded:\")\n",
    "display(Markdown(f\"```python\\n{code}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Foundations of a Chain Workflow\n",
    "\n",
    "You now have the foundations of a chain workflow application. We chained an LLM call (`llm_call`) and action together (`create_pdf`) in a defined order (user input → LLM call → PDF generation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo (Expand for instructor notes or to run on your own)\n",
    "<!--\n",
    "Normal Execution Demo:\n",
    "1. To demonstrate the power of durable execution, we'll first show the power of running the app with no durable execution. This is the code that we showed in the first notebook.\n",
    "2. Clone this repository: `https://github.com/temporalio/edu-ai-workshop-agentic-loop`. The instructions will also be in the README.\n",
    "2. From the `demos/module_one_01_foundations_aiic_loop/app.py` directory, run `app.py` with `python app.py`.\n",
    "3. When prompted, provide the prompt you want to prompt OpenAI in the command line.\n",
    "4. Before the process generates a PDF, kill the process.\n",
    "5. Rerun the application again with `python app.py` and show that the process restarted and you have to have your application start the research again. Emphasize that from a cost perspective, this could be very costly, because you could have to re-run through many tokens to get to where you left off.\n",
    "\n",
    "Durable Execution Demo:\n",
    "1. Now show the durable version by switching into the ``demos/module_one_02_adding_durability` directory.\n",
    "2. Run the Worker with `python worker.py`.\n",
    "3. Run the Workflow with `python workflow.py`.\n",
    "4. When prompted, provide the prompt you want to prompt OpenAI in the command line.\n",
    "5. Before the process generates a PDF, kill the Worker.\n",
    "6. Rerun the Worker and show that you continue right where you left off.\n",
    "7. Emphasize that you lost no progress or data. The Workflow will continue by generating the PDF (available in the same directory) and completing the process successfully.\n",
    "10. Show the Workflow Execution completion in the Web UI.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges of GenAI Applications\n",
    "\n",
    "When you combine LLMs with other actions—calling external APIs, querying databases, processing files, you're coordinating multiple services across network boundaries.\n",
    "\n",
    "But challenges can happen:\n",
    "\n",
    "- Networks can be flakey\n",
    "- LLMs are often rate limited\n",
    "- Tool resources (APIs and databases) go down\n",
    "- LLMs are inherently non-deterministic\n",
    "- How do we scale these applications?\n",
    "- What happens when they take a long time to finish?\n",
    "…\n",
    "What else?\n",
    "\n",
    "**GenAI based applications are distributed systems**. And we are going to show you how to make these are resilient.\n",
    "\n",
    "<img src=\"https://i.postimg.cc/xCWJ2qym/chain-workflow-challenges.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents are distributed systems!\n",
    "\n",
    "<img src=\"https://i.postimg.cc/hj7vpcW4/Screenshot-2025-10-07-at-8-11-37-PM.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIkENgxorl8I"
   },
   "source": [
    "### The Challenges Only Amplify with Agents\n",
    "\n",
    "What you built:\n",
    "- User input → LLM call → PDF generation\n",
    "- A simple 3-step chain\n",
    "\n",
    "However, there are a few significant challenges to getting production AI at scale. Your simple agent works great in demos, but production environments are messy and unpredictable.\n",
    "\n",
    "In production, this becomes:\n",
    "- **Longer chains**: User input → Web search → LLM analysis → Database query → LLM refinement → PDF generation → Email delivery\n",
    "- **External dependencies**: APIs, databases, file systems\n",
    "- **Network failures**: Any step can fail\n",
    "- **Expensive retries**: Re-running a 30-second LLM call because step 5 failed\n",
    "\n",
    "Imagine if the user asks for report → LLM times out → half-generated PDF → frustrated user\n",
    "\n",
    "<img src=\"https://i.postimg.cc/bJQ8ZJft/plan-observe-execute.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSvk4rUbaU-h"
   },
   "source": [
    "### Distributed System Issues\n",
    "\n",
    "When workflows get even more sophisticated, they evolve into **agentic systems** - where the LLM makes decisions about which actions to take next, potentially calling other specialized agents that themselves have complex workflows.\n",
    "\n",
    "Increasingly, we are seeing agents calling agents, which are calling other agents.\n",
    "\n",
    "For example, your research application might:\n",
    "- Call a \"Web Scraper\" agent to gather sources (scrapes 5 research websites)\n",
    "- Call a \"Fact Checker\" agent to verify claims (validates statistics against government data)\n",
    "- Call a \"PDF Generator\" agent (what you built!)\n",
    "\n",
    "Each agent has its own event loop **(Plan → Execute → Observe)**:\n",
    "\n",
    "For example: Research Agent → Executes web search → Observes results → Decides if it needs more data → Executes another search or moves to analysis → Observes → Decides next step → etc.\n",
    "\n",
    "<img src=\"https://i.postimg.cc/sXRCCrG1/agents-calling-agents.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This means your \"simple\" research request triggers a complex orchestration.\n",
    "\n",
    "<img src=\"https://i.postimg.cc/SKdfkLJW/agent-orchestration.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now overlay what can go wrong: network partitions, timeouts, and service failures at any step can break the chain anywhere!\n",
    "\n",
    "<img src=\"https://i.postimg.cc/9MftM49H/agent-orchestration-problems.png\" width=\"500\"/>\n",
    "\n",
    "The bottom line: **What looks like one AI task is actually a distributed system challenge!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hcPLb8GWhpc"
   },
   "source": [
    "### Agents are Distributed Systems!\n",
    "\n",
    "**This is why durability matters.** Without it, complex workflows become fragile and expensive. With it, failures become manageable interruptions instead of catastrophic losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Does Durability Mean?\n",
    "\n",
    "- If an LLM call fails halfway through processing, you **don't lose the work already completed**.\n",
    "- If a database query times out, you can **retry just that step** without restarting everything.\n",
    "- If your application crashes, it can **resume from the last successful operation**.\n",
    "- **Long-running processes** can span hours or days without losing context.\n",
    "\n",
    "Without durability, every failure means starting over.\n",
    "With durability, failures become recoverable interruptions instead of catastrophic losses. This is especially critical for GenAI applications where LLM calls are expensive, slow, and unpredictable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fl6jmr_tvG86"
   },
   "source": [
    "### AI Needs Durability\n",
    "\n",
    "  Your research application needs to:\n",
    "  1. Accept user input\n",
    "    - Possible problems: input validation service, rate limiting\n",
    "  2. Call the LLM for research\n",
    "    - Possible problems: Internet connection, API down, rate limiting, timeout\n",
    "  3. Generate PDF\n",
    "    - Possible problems: Memory limits\n",
    "  4. Return success/failure\n",
    "    - Possible problem: Connection dropped\n",
    "\n",
    "  Each step can fail.\n",
    "  Each step might need different agents.\n",
    "  This is a **workflow** - and workflows need orchestration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbw85OliybvN"
   },
   "source": [
    "---\n",
    "\n",
    "# Exercise 1 - Adding More Actions\n",
    "\n",
    "* In this exercise, you'll:\n",
    "  * Call actions with your GenAI Application\n",
    "  * Extract structured information from LLM responses to coordinate between different actions.\n",
    "* Go to the **Exercise** Directory and open the **01_Foundations_of_Durable_AI_with_Temporal** Directory\n",
    "* Open the _Practice_ directory and follow the instructions and filling in the `TODO` statements.\n",
    "* If you get stuck, raise your hand and someone will come by and help. You can also check the `Solution` directory for the answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Next?\n",
    "\n",
    "This workshop introduced you the importance of durability in GenAI applications. Further your learning with these resources:\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [A mental model for agentic AI Applications blogpost](https://temporal.io/blog/a-mental-model-for-agentic-ai-applications)\n",
    "- [From AI hype to durable reality — why agentic flows need distributed-systems discipline blogpost](https://temporal.io/blog/from-ai-hype-to-durable-reality-why-agentic-flows-need-distributed-systems)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (edu-ai-workshop-mcp)",
   "language": "python",
   "name": "edu-ai-workshop-mcp"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
