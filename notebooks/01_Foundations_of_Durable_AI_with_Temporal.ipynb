{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h52a-EFoGu4x"
   },
   "source": [
    "# Foundations of Durable AI with Temporal\n",
    "\n",
    "In this workshop, we'll build toward creating AI agents, but let's start with a simple chain:\n",
    "\n",
    "Use an LLM to generate research -> then produce a PDF from that research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyJ9MCfUkdGm"
   },
   "source": [
    "### Notebook Setup\n",
    "\n",
    "This is a hands-on workshop!\n",
    "\n",
    "All of the instructors slides and code samples are are executable in the workshop notebooks.\n",
    "We encourage you to follow along and play with the samples!\n",
    "\n",
    "At the end of every chapter (notebook) will be a hands-on lab.\n",
    "This a self-guided experience where the instructor gives a prompt (not an llm haha) with a notebook and some starter code and the attendees solve the puzzle.\n",
    "\n",
    "We are going to create a Research application that makes a call to the OpenAI API, conducts research on a topic of your choice, and generates a PDF report from that research. Let's go ahead and first set up your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXp3VIAdZuhO"
   },
   "source": [
    "### Create an `.env` File\n",
    "\n",
    "Next you'll create a `.env` file to store your API keys.\n",
    "In the file browser on the left, create a new file and name it `.env`.\n",
    "Then double click on the `.env` file and add the following line with your API key.\n",
    "\n",
    "```\n",
    "LLM_API_KEY = YOUR_API_KEY\n",
    "LLM_MODEL = \"openai/gpt-4o\"\n",
    "```\n",
    "\n",
    "By default this notebook uses OpenAI's GPT-4o.\n",
    "If you want to use a different LLM provider, look up the appropriate model name [in the documentation](https://docs.litellm.ai/docs/providers) and change the `LLM_MODEL` field and provide your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSl-K8ATXLJ3"
   },
   "outputs": [],
   "source": [
    "# Create .env file\n",
    "with open(\".env\", \"w\") as fh:\n",
    "  fh.write(\"LLM_API_KEY = YOUR_API_KEY\\nLLM_MODEL = openai/gpt-4o\")\n",
    "\n",
    "# Now open the file and replace YOUR_API_KEY with your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fy4s0KTRdWxL",
    "outputId": "69af7aa5-c7fb-462e-d3e5-6bf8a4d73a01"
   },
   "outputs": [],
   "source": [
    "# Load environment variables and configure LLM settings\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Get LLM_API_KEY environment variable and print it to make sure that your .env file is properly loaded.\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"openai/gpt-4o\")\n",
    "LLM_API_KEY = os.getenv(\"LLM_API_KEY\", None)\n",
    "print(\"LLM API Key\", LLM_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Prompt the LLM\n",
    "\n",
    "Our application will make an LLM call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this codeblock to load it into the program\n",
    "from litellm import completion, ModelResponse\n",
    "\n",
    "def llm_call(prompt: str) -> ModelResponse:\n",
    "    response = completion(\n",
    "      model=LLM_MODEL,\n",
    "      api_key=LLM_API_KEY,\n",
    "      messages=[{ \"content\": prompt,\"role\": \"user\"}]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "prompt = \"Give me 5 facts about elephants.\"\n",
    "result = llm_call(prompt)[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making It Interactive\n",
    "\n",
    "Great! You've successfully made your first LLM call with a hardcoded prompt. Now, let's transform this into an interactive research assistant that will perform research on any topic of your choosing.\n",
    "\n",
    "This is the first step toward building our Gen-AI application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this codeblock to load it into the program\n",
    "# Make the API call\n",
    "print(\"Welcome to the Research Report Generator!\")\n",
    "prompt = input(\"Enter your research topic or question: \")\n",
    "result = llm_call(prompt)\n",
    "\n",
    "# Extract the response content\n",
    "response_content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "print(\"Research complete!\")\n",
    "print(\"-\"*80)\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Put our Research Results into a PDF\n",
    "\n",
    "Once you have your research data, you’ll call an action that takes the results and writes it out to a PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes text content and formats it into a professional-looking PDF document.\n",
    "# Run this codeblock to load it into the program.\n",
    "\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "\n",
    "def create_pdf(content: str, filename: str = \"research_report.pdf\") -> str:\n",
    "    doc = SimpleDocTemplate(filename, pagesize=letter)\n",
    "\n",
    "    styles = getSampleStyleSheet()\n",
    "    title_style = ParagraphStyle(\n",
    "        'CustomTitle',\n",
    "        parent=styles['Heading1'],\n",
    "        fontSize=24,\n",
    "        spaceAfter=30,\n",
    "        alignment=1\n",
    "    )\n",
    "\n",
    "    story = []\n",
    "    title = Paragraph(\"Research Report\", title_style)\n",
    "    story.append(title)\n",
    "    story.append(Spacer(1, 20))\n",
    "\n",
    "    paragraphs = content.split('\\n\\n')\n",
    "    for para in paragraphs:\n",
    "        if para.strip():\n",
    "            p = Paragraph(para.strip(), styles['Normal'])\n",
    "            story.append(p)\n",
    "            story.append(Spacer(1, 12))\n",
    "\n",
    "    doc.build(story)\n",
    "    return filename\n",
    "\n",
    "create_pdf(\"Hello PDF!\", filename=\"test.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the PDF\n",
    "\n",
    "Download the `test.pdf` PDF and open it by following these steps:\n",
    "  - Right-click the PDF file in the file explorer\n",
    "  - Select \"Download\"\n",
    "  - Open it locally on your machine\n",
    "\n",
    "You should see a title **Research Report** and the words **Hello PDF!** in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all Together\n",
    "\n",
    "You now have multiple functions that you can execute to achieve a task.\n",
    "Next, write the code to bring this all together:\n",
    "\n",
    "1. Use the LLM to respond to a prompt\n",
    "2. Call the `create_pdf` action to create a PDF with the response to your prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Call the `llm_call` function with `prompt` as an argument.\n",
    "# Step 2: Call the `create_pdf` function with `response_content` as the first argument and\n",
    "# the file name you want to call your PDF (as a string) as the second argument.\n",
    "# Step 3: Run the code block to execute the program.\n",
    "# Step 4: Download your PDF and open it by following these steps:\n",
    "#  - Right-click the PDF file in the file explorer\n",
    "#  - Select \"Download\"\n",
    "#  - Open it locally on your machine\n",
    "\n",
    "# Make the API call\n",
    "print(\"Welcome to the Research Report Generator!\")\n",
    "prompt = input(\"Enter your research topic or question: \")\n",
    "result = llm_call() # TODO: Call the `llm_call` function with `prompt` as an argument.\n",
    "\n",
    "# Extract the response content\n",
    "response_content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "pdf_filename = # TODO: Call the `create_pdf` function that takes in our 2 arguments: response_content and the file name you want to call your PDF (as a string).\n",
    "print(f\"SUCCESS! PDF created: {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Run this cell to load and display the solution\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "notebook_dir = Path(os.getcwd())\n",
    "solution_file = notebook_dir / \"Solutions_01_Foundations_of_Durable_AI_with_Temporal\" / \"pdf_generation_solution.py\"\n",
    "\n",
    "code = solution_file.read_text()\n",
    "\n",
    "print(\"Solution loaded:\")\n",
    "display(Markdown(f\"```python\\n{code}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructor-Led Demo (Expand for instructor notes or to run on your own)\n",
    "<!--\n",
    "Normal Execution Demo:\n",
    "1. To demonstrate the power of durable execution, we'll first show the power of running the app with no durable execution.\n",
    "2. The instructions will also be in the README at the `demos` level. Follow the `Setup` step first before running.\n",
    "3. From the `demos/module_one_01_foundations_aiic_loop/app.py` directory, run `app.py` with `python app.py`.\n",
    "4. When prompted, provide the prompt you want to prompt OpenAI in the command line.\n",
    "5. Before the process generates a PDF, kill the process.\n",
    "6. Rerun the application again with `python app.py` and show that the process restarted and you have to have your application start the research again. Emphasize that from a cost perspective, this could be very costly, because you could have to re-run through many tokens to get to where you left off.\n",
    "\n",
    "Durable Execution Demo:\n",
    "1. Now show the durable version by switching into the ``demos/module_one_02_adding_durability` directory.\n",
    "2. Run the Worker with `uv run worker.py`.\n",
    "3. Run the Workflow with `uv run starter.py`.\n",
    "4. When prompted, provide the prompt you want to prompt OpenAI in the command line.\n",
    "5. Before the process generates a PDF, kill the Worker.\n",
    "6. Rerun the Worker and show that you continue right where you left off.\n",
    "7. Emphasize that you lost no progress or data. The Workflow will continue by generating the PDF (available in the same directory) and completing the process successfully.\n",
    "10. Show the Workflow Execution completion in the Web UI.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbw85OliybvN"
   },
   "source": [
    "---\n",
    "\n",
    "# Exercise 1 - Adding More Actions\n",
    "\n",
    "* In this exercise, you'll:\n",
    "  * Call actions with your GenAI Application\n",
    "  * Extract structured information from LLM responses to coordinate between different actions.\n",
    "* Go to the **Exercises** Directory and open the **01_Foundations_of_Durable_AI_with_Temporal** Directory\n",
    "* Open the _Practice_ directory and follow the instructions and filling in the `TODO` statements and running the codeblocks.\n",
    "* If you get stuck, raise your hand and someone will come by and help. You can also check the `Solution` directory for the answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Next?\n",
    "\n",
    "This chapter introduced you the importance of durability in GenAI applications. Further your learning with these resources:\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [A mental model for agentic AI Applications blogpost](https://temporal.io/blog/a-mental-model-for-agentic-ai-applications)\n",
    "- [From AI hype to durable reality — why agentic flows need distributed-systems discipline blogpost](https://temporal.io/blog/from-ai-hype-to-durable-reality-why-agentic-flows-need-distributed-systems)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (edu-ai-workshop-mcp)",
   "language": "python",
   "name": "edu-ai-workshop-mcp"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
