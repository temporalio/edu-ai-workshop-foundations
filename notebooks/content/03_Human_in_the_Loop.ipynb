{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42Spdn4_DuBH"
   },
   "source": [
    "# Human in the Loop\n",
    "\n",
    "In this section, we will do the following:\n",
    "- Send external data to running workflows (new user input, updated requirements)\n",
    "- Update our AI agent to receive feedback and change its execution path without restarting\n",
    "- Add query handlers to expose Workflow details without interrupting execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfYOsDC1Jc5M"
   },
   "source": [
    "## Setup Notebook\n",
    "\n",
    "Run the following code blocks to install various packages and tools necessary to run this notebook\n",
    "\n",
    "**Be sure to add your .env file again. It doesn't persist across notebooks or sesions**\n",
    "\n",
    "```\n",
    "LLM_API_KEY = YOUR_API_KEY\n",
    "LLM_MODEL = openai/gpt-4o\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ZqFOSwtoJv1L"
   },
   "outputs": [],
   "source": [
    "# We'll first install the necessary packages for this workshop.\n",
    "\n",
    "%pip install --quiet temporalio litellm reportlab python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rjtlX8lJ8rZ"
   },
   "outputs": [],
   "source": [
    "# Create .env file\n",
    "with open(\".env\", \"w\") as fh:\n",
    "    fh.write(\"LLM_API_KEY = YOUR_API_KEY\\nLLM_MODEL = openai/gpt-4o\")\n",
    "\n",
    "    # Now open the file and replace YOUR_API_KEY with your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fy4s0KTRdWxL"
   },
   "outputs": [],
   "source": [
    "# Load environment variables and configure LLM settings\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Get LLM_API_KEY environment variable and print it to make sure that your .env file is properly loaded.\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"openai/gpt-4o\")\n",
    "LLM_API_KEY = os.getenv(\"LLM_API_KEY\", None)\n",
    "print(\"LLM API Key\", LLM_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "at3q7U8wGuSp"
   },
   "outputs": [],
   "source": [
    "# Mermaid renderer, run at the beginning to setup rendering of diagrams\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "def render_mermaid(graph_definition):\n",
    "    \"\"\"\n",
    "    Renders a Mermaid diagram in Google Colab using mermaid.ink.\n",
    "\n",
    "    Args:\n",
    "        graph_definition (str): The Mermaid diagram code (e.g., \"graph LR; A-->B;\").\n",
    "    \"\"\"\n",
    "    graph_bytes = graph_definition.encode(\"ascii\")\n",
    "    base64_bytes = base64.b64encode(graph_bytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLHoJlR4R8Au"
   },
   "outputs": [],
   "source": [
    "# This allows us to run the Temporal Asyncio event loop within the event loop of Jupyter Notebooks\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etzQbb8rphcH"
   },
   "outputs": [],
   "source": [
    "# Running this will download the Temporal CLI\n",
    "\n",
    "!curl -sSf https://temporal.download/cli.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAUFcEulLgwr"
   },
   "source": [
    "### Running a Temporal Service\n",
    "\n",
    "* The Temporal Service brings it all together\n",
    "* The Temporal Service can be run locally, self-hosted, or you can use Temporal Cloud\n",
    "* The service acts as the supervisor of your Workflows, Activities, and everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfMzAaJ2Sfeh"
   },
   "outputs": [],
   "source": [
    "# Start the Temporal Dev Server\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "command = \"/root/.temporalio/bin/temporal server start-dev --ui-port 8000\"\n",
    "temporal_server = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, preexec_fn=os.setsid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N72CUs-ZSgt-"
   },
   "outputs": [],
   "source": [
    "# Uncomment this to Kill the Temporal Dev Server\n",
    "# import signal\n",
    "\n",
    "# os.killpg(os.getpgid(temporal_server.pid), signal.SIGTERM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmBTPDc1K1Oc"
   },
   "source": [
    "## Review the Previous Workflow\n",
    "\n",
    "Let's quickly review the previous Workflow below to refresh your memory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMzXQ7ivLClZ"
   },
   "source": [
    "### Data Models\n",
    "\n",
    "* Temporal recommends passing data to and from Activities and Workflows as a single object.\n",
    "* Use a dataclass for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtTsi1b9K86m"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LLMCallInput:\n",
    "    prompt: str\n",
    "    llm_api_key: str\n",
    "    llm_model: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PDFGenerationInput:\n",
    "    content: str\n",
    "    filename: str = \"research_pdf.pdf\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GenerateReportInput:\n",
    "    prompt: str\n",
    "    llm_api_key: str\n",
    "    llm_research_model: str = \"openai/gpt-4o\"\n",
    "    llm_image_model: str = \"dall-e-3\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GenerateReportOutput:\n",
    "    result: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9X5Xh50LFJs"
   },
   "source": [
    "### Activities\n",
    "\n",
    "* An Activity is a function/method that is prone to failure and/or non-deterministic.\n",
    "* Temporal requires all non-deterministic code be run in an Activity\n",
    "* Activities retry over and over until they succeed or until your customized retry or timeout configuration is hit.\n",
    "* You define an Activity by adding the `@activity.defn` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doN_2Wzganj5"
   },
   "outputs": [],
   "source": [
    "from temporalio import activity\n",
    "from litellm import completion, ModelResponse\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.units import inch\n",
    "\n",
    "\n",
    "@activity.defn\n",
    "def llm_call(input: LLMCallInput) -> ModelResponse:\n",
    "    response = completion(\n",
    "        model=input.llm_model, api_key=input.llm_api_key, messages=[{\"content\": input.prompt, \"role\": \"user\"}]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "@activity.defn\n",
    "def create_pdf(input: PDFGenerationInput) -> str:\n",
    "    print(\"Creating PDF document...\")\n",
    "\n",
    "    doc = SimpleDocTemplate(input.filename, pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    title_style = ParagraphStyle(\"CustomTitle\", parent=styles[\"Heading1\"], fontSize=24, spaceAfter=30, alignment=1)\n",
    "\n",
    "    story = []\n",
    "    title = Paragraph(\"Research Report\", title_style)\n",
    "    story.append(title)\n",
    "    story.append(Spacer(1, 20))\n",
    "    paragraphs = input.content.split(\"\\n\\n\")\n",
    "    for para in paragraphs:\n",
    "        if para.strip():\n",
    "            p = Paragraph(para.strip(), styles[\"Normal\"])\n",
    "            story.append(p)\n",
    "            story.append(Spacer(1, 12))\n",
    "\n",
    "    doc.build(story)\n",
    "\n",
    "    print(f\"SUCCESS! PDF created: {input.filename}\")\n",
    "    return input.filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NyXfwzyLHB9"
   },
   "source": [
    "### Workflow\n",
    "\n",
    "* Activities are orchestrated within a Temporal Workflow.\n",
    "* Workflows must **not** make API calls, file system calls, or anything non-deterministic. That is what Activities are for.\n",
    "* Workflows are async, and you define them as a class decorated with the `@workflow.defn` decorator.\n",
    "* Every Workflow has a **single** entry point, which is an `async` method decorated with `@workflow.run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnJltM-3LIMg"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "\n",
    "from temporalio import workflow\n",
    "\n",
    "\n",
    "# sandboxed=False is a Notebook only requirement. You normally don't do this\n",
    "@workflow.defn(sandboxed=False)\n",
    "class GenerateReportWorkflow:\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "        llm_call_input = LLMCallInput(prompt=input.prompt, llm_api_key=LLM_API_KEY, llm_model=LLM_MODEL)\n",
    "\n",
    "        research_facts = await workflow.execute_activity(\n",
    "            llm_call,\n",
    "            llm_call_input,\n",
    "            start_to_close_timeout=timedelta(seconds=30),\n",
    "        )\n",
    "\n",
    "        workflow.logger.info(\"Research complete!\")\n",
    "\n",
    "        # Uncomment to add delay\n",
    "        # await workflow.sleep(timedelta(seconds=20))\n",
    "\n",
    "        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "        pdf_filename = await workflow.execute_activity(\n",
    "            create_pdf,\n",
    "            pdf_generation_input,\n",
    "            start_to_close_timeout=timedelta(seconds=10),\n",
    "        )\n",
    "\n",
    "        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ug50j4jWLLMo"
   },
   "source": [
    "### Worker\n",
    "\n",
    "* Temporal Workflows are run on Workers\n",
    "* Workers wait for tasks to do, such as executing an Activity or Workflow, and perform them\n",
    "* Workers find tasks by listenting on a Task Queue\n",
    "* Workers have Workflows and Activities registered to them so the Worker knows what it is allowed to execute\n",
    "* This makes the execution of work indirect; _any_ Worker can pick up a registered Workflow or Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sl_MGX0-LUES"
   },
   "outputs": [],
   "source": [
    "from temporalio.client import Client\n",
    "from temporalio.worker import Worker\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "async def run_worker() -> None:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "\n",
    "    client = await Client.connect(\"localhost:7233\", namespace=\"default\")\n",
    "\n",
    "    # Run the Worker\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as activity_executor:\n",
    "        worker = Worker(\n",
    "            client,\n",
    "            task_queue=\"research\",\n",
    "            workflows=[GenerateReportWorkflow],\n",
    "            activities=[llm_call, create_pdf],\n",
    "            activity_executor=activity_executor,\n",
    "        )\n",
    "\n",
    "        print(f\"Starting the worker....\")\n",
    "        await worker.run()\n",
    "\n",
    "\n",
    "## We aren't starting the Worker here, just defining it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbB6XqN-NJR6"
   },
   "source": [
    "## LLM Powered Decision Making\n",
    "\n",
    "- Execution path may be determined at runtime by the LLM\n",
    "- The LLM knows what its **goal** is, current context, available data, what **tools** it has at its disposal, and then determines what step to perform next\n",
    "- **This is considered _Agentic_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBfUn_5wEfvE"
   },
   "source": [
    "## Human Interactions in Your AI Applications\n",
    "\n",
    " - While some AI applications may operate entirely autonomously, many require human intervention\n",
    " - May provide input on launch or at various points throughout the execution\n",
    " - Examples:\n",
    "  - Validation at critical decision points\n",
    "  - Final review before implementation\n",
    "  - Feedback loops\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cFySLL0sagn"
   },
   "source": [
    "### Example: Customer Service Agent with Dynamic Decision Making\n",
    "The Scenario:\n",
    "Customer contacts support: \"My order hasn't arrived and I need it urgently\"\n",
    "\n",
    "Agentic Workflow in Action:\n",
    "- LLM decides: Check order status first\n",
    "- Action result: Order shows \"delayed in transit\"\n",
    "- LLM evaluates options and chooses next step based on findings:\n",
    "\n",
    "- Option A: Process immediate refund (if customer prefers money back)\n",
    "- Option B: Expedite replacement shipment (if item still needed)\n",
    "- Option C: Escalate to human agent (if complex issue detected)\n",
    "\n",
    "Why this Matters\n",
    "- Cost control\n",
    "- Security review\n",
    "- Risk mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bB1hUId-Nolr"
   },
   "outputs": [],
   "source": [
    "diagram = \"\"\"\n",
    "graph LR\n",
    "    U1[User Interaction optional] -.-> L[LLM makes decisions on what to do next]\n",
    "    L --> A[Action does what the LLM decided]\n",
    "    A --> L\n",
    "    U2[User Interaction optional] -.-> A\n",
    "\"\"\"\n",
    "render_mermaid(diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_a8_7myY80C"
   },
   "source": [
    "## Challenges in Non-Durable Human in the Loop Processes\n",
    "\n",
    "**Consider the following scenario**\n",
    "\n",
    "- A user needs to  approve a transaction.\n",
    "- As they are doing this, the website goes down.\n",
    "- How do you mitigate this?\n",
    "  - Do we notify the user to approve the payment again? (creating confusion since the user already 'approved')\n",
    "  - Do we assume approval and risk processing an unauthorized payment?\n",
    "\n",
    "Without durable processes, you're forced to choose between security, user experience, and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_R22lxWTNs1"
   },
   "source": [
    "## Signals in Temporal\n",
    "\n",
    "In Temporal, human interaction in Temporal systems is achieved through a [Temporal Signal](https://docs.temporal.io/encyclopedia/workflow-message-passing).\n",
    "\n",
    "A Signal is a:\n",
    "* Message sent asynchronously to a running Workflow Execution\n",
    "* Used to to change the state and control the flow of a Workflow Execution.\n",
    "\n",
    "*Take 2 minutes to discuss with your neighbor when we might use a Signal, then we'll share answers*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FM27DzFXKQMJ"
   },
   "source": [
    "## Example Signal Usage\n",
    "\n",
    "\n",
    "1. The user initiates a request (Signal).\n",
    "2. The agents (Activities) determine the next step.\n",
    "3. Possible agent responses:\n",
    "    - Ask the user for more information\n",
    "    - Request permission to run a tool\n",
    "4. The user confirms the tool run (Signal).\n",
    "5. The tool runs (API call) and the response is parsed by an LLM and sent back to the user.\n",
    "6. Steps repeat until the agent reaches its goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULS1W9BeZt7X"
   },
   "source": [
    "## Durably Storing Human Interactions\n",
    "\n",
    "Let's go back to the user approving a payment example now. With Temporal, when the user clicks \"approve\" in the finance portal, the approval decision gets durably stored.\n",
    "\n",
    "The user can close the browser, go to lunch, and the Workflow will continue running in the background.\n",
    "\n",
    "If the payment gateway times out, returns an error or becomes unavailable, Temporal automatically retries the payment step. IT does not need to re-ask the user for approval, because that decision is already durably stored in the Workflow state.\n",
    "\n",
    "- **No duplicate work** (user does not have to re-approve the same expense)\n",
    "- **No lost approvals** (Signal persists and processing resumes automatically when system recovesr)\n",
    "- **No manual intervention** (does not need to manually reconcile failed payments or investigate whetehr an expense was actually approved)\n",
    "- **Reliable processing** (business can count on approved expenses being paid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1RStPXFT2OA"
   },
   "source": [
    "## Developing Signals in Temporal\n",
    "\n",
    "There are two steps for adding support for a Signal to your Workflow code:\n",
    "\n",
    "1. **Defining the Signal** - You specify the name and data structure used by Temporal Clients when sending the Signal.\n",
    "2. **Handling the Signal** - You write code that will be invoked when the Signal is received from a Temporal Client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZfKmHu5UBO2"
   },
   "source": [
    "## Implementing a Signal\n",
    "\n",
    "- Create a custom model for the information\n",
    "- Implement an `__init__` method within your Workflow class and add an instance variable to store the Signal\n",
    "- Implement a method to handle the Signal, and decorate it with `@workflow.signal`\n",
    "- Handle the Signal within your Workflow code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLAQZtPMe813"
   },
   "source": [
    "## Let's Add a Signal to Our Workflow\n",
    "\n",
    "We are going to create a Signal to send to our `GenerateReportWorkflow`. After generating the research, before creating a PDF report, the user can review the research. The user has two choices to send to the workflow:\n",
    "  - Keep the research and generate a PDF report by signaling \"keep\"\n",
    "  - Modify the research before generating a PDF report by signaling \"edit\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4ATexy5Cfsx"
   },
   "source": [
    "### Create a Model\n",
    "\n",
    "- Create a model for the Signal to be stored in\n",
    "- Similar to Activities and Workflows, `dataclasses` are recommended here\n",
    "- The model can be nested of other classes, such as `StrEnum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9b4ZxmVYnO7"
   },
   "outputs": [],
   "source": [
    "from enum import StrEnum\n",
    "\n",
    "\n",
    "class UserDecision(StrEnum):\n",
    "    KEEP = \"KEEP\"\n",
    "    EDIT = \"EDIT\"\n",
    "    WAIT = \"WAIT\"  # UserDecision Signal will start with WAIT as the default state\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UserDecisionSignal:\n",
    "    decision: UserDecision\n",
    "    additional_prompt: str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UE3GaTs9fiTN"
   },
   "source": [
    "## Defining a Signal Handler\n",
    "\n",
    "- A Signal is defined in your code and handled in your Workflow Definition.\n",
    "- A given Workflow Definition can support multiple Signals.\n",
    "- To define a Signal, set the Signal decorator `@workflow.signal` on the Signal function inside your Workflow class.\n",
    "- Signal methods define what happens when the signal is received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ebvjx4ZWhIHl"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from temporalio import workflow\n",
    "\n",
    "\n",
    "@workflow.defn(sandboxed=False)  # sandboxed=False is a Notebook only requirement. You normally don't do this)\n",
    "class GenerateReportWorkflow:\n",
    "    def __init__(self) -> None:\n",
    "        # Instance variable to store Signal data\n",
    "        self._user_decision: UserDecisionSignal = UserDecisionSignal(\n",
    "            decision=UserDecision.WAIT\n",
    "        )  # UserDecision Signal starts with WAIT as the default state\n",
    "        self._research_result: str = \"\"\n",
    "\n",
    "    # Define the Signal handler\n",
    "    @workflow.signal\n",
    "    async def user_decision_signal(self, decision_data: UserDecisionSignal) -> None:\n",
    "        # Update instance variable when Signal is received\n",
    "        self._user_decision = decision_data\n",
    "\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "        self._current_prompt = input.prompt\n",
    "\n",
    "        llm_call_input = LLMCallInput(\n",
    "            prompt=self._current_prompt,\n",
    "            llm_api_key=input.llm_api_key,\n",
    "            llm_model=input.llm_research_model,\n",
    "        )\n",
    "        # rest of code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDWk-Ca39mmO"
   },
   "source": [
    "## Waiting for a Signal\n",
    "\n",
    "- Use `workflow.wait_condition()` to pause until Signal is received (user decides the next step).\n",
    "- Creates a blocking checkpoint where the workflow stops and waits\n",
    "- Resumes execution only when the specified condition becomes true\n",
    "\n",
    "Benefits:\n",
    "- Prevents resource waste: Workflow doesn't consume compute cycles while waiting. During these waiting periods, the Workflow instance is not consuming CPU or memory. The Worker only “wakes up” when a Workflow needs to perform something.\n",
    "- Enables true asynchronous interaction: Users can respond at their own pace without timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INGtRIkm96bZ"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from temporalio import workflow\n",
    "\n",
    "\n",
    "@workflow.defn(sandboxed=False)\n",
    "class GenerateReportWorkflow:\n",
    "    def __init__(self) -> None:\n",
    "        self._current_prompt: str = \"\"\n",
    "        # Instance variable to store the Signal in\n",
    "        self._user_decision: UserDecisionSignal = UserDecisionSignal(\n",
    "            decision=UserDecision.WAIT\n",
    "        )  # UserDecision Signal starts with WAIT as the default state\n",
    "        self._research_result: str = \"\"\n",
    "\n",
    "    # Method to handle the Signal\n",
    "    @workflow.signal\n",
    "    async def user_decision_signal(self, decision_data: UserDecisionSignal) -> None:\n",
    "        # Update the instance variable with the received Signal data\n",
    "        self._user_decision = decision_data\n",
    "\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "        self._current_prompt = input.prompt\n",
    "\n",
    "        llm_call_input = LLMCallInput(\n",
    "            prompt=self._current_prompt,\n",
    "            llm_api_key=input.llm_api_key,\n",
    "            llm_model=input.llm_research_model,\n",
    "        )\n",
    "\n",
    "        continue_agent_loop = True\n",
    "\n",
    "        while continue_agent_loop:\n",
    "            research_facts = await workflow.execute_activity(\n",
    "                llm_call,\n",
    "                llm_call_input,\n",
    "                start_to_close_timeout=timedelta(seconds=30),\n",
    "            )\n",
    "\n",
    "            self._research_result = research_facts[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "            print(f\"Research content: {research_facts}\")\n",
    "\n",
    "            # Waiting for Signal with user decision\n",
    "            await workflow.wait_condition(lambda: self._user_decision.decision != UserDecision.WAIT)\n",
    "\n",
    "            # rest of code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zb1UaTYPCssj"
   },
   "source": [
    "### Handle the Signal: Storing the Signal state\n",
    "\n",
    "Once a Signal is received, your Workflow needs to process that data and take appropriate action based on the Signal handler function.\n",
    "\n",
    "There are three main components:\n",
    "\n",
    "1. Store Signal State\n",
    "\n",
    "- Use instance variables to persist signal data across workflow execution\n",
    "- Can be a simple variable, or a Queue for handling many signals\n",
    "- Initialize with default values that indicate \"no signal received yet\"\n",
    "\n",
    "```\n",
    "# Instance variable to hold signal data\n",
    "self._user_decision: UserDecisionSignal = UserDecisionSignal (decision=UserDecision.WAIT) # UserDecision Signal starts with WAIT as the default state\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QtJa74__tKx"
   },
   "source": [
    "### Handle the Signal: Signal Handler Method\n",
    "\n",
    "- Executes immediately when signal arrives\n",
    "- In this example, Signal updates workflow state with received data\n",
    "\n",
    "```\n",
    "@workflow.signal\n",
    "async def user_decision_signal(self, decision_data: UserDecisionSignal) -> None:\n",
    "    self._user_decision = decision_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3m4wCPrASVc"
   },
   "source": [
    "## Handle the Signal: React to Signal in Workflow Logic\n",
    "\n",
    "After receiving a Signal, your Workflow's main execution logic must evaluate the Signal data and determine the appropriate response. This is where the Workflow's business logic intersects with human input.\n",
    "\n",
    "In this example, we are branching execution based on Signal content.\n",
    "\n",
    "```\n",
    "if self._user_decision.decision == UserDecision.KEEP:\n",
    "    continue_agent_loop = False\n",
    "elif self._user_decision.decision == UserDecision.EDIT:\n",
    "    # Modify prompt and reset for next iteration\n",
    "    self._user_decision = UserDecisionSignal(decision=UserDecision.WAIT)\n",
    "```\n",
    "\n",
    "- If the Workflow receives `KEEP` as the `UserDecision`, then the Workflow exits the research loop and proceeds to PDF generation.\n",
    "- If the Workflow receives `EDIT` as the `UserDecision`, then the Workflow incorporates any additional feedback into the prompt, updates the research parameters, and resets the Signal state back to WAIT so it can loop again to regenerate the research and wait for the next user decision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWfa8im4YWLv"
   },
   "outputs": [],
   "source": [
    "# Putting it together, we see we have set the Signal state, created the Signal handler method, and have logic in the Workflow to how it will react to the Signal.\n",
    "\n",
    "import asyncio\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "\n",
    "from temporalio import workflow\n",
    "\n",
    "\n",
    "@workflow.defn(sandboxed=False)\n",
    "class GenerateReportWorkflow:\n",
    "    def __init__(self) -> None:\n",
    "        self._current_prompt: str = \"\"\n",
    "        # Instance variable to store the Signal in\n",
    "        self._user_decision: UserDecisionSignal = UserDecisionSignal(decision=UserDecision.WAIT)\n",
    "        self._research_result: str = \"\"\n",
    "\n",
    "    # Method to handle the Signal\n",
    "    @workflow.signal\n",
    "    async def user_decision_signal(self, decision_data: UserDecisionSignal) -> None:\n",
    "        # Update the instance variable with the received Signal data\n",
    "        self._user_decision = decision_data\n",
    "\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "        self._current_prompt = input.prompt\n",
    "\n",
    "        llm_call_input = LLMCallInput(\n",
    "            prompt=self._current_prompt,\n",
    "            llm_api_key=input.llm_api_key,\n",
    "            llm_model=input.llm_research_model,\n",
    "        )\n",
    "\n",
    "        continue_agent_loop = True\n",
    "\n",
    "        while continue_agent_loop:\n",
    "            research_facts = await workflow.execute_activity(\n",
    "                llm_call,\n",
    "                llm_call_input,\n",
    "                start_to_close_timeout=timedelta(seconds=30),\n",
    "            )\n",
    "\n",
    "            self._research_result = research_facts[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "            print(f\"Research content: {research_facts}\")\n",
    "\n",
    "            # Waiting for Signal with user decision\n",
    "            await workflow.wait_condition(lambda: self._user_decision.decision != UserDecision.WAIT)\n",
    "\n",
    "            if self._user_decision.decision == UserDecision.KEEP:\n",
    "                print(\"User approved the research. Creating PDF...\")\n",
    "                continue_agent_loop = False\n",
    "            elif self._user_decision.decision == UserDecision.EDIT:\n",
    "                print(\"User requested research modification.\")\n",
    "                if self._user_decision.additional_prompt != \"\":\n",
    "                    self._current_prompt = (\n",
    "                        f\"{self._current_prompt}\\n\\nAdditional instructions: {self._user_decision.additional_prompt}\"\n",
    "                    )\n",
    "                    print(f\"Regenerating research with updated prompt: {self._current_prompt}\")\n",
    "                else:\n",
    "                    print(\"No additional instructions provided. Regenerating with original prompt.\")\n",
    "                llm_call_input.prompt = self._current_prompt\n",
    "\n",
    "                # Set the decision back to WAIT for the next loop\n",
    "                self._user_decision = UserDecisionSignal(decision=UserDecision.WAIT)\n",
    "\n",
    "        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "        pdf_filename: str = await workflow.execute_activity(\n",
    "            create_pdf,\n",
    "            pdf_generation_input,\n",
    "            start_to_close_timeout=timedelta(seconds=20),\n",
    "        )\n",
    "\n",
    "        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0fstw-TE8Ck"
   },
   "source": [
    "## Test the Workflow\n",
    "\n",
    "Now that your Signal is implemented, you can start the Workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_knBkUXcEUuR"
   },
   "source": [
    "### Run a Worker\n",
    "\n",
    "As always, code won't execute if a Worker isn't running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPjwyhKPEUEY"
   },
   "outputs": [],
   "source": [
    "worker = asyncio.create_task(run_worker())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tS5s2xtiFPst"
   },
   "source": [
    "### Start the Workflow\n",
    "\n",
    "Now start the Workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TO81vymKFV8x"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from temporalio.client import Client\n",
    "\n",
    "\n",
    "client = await Client.connect(\"localhost:7233\", namespace=\"default\")\n",
    "\n",
    "print(\"Welcome to the Research Report Generator!\")\n",
    "prompt = input(\"Enter your research topic or question: \").strip()\n",
    "\n",
    "if not prompt:\n",
    "    prompt = \"Give me 5 fun and fascinating facts about tardigrades. Make them interesting and educational!\"\n",
    "    print(f\"No prompt entered. Using default: {prompt}\")\n",
    "\n",
    "# Asynchronous start of a Workflow\n",
    "handle = await client.start_workflow(\n",
    "    GenerateReportWorkflow.run,\n",
    "    GenerateReportInput(prompt=prompt, llm_api_key=LLM_API_KEY),\n",
    "    id=\"generate-research-report-workflow\",\n",
    "    task_queue=\"research\",\n",
    ")\n",
    "\n",
    "print(f\"Started workflow. Workflow ID: {handle.id}, RunID {handle.result_run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUJ9n1rnFhZl"
   },
   "source": [
    "## Observing Signals in the Web UI\n",
    "\n",
    "Now, look at the Web UI. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qc120QlcSiom"
   },
   "outputs": [],
   "source": [
    "# Get the Temporal Web UI URL\n",
    "from google.colab.output import eval_js\n",
    "\n",
    "print(eval_js(\"google.colab.kernel.proxyPort(8000)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMBrnpkfbZNU"
   },
   "source": [
    "## Sending Signals\n",
    "\n",
    "- We can now run the Workflow and send our Signal\n",
    "- There are multiple ways to send a Signal\n",
    "  - Using a Temporal Client in an SDK\n",
    "  - Using the Web UI\n",
    "  - Using the `temporal` cli\n",
    "- For this example, we will use a Temporal Client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYvPG66YERjv"
   },
   "source": [
    "## Sending a Signal with the Client\n",
    "\n",
    "To send a Signal with the Temporal Client, we need to get a \"handle\" to a specific Workflow Execution, which will be used to interact with that Workflow.\n",
    "\n",
    "We'll do this with the `get_workflow_handle` method.\n",
    "\n",
    "```\n",
    "handle = client.get_workflow_handle(workflow_id)\n",
    "```\n",
    "\n",
    "With the handle on the Workflow Execution we want to Signal, we'll then pass in our Signal:\n",
    "\n",
    "```\n",
    "signal_data = UserDecisionSignal(decision=UserDecision.KEEP)\n",
    "await handle.signal(\"user_decision_signal\", signal_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gw4QMExOK63s"
   },
   "outputs": [],
   "source": [
    "# Let's send our Signal from the Client code\n",
    "\n",
    "\n",
    "async def send_user_decision_signal(client: Client, workflow_id: str) -> None:\n",
    "    handle = client.get_workflow_handle(\n",
    "        workflow_id\n",
    "    )  # Get a handle on the Workflow Execution we want to send a Signal to.\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"1. Type 'keep' to approve the research and create PDF\")\n",
    "        print(\"2. Type 'edit' to modify the research\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        decision = input(\"Your decision (keep/edit): \").strip().lower()\n",
    "\n",
    "        if decision in {\"keep\", \"1\"}:\n",
    "            signal_data = UserDecisionSignal(decision=UserDecision.KEEP)\n",
    "            await handle.signal(\n",
    "                \"user_decision_signal\", signal_data\n",
    "            )  # Send our Keep Signal to our Workflow Eexecution we have a handle on\n",
    "            print(\"Signal sent to keep research and create PDF\")\n",
    "            break\n",
    "        if decision in {\"edit\", \"2\"}:\n",
    "            additional_prompt_input = input(\"Enter additional instructions for the research (optional): \").strip()\n",
    "            additional_prompt = additional_prompt_input if additional_prompt_input else \"\"\n",
    "\n",
    "            signal_data = UserDecisionSignal(decision=UserDecision.EDIT, additional_prompt=additional_prompt)\n",
    "            await handle.signal(\n",
    "                \"user_decision_signal\", signal_data\n",
    "            )  # Send our Edit Signal to our Workflow Eexecution we have a handle on\n",
    "            print(\"Signal sent to regenerate research\")\n",
    "\n",
    "        else:\n",
    "            print(\"Please enter either 'keep', 'edit'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FmMj5yaLIJJ"
   },
   "outputs": [],
   "source": [
    "send_signal = asyncio.run(send_user_decision_signal(client, handle.id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IX2wuzegGeeH"
   },
   "source": [
    "## Observing Signals in the Web UI\n",
    "\n",
    "Refresh your Web UI. Look for the `Workflow Execution Signaled` Event. What was the input?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZ59oITmDr0H"
   },
   "source": [
    "## Queries in Temporal\n",
    "\n",
    "You can also extract state to show the user with Queries. This can be done during or even after the Workflow Execution. For example, you might want to:\n",
    "\n",
    "- **Monitor Progress of Long-Running Workflows**: A Client might want to receive updates on the progress, like the percentage of the task completed.\n",
    "- **Retrieve Results**: Queries can be used to fetch the results of Activities without waiting for the entire Workflow to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hbb6Z4uEEvFI"
   },
   "source": [
    "## Developing Queries in Temporal\n",
    "\n",
    "Once a Query is issued, the Client waits for a response from the Workflow. Although Queries are typically used to access the state of an open (running) Workflow Execution, it is also possible to send a Query to a closed Workflow Execution. In either case, there must be at least one running Worker for the Task Queue to which that Workflow belongs.\n",
    "\n",
    "Your Query should not include any logic that generates commands (such as executing Activities). Remember, Queries are intended to be read-only operations that do not alter the Workflow's state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBKk05TPFDmx"
   },
   "source": [
    "## Handling a Query\n",
    "\n",
    "Let's create a Query which will allow external clients to read the current research content from a running Workflow without interrupting its execution.\n",
    "\n",
    "Similar to Signals, in the Python SDK, you can handle Queries by annotating a function within your Workflow with `@workflow.query`:\n",
    "\n",
    "```\n",
    "@workflow.query\n",
    "def get_research_result(self) -> str | None:\n",
    "    \"\"\"Query to get the current research result\"\"\"\n",
    "    return self._research_result\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77ZzdbGgIt1s"
   },
   "outputs": [],
   "source": [
    "# Run this to kill the current Worker\n",
    "x = worker.cancel()\n",
    "\n",
    "if x:\n",
    "    print(\"Worker killed\")\n",
    "else:\n",
    "    print(\"Worker was not running. Nothing to kill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sg3Bw8fLJpHD"
   },
   "outputs": [],
   "source": [
    "# Starting the Worker again\n",
    "worker = asyncio.create_task(run_worker())\n",
    "\n",
    "# Check if the task is in the set of all tasks\n",
    "if worker in asyncio.all_tasks():\n",
    "    # The sleep is necessary because of the async task scheduling in Jupyter\n",
    "    print(\"Task is currently active.\")  # The Worker now registers the updated Workflow changes\n",
    "else:\n",
    "    print(\"Task is not found in active tasks (might have finished or not yet scheduled).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eIvD56wHhWO"
   },
   "outputs": [],
   "source": [
    "# Adding a Query into our Workflow code\n",
    "\n",
    "import asyncio\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "\n",
    "from temporalio import workflow\n",
    "\n",
    "\n",
    "@workflow.defn(sandboxed=False)\n",
    "class GenerateReportWorkflow:\n",
    "    def __init__(self) -> None:\n",
    "        self._current_prompt: str = \"\"\n",
    "        # Instance variable to store the Signal in\n",
    "        self._user_decision: UserDecisionSignal = UserDecisionSignal(decision=UserDecision.WAIT)\n",
    "        self._research_result: str = \"\"\n",
    "\n",
    "    # Method to handle the Signal\n",
    "    @workflow.signal\n",
    "    async def user_decision_signal(self, decision_data: UserDecisionSignal) -> None:\n",
    "        # Update the instance variable with the received Signal data\n",
    "        self._user_decision = decision_data\n",
    "\n",
    "    @workflow.query  # Query to get the current research result\n",
    "    def get_research_result(self) -> str | None:\n",
    "        return self._research_result\n",
    "\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "        self._current_prompt = input.prompt\n",
    "\n",
    "        llm_call_input = LLMCallInput(\n",
    "            prompt=self._current_prompt,\n",
    "            llm_api_key=input.llm_api_key,\n",
    "            llm_model=input.llm_research_model,\n",
    "        )\n",
    "\n",
    "        continue_agent_loop = True\n",
    "\n",
    "        while continue_agent_loop:\n",
    "            research_facts = await workflow.execute_activity(\n",
    "                llm_call,\n",
    "                llm_call_input,\n",
    "                start_to_close_timeout=timedelta(seconds=30),\n",
    "            )\n",
    "\n",
    "            self._research_result = research_facts[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "            print(f\"Research content: {research_facts}\")\n",
    "\n",
    "            # Waiting for Signal with user decision\n",
    "            await workflow.wait_condition(lambda: self._user_decision.decision != UserDecision.WAIT)\n",
    "\n",
    "            if self._user_decision.decision == UserDecision.KEEP:\n",
    "                print(\"User approved the research. Creating PDF...\")\n",
    "                continue_agent_loop = False\n",
    "            elif self._user_decision.decision == UserDecision.EDIT:\n",
    "                print(\"User requested research modification.\")\n",
    "                if self._user_decision.additional_prompt != \"\":\n",
    "                    self._current_prompt = (\n",
    "                        f\"{self._current_prompt}\\n\\nAdditional instructions: {self._user_decision.additional_prompt}\"\n",
    "                    )\n",
    "                    print(f\"Regenerating research with updated prompt: {self._current_prompt}\")\n",
    "                else:\n",
    "                    print(\"No additional instructions provided. Regenerating with original prompt.\")\n",
    "                llm_call_input.prompt = self._current_prompt\n",
    "\n",
    "                # Set the decision back to WAIT for the next loop\n",
    "                self._user_decision = UserDecisionSignal(decision=UserDecision.WAIT)\n",
    "\n",
    "        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "        pdf_filename: str = await workflow.execute_activity(\n",
    "            create_pdf,\n",
    "            pdf_generation_input,\n",
    "            start_to_close_timeout=timedelta(seconds=20),\n",
    "        )\n",
    "\n",
    "        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xx6oZ4UqGGBV"
   },
   "source": [
    "## Sending a Query\n",
    "\n",
    "After defining and setting a handler for the Queries in your Workflow, the next step is to send a Query, which is sent from a Temporal Client. To do this, use the query method. To do this, we will again:\n",
    "\n",
    "1. Get a handle of the Workflow Execution we will query\n",
    "2. Send a query with the `query` method.\n",
    "\n",
    "```\n",
    "research_result = await handle.query(GenerateReportWorkflow.get_research_result)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMmo3_0MIPM5"
   },
   "outputs": [],
   "source": [
    "# Let's send our Query from the Client code\n",
    "\n",
    "\n",
    "async def query_research_result(client: Client, workflow_id: str) -> None:\n",
    "    handle = client.get_workflow_handle(workflow_id)\n",
    "\n",
    "    try:\n",
    "        research_result = await handle.query(GenerateReportWorkflow.get_research_result)\n",
    "        if research_result:\n",
    "            print(f\"Research Result: {research_result}\")\n",
    "        else:\n",
    "            print(\"Research Result: Not yet available\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Query failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTW9HXxGJUQX"
   },
   "outputs": [],
   "source": [
    "send_query = asyncio.run(query_research_result(client, handle.id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSU6nnwcKbY6"
   },
   "source": [
    "## Putting it Together\n",
    "\n",
    "You can now make a research call, check your results by querying your Workflow, then choose to edit or keep the research!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbw85OliybvN"
   },
   "source": [
    "---\n",
    "# Exercise 3 - Human in the Loop\n",
    "\n",
    "* In these exercises you will:\n",
    "  * **FILL IN**\n",
    "* Go to the **Exercise** Directory in the Google Drive and open the **Practice** Directory\n",
    "* Open _01-An-AI-Agent-Practice.ipynb_ and follow the instructions\n",
    "* If you get stuck, raise your hand and someone will come by and help. You can also check the `Solution` directory for the answers\n",
    "* **You have 5 mins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eeij5uRFbQiR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
