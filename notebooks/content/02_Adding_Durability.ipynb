{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Durability\n",
        "\n",
        "In this section, we will do the following:\n",
        "- Describe the concepts of durable execution\n",
        "- Transform the previous agent into a Temporal Workflow\n",
        "- Use Temporal tooling to manage the lifecycle of your agent"
      ],
      "metadata": {
        "id": "Iy6hs6HQZY-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SETUP NOTEBOOK\n",
        "\n",
        "Run the following code blocks to install various packages and tools necessary to run this notebook\n",
        "\n",
        "**Be sure to add your .env file again. It doesn't persist across notebooks or sesions**\n",
        "\n",
        "```\n",
        "LLM_API_KEY = YOUR_API_KEY\n",
        "LLM_MODEL = openai/gpt-4o\n",
        "```"
      ],
      "metadata": {
        "id": "DHV0ZA9gaV7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# allows us to run the Temporal Asyncio event loop within the event loop of Jupyter Notebooks\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "TLHoJlR4R8Au"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create .env file\n",
        "with open(\".env\", \"w\") as fh:\n",
        "  fh.write(\"LLM_API_KEY = YOUR_API_KEY\\nLLM_MODEL = openai/gpt-4o\")"
      ],
      "metadata": {
        "id": "4rjtlX8lJ8rZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install temporalio litellm reportlab python-dotenv"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZqFOSwtoJv1L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3b8ad4b-0b0f-497c-d175-47ca07f11aae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting temporalio\n",
            "  Downloading temporalio-1.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting litellm\n",
            "  Downloading litellm-1.76.0-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting reportlab\n",
            "  Downloading reportlab-4.4.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Collecting nexus-rpc==1.1.0 (from temporalio)\n",
            "  Downloading nexus_rpc-1.1.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.12/dist-packages (from temporalio) (5.29.5)\n",
            "Collecting types-protobuf>=3.20 (from temporalio)\n",
            "  Downloading types_protobuf-6.30.2.20250822-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from temporalio) (4.14.1)\n",
            "Requirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.12/dist-packages (from litellm) (3.12.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from litellm) (8.2.1)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (8.7.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from litellm) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (4.25.1)\n",
            "Requirement already satisfied: openai>=1.99.5 in /usr/local/lib/python3.12/dist-packages (from litellm) (1.100.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (2.11.7)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (0.11.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from litellm) (0.21.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from reportlab) (11.3.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from reportlab) (3.4.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (1.20.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=6.8.0->litellm) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.27.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.5->litellm) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.5->litellm) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.5->litellm) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.5->litellm) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.4.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->litellm) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->litellm) (2.32.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers->litellm) (0.34.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (1.1.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.5.0)\n",
            "Downloading temporalio-1.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nexus_rpc-1.1.0-py3-none-any.whl (27 kB)\n",
            "Downloading litellm-1.76.0-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading reportlab-4.4.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_protobuf-6.30.2.20250822-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: types-protobuf, reportlab, nexus-rpc, temporalio, litellm\n",
            "Successfully installed litellm-1.76.0 nexus-rpc-1.1.0 reportlab-4.4.3 temporalio-1.16.0 types-protobuf-6.30.2.20250822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -sSf https://temporal.download/cli.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBeJ9pGZSVAs",
        "outputId": "e9bc4a04-143b-40ec-c72a-a5d6387511dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mtemporal:\u001b[0m Downloading Temporal CLI latest\n",
            "\u001b[1mtemporal:\u001b[0m Temporal CLI installed at /root/.temporalio/bin/temporal\n",
            "\u001b[1mtemporal:\u001b[0m For convenience, we recommend adding it to your PATH\n",
            "\u001b[1mtemporal:\u001b[0m If using bash, run echo export PATH=\"\\$PATH:/root/.temporalio/bin\" >> ~/.bashrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mermaid renderer, run at the beginning to setup rendering of diagrams\n",
        "import base64\n",
        "from IPython.display import Image, display\n",
        "\n",
        "def render_mermaid(graph_definition):\n",
        "    \"\"\"\n",
        "    Renders a Mermaid diagram in Google Colab using mermaid.ink.\n",
        "\n",
        "    Args:\n",
        "        graph_definition (str): The Mermaid diagram code (e.g., \"graph LR; A-->B;\").\n",
        "    \"\"\"\n",
        "    graph_bytes = graph_definition.encode(\"ascii\")\n",
        "    base64_bytes = base64.b64encode(graph_bytes)\n",
        "    base64_string = base64_bytes.decode(\"ascii\")\n",
        "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))"
      ],
      "metadata": {
        "id": "ckJJ2NbFgR5y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Durability"
      ],
      "metadata": {
        "id": "-5lA3iJ9X8Na"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What Can Go Wrong with AI Agents?\n",
        "\n",
        "Let's brainstorm the issues you might face when running your research agent from Notebook 1 in production.\n",
        "\n",
        "**Think about these categories:**\n",
        "* **Technical failures:** What external services could fail?\n",
        "* **Timing issues:** What if something takes longer than expected?\n",
        "* **Recovery challenges:** If something breaks halfway through, what happens?\n",
        "\n",
        "*Take 2 minutes to discuss with your neighbor, then we'll share answers*"
      ],
      "metadata": {
        "id": "kD53k7xOuL6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Issues with Agents in Production\n",
        "\n",
        "**Common answers we typically hear:**\n",
        "* LLM API timeouts or rate limiting\n",
        "* PDF generation fails due to disk space\n",
        "* Network connectivity issues\n",
        "* Process crashes mid-execution\n",
        "* Restarting burns money"
      ],
      "metadata": {
        "id": "OKIuzLjvucLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## These Aren't New Problems\n",
        "\n",
        "The challenges you just identified? They're the same problems we've been solving in distributed systems for decades:\n",
        "\n",
        "**Your Research Agent in Production Reality:**\n",
        "* **LLM API call** - External service that can timeout, rate limit, or be down.\n",
        "* **PDF generation** - File system operation that can fail due to disk space\n",
        "* **User input/output** - Network operations that can be interrupted\n",
        "\n",
        "**This is a distributed system!** Your \"simple\" agent is actually:\n",
        "* Multiple network calls to external services\n",
        "* File system operations\n",
        "* State that needs to persist across failures\n",
        "* Coordination between different steps\n",
        "\n",
        "**The good news:** We have battle-tested solutions for these problems.\n",
        "\n",
        "**The challenge:** Traditional distributed systems tools weren't designed for AI workflows."
      ],
      "metadata": {
        "id": "JQOR9QLGZ_hw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AI Needs Durable Execution\n",
        "\n",
        "Recall your research agent from Notebook 1? Here's what happens in production:\n",
        "\n",
        "**Scenario:** User asks for research on \"sustainable energy trends\"\n",
        "1. LLM call succeeds - generates great research content\n",
        "2. PDF generation fails - disk full or permission error\n",
        "3. **User has to start over completely**"
      ],
      "metadata": {
        "id": "a_1jsgtvaLJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What Developers Actually Want\n",
        "\n",
        "* \"Just fix the disk issue and generate the PDF from the research you already got\"\n",
        "* \"Don't make me pay for the same LLM call twice\"\n",
        "* \"Don't lose my work because of a simple file system error\""
      ],
      "metadata": {
        "id": "bZ28Xb3gwUAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What Normal Execution Gives Us\n",
        "\n",
        "* Start from the beginning every time\n",
        "* Lose all intermediate results\n",
        "* No memory of what succeeded vs what failed"
      ],
      "metadata": {
        "id": "t_cCDze0xOZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What We Need\n",
        "\n",
        "A way to make our AI agents resilient to these failures."
      ],
      "metadata": {
        "id": "yyO36uq0xUBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This is Durable Execution\n",
        "\n",
        "<!-- This is a big slide in the middle with only a title for effect -->"
      ],
      "metadata": {
        "id": "RPwO4YKQxkgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What Is Durable Execution?\n",
        "\n",
        "* Crash-proof execution\n",
        "* Retries upon failure\n",
        "* Maintains application state, resuming after a crash at the point of failure\n",
        "* Can run across a multitude of processes, even on different machines\n",
        "  * Virtualizes execution"
      ],
      "metadata": {
        "id": "MAlIqew1aQhp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Durable Execution Requirements\n",
        "\n",
        "Temporal relies on a Replay mechanism to recover from failure.\n",
        "As your program progresses, Temporal saves the input and output from function calls to the history.\n",
        "This allows a failed program to restart right where it left off.\n",
        "\n",
        "**Because of this, Temporal requires your workflow to be deterministic**\n",
        "\n",
        "A Workflow is deterministic if it produces the same output given the same input."
      ],
      "metadata": {
        "id": "rjVtkEtCbcGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Temporal Provides Durable Execution\n",
        "\n",
        "* Open-Source MIT Licensed\n",
        "* Code based approach to Workflow design\n",
        "* Use your own tools, processes, and libraries\n",
        "* Support for 7 languages\n",
        "  * Python, TypeScript, Ruby, Java, Go, PHP, .NET"
      ],
      "metadata": {
        "id": "xyhiqZsp0j5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo\n",
        "\n",
        "<!--\n",
        "1. To demonstrate the power of durable execution, we'll first show the power of running the app with no durable execution.\n",
        "2. From the normal directory, run `app.py`. Follow the README instructions on how to do so.  \n",
        "3. When prompted, provide the research topic you want OpenAI to\n",
        "perform research for in the CLI.\n",
        "4. Before the process generates a PDF, kill the process.\n",
        "5. Rerun `app.py` and show that the process restarted and you have to have your agent start the research again. Emphasize that from a cost perspective, this could be very costly, because you could have to re-run through many tokens to get to where you left off.\n",
        "6. Now show the durable version by running the Worker and Workflow from the `durable` directory. Follow the README instructions on how to do so.\n",
        "7. When prompted, provide the research topic you want OpenAI to perform research for in the CLI.\n",
        "8. Before the process generates a PDF, kill the Worker.\n",
        "8. Rerun the Worker and show that you continue right where you left off.\n",
        "9. Emphasize that you lost no progress or data. The Workflow will continue by generating the PDF (available in the `durable` directory) and completing the process successfully.\n",
        "10. Show the Workflow Execution completion in the Web UI.\n",
        "-->"
      ],
      "metadata": {
        "id": "qyAg5UjQwaof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _Wait, how can AI code be deterministic?_\n",
        "\n",
        "Your **workflow** needs to be deterministic, not the entire application.\n",
        "\n",
        "The key to understanding this is to separate your applications repeatable (deterministic) and non-repeatable (non-deterministic) parts.\n",
        "\n",
        "1. **Deterministic parts** - Execute the same way when re-run with the same input\n",
        "  * Ex: Branching, looping, mathematical operations, etc.\n",
        "2. **Non-deterministic parts** - Run arbitrary code that has the potential to fail due to external conditions\n",
        "  * Ex: Calling LLMs, accessing the file system, writing to a database"
      ],
      "metadata": {
        "id": "yhjwg4tTcEnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consider the Following Example\n",
        "\n",
        "* Depending on the time of day, a different decision is made\n",
        "* If it's 5:00pm, it's dinner time\n",
        "* If it's 9:30am, it's breakfast time\n",
        "\n",
        "**What would happen if a user ran this application at 11:59am, it crashed and was replayed at 12:01pm? What would the user expect?**"
      ],
      "metadata": {
        "id": "pJCi-5xlhMH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diagram = \"\"\"\n",
        "graph TD\n",
        "    A[\"Get Current Time\"] --> B[\"Is am or pm?\"]\n",
        "    B --> C[\"Time for breakfast\"]\n",
        "    B --> D[\"Time for dinner\"]\n",
        "\"\"\"\n",
        "render_mermaid(diagram)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "o68o8I0BgHPA",
        "outputId": "96459997-5b35-4523-b632-789f6fd65fc6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRECiAgICBBWyJHZXQgQ3VycmVudCBUaW1lIl0gLS0+IEJbIklzIGFtIG9yIHBtPyJdCiAgICBCIC0tPiBDWyJUaW1lIGZvciBicmVha2Zhc3QiXQogICAgQiAtLT4gRFsiVGltZSBmb3IgZGlubmVyIl0K\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _What Does This Have to Do with AI?_\n",
        "\n",
        "_Statement_: Since workflows need to be deterministic, your agents will follow the same path every time.\n",
        "\n",
        "_Reality_: **This statement is 100% incorrect.**\n",
        "\n",
        "### **Determistic != predetermined**"
      ],
      "metadata": {
        "id": "HOI8Mhmhhujv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AI Agent Reality Check\n",
        "\n",
        "**Common Fear:** \"If my workflow is deterministic, my AI agent will always do the same thing.\"\n",
        "\n",
        "**Reality:** Your agent can be completely dynamic while still being deterministic."
      ],
      "metadata": {
        "id": "xxb6hx9bymHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AI Research Agent Examples\n",
        "\n",
        "**Each run is completely different** (dynamic), but **each individual run is reproducible** (deterministic)."
      ],
      "metadata": {
        "id": "kQ0StPpCyteS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diagram = \"\"\"\n",
        "graph TD\n",
        "    A[\"Ask AI for a Plan\"] --> B{\"Make Tea\"}\n",
        "    B --> C[\"Boil Water\"]\n",
        "    C --> D[\"Steep tea\"]\n",
        "    D --> E[\"Remove and enjoy\"]\n",
        "\"\"\"\n",
        "render_mermaid(diagram)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "Y0rH6AEijBdF",
        "outputId": "f217b318-2496-4d1a-9077-f7062d2f2cbb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRECiAgICBBWyJBc2sgQUkgZm9yIGEgUGxhbiJdIC0tPiBCeyJNYWtlIFRlYSJ9CiAgICBCIC0tPiBDWyJCb2lsIFdhdGVyIl0KICAgIEMgLS0+IERbIlN0ZWVwIHRlYSJdCiAgICBEIC0tPiBFWyJSZW1vdmUgYW5kIGVuam95Il0K\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diagram = \"\"\"\n",
        "graph TD\n",
        "    A[\"Ask AI for a Plan\"] --> B{\"Slay a dragon\"}\n",
        "    B --> C[\"Find the weak spot\"]\n",
        "    C --> D[\"Acquire the correct weapon\"]\n",
        "    D --> E[\"Carry out your attack\"]\n",
        "\"\"\"\n",
        "render_mermaid(diagram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "-p3e4gLrjqV-",
        "outputId": "79d03cad-adb2-4375-bb76-172ee415e8d6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRECiAgICBBWyJBc2sgQUkgZm9yIGEgUGxhbiJdIC0tPiBCeyJTbGF5IGEgZHJhZ29uIn0KICAgIEIgLS0+IENbIkZpbmQgdGhlIHdlYWsgc3BvdCJdCiAgICBDIC0tPiBEWyJBY3F1aXJlIHRoZSBjb3JyZWN0IHdlYXBvbiJdCiAgICBEIC0tPiBFWyJDYXJyeSBvdXQgeW91ciBhdHRhY2siXQo=\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diagram = \"\"\"\n",
        "graph TD\n",
        "    A[\"Ask AI for a Plan\"] --> B{\"Write Code\"}\n",
        "    B --> C[\"Locate files\"]\n",
        "    C --> D[\"Write code\"]\n",
        "    D --> E[\"Evaluate result\"]\n",
        "\"\"\"\n",
        "render_mermaid(diagram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        },
        "id": "yJicN8wkjrmo",
        "outputId": "3590255c-4693-4f5e-aac5-05f7b6f23e0c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRECiAgICBBWyJBc2sgQUkgZm9yIGEgUGxhbiJdIC0tPiBCeyJXcml0ZSBDb2RlIn0KICAgIEIgLS0+IENbIkxvY2F0ZSBmaWxlcyJdCiAgICBDIC0tPiBEWyJXcml0ZSBjb2RlIl0KICAgIEQgLS0+IEVbIkV2YWx1YXRlIHJlc3VsdCJdCg==\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How Deterministic Workflows Are *Essential* for AI Workflows\n",
        "\n",
        "* The steps of evaluate goal, locate tools, execute tools, evaluate if goal is complete **makes up the Agentic loop**.\n",
        "* Tools that the LLM decides to call become **dynamic**, not **non-deterministic**.\n",
        "* **Deterministic, not predetermined**"
      ],
      "metadata": {
        "id": "GFdMr3gjkb6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's Make Your Agent Durable\n",
        "\n",
        "We're about to transform your simple research agent into a durable one. Here's what changes:\n",
        "\n",
        "* **Functions** → **Activities** (your tools become crash-proof)\n",
        "* **Direct calls** → **Workflow coordination** (orchestrates activities safely)\n",
        "* **Manual error handling** → **Automatic retries and recovery**\n",
        "\n",
        "This results in a process such as:\n",
        "LLM Decision → Tool A → Result X (Saved in history, then on replay, same result X will result in the same next decision) → Next Decision"
      ],
      "metadata": {
        "id": "jE-mLgDTy_I8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What stays the same\n",
        "\n",
        "* Your core logic (LLM call → PDF generation)\n",
        "* Your inputs and outputs\n",
        "* Your business requirements"
      ],
      "metadata": {
        "id": "nt8y5WgdzL1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What gets better\n",
        "\n",
        "* Automatic retries when API calls fail, timeout, or rate-limit\n",
        "* Resume exactly where you left off after crashes  \n",
        "* Built-in observability and monitoring"
      ],
      "metadata": {
        "id": "VIWu9HifzPoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Package Our Inputs & Outputs for Ease of Management\n",
        "\n",
        "For ease of use, evolution of parameters, and type checking, Temporal recommends passing and returing a single object from functions. `dataclass` is the recommended structure here, but anything serializable will work."
      ],
      "metadata": {
        "id": "NtDXSdICK0QV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class LLMCallInput:\n",
        "  prompt: str\n",
        "  llm_api_key: str\n",
        "  llm_model: str\n",
        "\n",
        "@dataclass\n",
        "class PDFGenerationInput:\n",
        "  content: str\n",
        "  filename: str = \"research_pdf.pdf\""
      ],
      "metadata": {
        "id": "vtTsi1b9K86m"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tasks/Tools become Activities\n",
        "\n",
        "To turn a function/method into an Activity, add the `@activity.defn` decorator."
      ],
      "metadata": {
        "id": "Duu7FpnzadyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from temporalio import activity\n",
        "from litellm import completion, ModelResponse\n",
        "\n",
        "@activity.defn\n",
        "def llm_call(input: LLMCallInput) -> ModelResponse:\n",
        "    response = completion(\n",
        "      model=input.llm_model,\n",
        "      api_key=input.llm_api_key,\n",
        "      messages=[{ \"content\": input.prompt,\"role\": \"user\"}]\n",
        "    )\n",
        "    return response"
      ],
      "metadata": {
        "id": "doN_2Wzganj5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.lib.units import inch\n",
        "\n",
        "@activity.defn\n",
        "def create_pdf_activity(input: PDFGenerationInput) -> str:\n",
        "    print(\"Creating PDF document...\")\n",
        "\n",
        "    doc = SimpleDocTemplate(input.filename, pagesize=letter)\n",
        "    styles = getSampleStyleSheet()\n",
        "    title_style = ParagraphStyle(\n",
        "        'CustomTitle',\n",
        "        parent=styles['Heading1'],\n",
        "        fontSize=24,\n",
        "        spaceAfter=30,\n",
        "        alignment=1\n",
        "    )\n",
        "\n",
        "    story = []\n",
        "    title = Paragraph(\"Research Report\", title_style)\n",
        "    story.append(title)\n",
        "    story.append(Spacer(1, 20))\n",
        "    paragraphs = input.content.split('\\n\\n')\n",
        "    for para in paragraphs:\n",
        "        if para.strip():\n",
        "          p = Paragraph(para.strip(), styles['Normal'])\n",
        "          story.append(p)\n",
        "          story.append(Spacer(1, 12))\n",
        "\n",
        "    doc.build(story)\n",
        "\n",
        "    print(f\"SUCCESS! PDF created: {input.filename}\")\n",
        "    return input.filename"
      ],
      "metadata": {
        "id": "_OOx3-Y4lkkW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is an Activity\n",
        "\n",
        "* An Activity is a function/method that is prone to failure and/or non-deterministic.\n",
        "* Temporal requires all non-deterministic code be run in an Activity\n",
        "* Activities retry over and over until they succeed or until your customized retry or timeout configuration is hit."
      ],
      "metadata": {
        "id": "whhJ6z6P1hXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What Activities give you\n",
        "\n",
        "* **Automatic retries** when external the code fails\n",
        "* **Timeout handling** for slow operations and detecting failures\n",
        "* **Detailed visibility** of execution, including inputs/outputs for debugging\n",
        "* **Automatic checkpoints** - if your workflow crashes, Activities aren't re-executed, and continue from the last known good state\n",
        "\n"
      ],
      "metadata": {
        "id": "Vx-Q6RTU19NU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your Code\n",
        "\n",
        "**Your LLM call is now:**\n",
        "* Protected against API timeouts\n",
        "* Automatically retried with backoff\n",
        "* Observable for debugging\n",
        "\n",
        "**Your PDF generation is now:**\n",
        "* Protected against file system errors\n",
        "* Automatically retried if temporary failures\n",
        "* Tracked for completion verification"
      ],
      "metadata": {
        "id": "gW4eZ3Q-2xF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activities Are Called from Workflows\n",
        "\n",
        "You orchestrate the execution of your Activities from within a Workflow"
      ],
      "metadata": {
        "id": "KF98wlHSKnak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More Input/Output Packaging\n",
        "\n",
        "Just like with Activities, Temporal recommends passing a single object to the Workflow for input and returning a single object."
      ],
      "metadata": {
        "id": "NlWk6DY63Ehg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GenerateReportInput:\n",
        "    prompt: str\n",
        "\n",
        "@dataclass\n",
        "class GenerateReportOutput:\n",
        "    result: str"
      ],
      "metadata": {
        "id": "0mesIIXMMkNG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Environment Variables\n",
        "\n",
        "Now is the time to load in your environment variables with your `LLM_API_KEY`"
      ],
      "metadata": {
        "id": "K32HHolU3nzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(override=True)\n",
        "\n",
        "\n",
        "# Get LLM_API_KEY environment variable\n",
        "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"openai/gpt-4o\")\n",
        "LLM_API_KEY = os.getenv(\"LLM_API_KEY\", None)"
      ],
      "metadata": {
        "id": "fy4s0KTRdWxL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Workflow\n",
        "\n",
        "* Activities are orchestrated within a Temporal Workflow.\n",
        "* Workflows must **not** make API calls, file system calls, or anything non-deterministic. That is what Activities are for.\n",
        "* Workflows are async, and you define them as a class decorated with the `@workflow.defn` decorator.\n",
        "* Every Workflow has a **single** entry point, which is an `async` method decorated with `@workflow.run`."
      ],
      "metadata": {
        "id": "9qEvPXcHOn80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from datetime import timedelta\n",
        "import logging\n",
        "\n",
        "from temporalio import workflow\n",
        "\n",
        "# sandboxed=False is a Notebook only requirement. You normally don't do this\n",
        "@workflow.defn(sandboxed=False)\n",
        "class GenerateReportWorkflow:\n",
        "\n",
        "    @workflow.run\n",
        "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
        "\n",
        "        llm_call_input = LLMCallInput(prompt=input.prompt, llm_api_key=LLM_API_KEY, llm_model=LLM_MODEL)\n",
        "\n",
        "        research_facts = await workflow.execute_activity(\n",
        "            llm_call,\n",
        "            llm_call_input,\n",
        "            start_to_close_timeout=timedelta(seconds=30),\n",
        "        )\n",
        "\n",
        "        workflow.logger.info(\"Research complete!\")\n",
        "\n",
        "        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
        "\n",
        "        pdf_filename = await workflow.execute_activity(\n",
        "            create_pdf_activity,\n",
        "            pdf_generation_input,\n",
        "            start_to_close_timeout=timedelta(seconds=10),\n",
        "        )\n",
        "\n",
        "        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}\")"
      ],
      "metadata": {
        "id": "3tq0aUW3OQkS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running a Worker\n",
        "\n",
        "* Temporal Workflows are run on Workers\n",
        "* Workers wait for tasks to do, such as executing an Activity or Workflow, and perform them\n",
        "* Workers find tasks by listenting on a Task Queue\n",
        "* Workers have Workflows and Activities registered to them so the Worker knows what it is allowed to execute\n",
        "* This makes the execution of work indirect; _any_ Worker can pick up a registered Workflow or Activity"
      ],
      "metadata": {
        "id": "Q0gKu3eSQxbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from temporalio.client import Client\n",
        "from temporalio.worker import Worker\n",
        "import concurrent.futures\n",
        "\n",
        "async def run_worker() -> None:\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
        "\n",
        "    client = await Client.connect(\"localhost:7233\", namespace=\"default\")\n",
        "\n",
        "    # Run the Worker\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as activity_executor:\n",
        "        worker = Worker(\n",
        "            client,\n",
        "            task_queue=\"research\",\n",
        "            workflows=[GenerateReportWorkflow],\n",
        "            activities=[llm_call, create_pdf_activity],\n",
        "            activity_executor=activity_executor\n",
        "        )\n",
        "\n",
        "        print(f\"Starting the worker....\")\n",
        "        await worker.run()"
      ],
      "metadata": {
        "id": "ZB5mZ3HxOk6u"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running a Temporal Service\n",
        "\n",
        "* The Temporal Service brings it all together\n",
        "* The Temporal Service can be run locally, self-hosted, or you can use Temporal Cloud\n",
        "* The service acts as the supervisor of your Workflows, Activities, and everything else"
      ],
      "metadata": {
        "id": "b8-AKv5nOYmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the Temporal Dev Server\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "command = \"/root/.temporalio/bin/temporal server start-dev --ui-port 8000\"\n",
        "temporal_server = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, preexec_fn=os.setsid)"
      ],
      "metadata": {
        "id": "HfMzAaJ2Sfeh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Starting the Worker\n",
        "\n",
        "* A Workflow can't execute if a Worker isn't running"
      ],
      "metadata": {
        "id": "JhjgaQBbNZum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Due to the limitation of Jupyter Notebooks and Google Collab, this is how\n",
        "# you must start the worker in a Notebook environment\n",
        "worker = asyncio.create_task(run_worker())\n",
        "\n",
        "\n",
        "# If you are running this code in a typical Python environment, you can start\n",
        "# the Worker by just calling `asyncio.run`\n",
        "# if __name__ == \"__main__\":\n",
        "#    asyncio.run(run_worker())"
      ],
      "metadata": {
        "id": "FB4oDvu7Nbwm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Executing the Workflow\n",
        "\n",
        "* Temporal Workflows are executed indirectly\n",
        "* You **don't** just execute the file, you request execution from the Temporal Service\n",
        "* You do this using a Temporal Client\n",
        "* In the client you specfiy the Workflow to run, the data, a Workflow ID to identify the execution, and the Task Queue to request on\n",
        "  * This Task Queue **must exactly match** the Task Queue specified in the Worker\n",
        "* Workflows can be started asynchonously or synchronously\n"
      ],
      "metadata": {
        "id": "lZiLuWj_Q0XR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "from temporalio.client import Client\n",
        "\n",
        "\n",
        "client = await Client.connect(\"localhost:7233\", namespace=\"default\")\n",
        "\n",
        "print(\"Welcome to the Research Report Generator!\")\n",
        "prompt = input(\"Enter your research topic or question: \").strip()\n",
        "\n",
        "if not prompt:\n",
        "    prompt = \"Give me 5 fun and fascinating facts about tardigrades. Make them interesting and educational!\"\n",
        "    print(f\"No prompt entered. Using default: {prompt}\")\n",
        "\n",
        "# Asynchronous start of a Workflow\n",
        "handle = await client.start_workflow(\n",
        "    GenerateReportWorkflow.run,\n",
        "    GenerateReportInput(prompt=prompt),\n",
        "    id=\"generate-research-report-workflow\",\n",
        "    task_queue=\"research\",\n",
        ")\n",
        "\n",
        "print(f\"Started workflow. Workflow ID: {handle.id}, RunID {handle.result_run_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTOOaYd8UnBL",
        "outputId": "6d8beb08-ff57-465e-83fd-124dabe3f00b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Research Report Generator!\n",
            "Enter your research topic or question: \n",
            "No prompt entered. Using default: Give me 5 fun and fascinating facts about tardigrades. Make them interesting and educational!\n",
            "Started workflow. Workflow ID: generate-research-report-workflow, RunID 0198ecc2-d2c4-7e9c-92b1-3776e02ca506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the Result\n",
        "\n",
        "The example above uses async execution. You can `await` the handle to get the result."
      ],
      "metadata": {
        "id": "kvM082EAtXZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the result\n",
        "result = await handle.result()\n",
        "print(f\"Result: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VODi5uGZAj3j",
        "outputId": "ba13871d-4955-47a4-8a9a-65ebb748e875"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: GenerateReportOutput(result='Successfully created research report PDF: research_pdf.pdf')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the Web UI\n",
        "\n",
        "- Temporal provides a robust Web UI for managing Workflow Executions\n",
        "- Can gain insights like responses from Activities, execution time, and failures\n",
        "- Great for debugging"
      ],
      "metadata": {
        "id": "c4gwjNk6wx-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the Temporal Web UI URL\n",
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(8000)\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "qc120QlcSiom",
        "outputId": "e2a471fd-77e6-4d55-f5d2-fdac6836db20"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://8000-m-s-1juj2863z1aj7-b.us-west1-0.prod.colab.dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulating Failure\n",
        "\n",
        "What happens if the Worker process were to crash during execution?"
      ],
      "metadata": {
        "id": "9ZIR7OS4Nk-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding a Durable Timer\n",
        "\n",
        "- Timers introduce delays in your Workflow\n",
        "- Durable Timers fire regardless if there is a Worker running\n",
        "- Let's add one to the Workflow to give us time to kill the Worker in the middle of execution."
      ],
      "metadata": {
        "id": "stu--jIeNwUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from datetime import timedelta\n",
        "import logging\n",
        "\n",
        "from temporalio import workflow\n",
        "\n",
        "# sandboxed=False is a Notebook only requirement. You normally don't do this\n",
        "@workflow.defn(sandboxed=False)\n",
        "class GenerateReportWorkflow:\n",
        "\n",
        "    @workflow.run\n",
        "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
        "\n",
        "        llm_call_input = LLMCallInput(prompt=input.prompt, llm_api_key=LLM_API_KEY, llm_model=LLM_MODEL)\n",
        "\n",
        "        research_facts = await workflow.execute_activity(\n",
        "            llm_call,\n",
        "            llm_call_input,\n",
        "            start_to_close_timeout=timedelta(seconds=30),\n",
        "        )\n",
        "\n",
        "        workflow.logger.info(\"Research complete!\")\n",
        "\n",
        "        # Adding a Timer here to pause the Workflow Execution\n",
        "        await workflow.sleep(timedelta(seconds=20))\n",
        "\n",
        "        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
        "\n",
        "        pdf_filename = await workflow.execute_activity(\n",
        "            create_pdf_activity,\n",
        "            pdf_generation_input,\n",
        "            start_to_close_timeout=timedelta(seconds=10),\n",
        "        )\n",
        "\n",
        "        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}\")"
      ],
      "metadata": {
        "id": "RbkxjA4TNvXm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Restart the Worker\n",
        "\n",
        "- After a Workflow change, you must restart the Worker for the change to take effect."
      ],
      "metadata": {
        "id": "NEMJDMaYOpek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to kill the current Worker\n",
        "x = worker.cancel()\n",
        "\n",
        "if x:\n",
        "  print(\"Worker killed\")\n",
        "else:\n",
        "  print(\"Worker was not running. Nothing to kill\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miEibF-kT5Ap",
        "outputId": "d460dcc5-d8c9-42ea-ad88-be8bf55728a4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worker killed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Starting the Worker again\n",
        "worker = asyncio.create_task(run_worker())\n",
        "\n",
        " # Check if the task is in the set of all tasks\n",
        "if worker in asyncio.all_tasks():\n",
        "    # The sleep is necessary because of the async task scheduling in Jupyter\n",
        "    print(\"Task is currently active.\")\n",
        "else:\n",
        "    print(\"Task is not found in active tasks (might have finished or not yet scheduled).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AlPpWHBOx4-",
        "outputId": "9c7c3cf6-779c-4e4f-f6e5-42ce237eaf63"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task is currently active.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start the Workflow and Simulate an Error\n",
        "\n",
        "Start the Workflow again, wait about ~10 seconds to let the first Activity complete, then kill the Worker."
      ],
      "metadata": {
        "id": "bfGUkFh_O4wI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "client = await Client.connect(\"localhost:7233\", namespace=\"default\")\n",
        "\n",
        "print(\"Welcome to the Research Report Generator!\")\n",
        "prompt = input(\"Enter your research topic or question: \").strip()\n",
        "\n",
        "if not prompt:\n",
        "    prompt = \"Give me 5 fun and fascinating facts about tardigrades. Make them interesting and educational!\"\n",
        "    print(f\"No prompt entered. Using default: {prompt}\")\n",
        "\n",
        "# Asynchronous start of a Workflow\n",
        "handle = await client.start_workflow(\n",
        "    GenerateReportWorkflow.run,\n",
        "    GenerateReportInput(prompt=prompt),\n",
        "    id=\"generate-research-report-workflow\",\n",
        "    task_queue=\"research\",\n",
        ")\n",
        "\n",
        "print(f\"Started workflow. Workflow ID: {handle.id}, RunID {handle.result_run_id}\")"
      ],
      "metadata": {
        "id": "A9U5eLhjPFzj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "459fed5b-9a39-417f-bfee-deab05233939"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Research Report Generator!\n",
            "Enter your research topic or question: \n",
            "No prompt entered. Using default: Give me 5 fun and fascinating facts about tardigrades. Make them interesting and educational!\n",
            "Started workflow. Workflow ID: generate-research-report-workflow, RunID 0198ecc6-4b2f-77c2-badb-418cbc0689c0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to kill the current Worker\n",
        "x = worker.cancel()\n",
        "\n",
        "if x:\n",
        "  print(\"Worker killed\")\n",
        "else:\n",
        "  print(\"Worker was not running. Nothing to kill\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08bc80ff-f32f-4964-e33d-b1d20975e92a",
        "id": "hFrX4DPmxYrc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worker killed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Watch the Progress in the Web UI\n",
        "\n",
        "- Go to the Web UI and watch the progress. Try to locate the following things:\n",
        "  - Input to the Workflow\n",
        "  - Result of the first Activity\n",
        "  - The Timer firing\n",
        "  - The Workflow timing out"
      ],
      "metadata": {
        "id": "X0xnVjylPj6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the Temporal Web UI URL\n",
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(8000)\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "q-EVnqlwPrWy",
        "outputId": "e52ad8ea-2e5a-46e3-82ad-0fc282f5af52"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://8000-m-s-1juj2863z1aj7-b.us-west1-0.prod.colab.dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Restart the Worker to Resume Execution\n",
        "\n",
        "- Restart the Worker and return to the WebUI\n",
        "- You will see the Workflow pick up where it left off as if nothing happened"
      ],
      "metadata": {
        "id": "99gwelS1P4iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Starting the Worker again\n",
        "worker = asyncio.create_task(run_worker())"
      ],
      "metadata": {
        "id": "t6czqh6bP-u5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Exercise 2 - Adding Durability\n",
        "\n",
        "* In these exercises you will:\n",
        "  * **FILL IN**\n",
        "* Go to the **Exercise** Directory in the Google Drive and open the **Practice** Directory\n",
        "* Open _02-Adding-Durability.ipynb_ and follow the instructions\n",
        "* If you get stuck, raise your hand and someone will come by and help. You can also check the `Solution` directory for the answers\n",
        "* **You have 5 mins**"
      ],
      "metadata": {
        "id": "Pbw85OliybvN"
      }
    }
  ]
}