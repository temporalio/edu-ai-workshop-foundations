{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iy6hs6HQZY-Z"
   },
   "source": [
    "# Adding Durability\n",
    "\n",
    "In this section, we will do the following:\n",
    "- Describe the concepts of durable execution\n",
    "- Transform the previous agent into a Temporal Workflow\n",
    "- Use Temporal tooling to manage the lifecycle of your agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHV0ZA9gaV7n"
   },
   "source": [
    "## Setup Your Notebook\n",
    "\n",
    "Run the following code blocks to install various packages and tools necessary to run this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ZqFOSwtoJv1L",
    "outputId": "94a71a5d-6d81-43e5-9118-e218b52f69ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m470.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# We'll first install the necessary packages for this workshop.\n",
    "\n",
    "%pip install --quiet temporalio litellm reportlab python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Znl4WVrt6T0d"
   },
   "source": [
    "## Create a `.env` File\n",
    "\n",
    "Next you'll create a `.env` file to store your API keys.\n",
    "In the file browser on the left, create a new file and name it `.env`.\n",
    "Note that this file doesn't persist across notebooks or sesions.\n",
    "\n",
    "**Note**: It may disappear as soon as you create it. This is because Google Collab hides hidden files (files that start with a `.`) by default.\n",
    "To make this file appear, click the icon that is a crossed out eye and hidden files will appear.\n",
    "\n",
    "Then double click on the `.env` file and add the following line with your API key.\n",
    "\n",
    "```\n",
    "LLM_API_KEY = YOUR_API_KEY\n",
    "LLM_MODEL = \"openai/gpt-4o\"\n",
    "```\n",
    "\n",
    "By default this notebook uses OpenAI's GPT-4o.\n",
    "If you want to use a different LLM provider, look up the appropriate model name [in their documentation](https://docs.litellm.ai/docs/providers) and change the `LLM_MODEL` field and provide your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvdK0Uw96qof"
   },
   "outputs": [],
   "source": [
    "# Create .env file\n",
    "with open(\".env\", \"w\") as fh:\n",
    "    fh.write(\"LLM_API_KEY = YOUR_API_KEY\\nLLM_MODEL = openai/gpt-4o\")\n",
    "\n",
    "# Now open the file and replace YOUR_API_KEY with your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uPJ0l0su7CWK",
    "outputId": "4be914e0-b472-482b-c087-b4dbc0445fc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM API Key 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\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables and configure LLM settings\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Get LLM_API_KEY environment variable and print it to make sure that your .env file is properly loaded.\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"openai/gpt-4o\")\n",
    "LLM_API_KEY = os.getenv(\"LLM_API_KEY\", None)\n",
    "print(\"LLM API Key\", LLM_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLHoJlR4R8Au"
   },
   "outputs": [],
   "source": [
    "# This allows us to run the Temporal Asyncio event loop within the event loop of Jupyter Notebooks\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SBeJ9pGZSVAs",
    "outputId": "8b369a59-87d0-458a-b454-e4b84d48e053"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mtemporal:\u001b[0m Downloading Temporal CLI latest\n",
      "\u001b[1mtemporal:\u001b[0m Temporal CLI installed at /root/.temporalio/bin/temporal\n",
      "\u001b[1mtemporal:\u001b[0m For convenience, we recommend adding it to your PATH\n",
      "\u001b[1mtemporal:\u001b[0m If using bash, run echo export PATH=\"\\$PATH:/root/.temporalio/bin\" >> ~/.bashrc\n"
     ]
    }
   ],
   "source": [
    "# Running this will download the Temporal CLI, which we need for this demo.\n",
    "\n",
    "!curl -sSf https://temporal.download/cli.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckJJ2NbFgR5y"
   },
   "outputs": [],
   "source": [
    "# Mermaid renderer, run at the beginning to setup rendering of diagrams\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "def render_mermaid(graph_definition):\n",
    "    \"\"\"\n",
    "    Renders a Mermaid diagram in Google Colab using mermaid.ink.\n",
    "\n",
    "    Args:\n",
    "        graph_definition (str): The Mermaid diagram code (e.g., \"graph LR; A-->B;\").\n",
    "    \"\"\"\n",
    "    graph_bytes = graph_definition.encode(\"ascii\")\n",
    "    base64_bytes = base64.b64encode(graph_bytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5lA3iJ9X8Na"
   },
   "source": [
    "# Adding Durability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kD53k7xOuL6a"
   },
   "source": [
    "## What Can Go Wrong with AI Agents?\n",
    "\n",
    "Let's brainstorm the issues you might face when running your research agent from Notebook 1 in production.\n",
    "\n",
    "**Think about these categories:**\n",
    "* **Technical failures:** What external services could fail?\n",
    "* **Timing issues:** What if something takes longer than expected?\n",
    "* **Recovery challenges:** If something breaks halfway through, what happens?\n",
    "\n",
    "*Take 2 minutes to discuss with your neighbor, then we'll share answers.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKIuzLjvucLf"
   },
   "source": [
    "## Common Issues with Agents in Production\n",
    "\n",
    "**Common answers we typically hear:**\n",
    "* LLM API timeouts or rate limiting\n",
    "* PDF generation fails due to disk space\n",
    "* Network connectivity issues\n",
    "* Process crashes mid-execution\n",
    "* Restarting burns money"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCBVj2WX7mwN"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQOR9QLGZ_hw"
   },
   "source": [
    "## These Aren't New Problems\n",
    "\n",
    "The challenges you just identified? They're the same problems we've been solving in distributed systems for decades:\n",
    "\n",
    "**Your Research Agent in Production Reality:**\n",
    "* **LLM API call** - External service that can timeout, rate limit, or be down.\n",
    "* **PDF generation** - File system operation that can fail due to disk space\n",
    "* **User input/output** - Network operations that can be interrupted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01ZkJPjC7njg"
   },
   "source": [
    "## Agents are Distributed Systems!\n",
    "\n",
    "**This is a distributed system!** Your \"simple\" agent is actually:\n",
    "* Multiple network calls to external services\n",
    "* File system operations\n",
    "* State that needs to persist across failures\n",
    "* Coordination between different steps\n",
    "\n",
    "**The challenge:** Traditional distributed systems tools weren't designed for AI workflows. They don't understand expensive LLM calls, context windows, or long-term state management.\n",
    "\n",
    "**The good news:** You can use a platform that guarantees the reliable execution of your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_1jsgtvaLJ1"
   },
   "source": [
    "## Your Report Generation Agent Needs Durable Execution\n",
    "\n",
    "Recall your research agent from Notebook 1? Here's what happens in production:\n",
    "\n",
    "**Scenario:** User asks for research on \"sustainable energy trends\"\n",
    "1. LLM call succeeds - generates comprehensive research content ($2.50 in API costs)\n",
    "2. PDF generation fails - disk full, permission error, or process crash\n",
    "3. **User has to start over completely** - losing expensive work and time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZ28Xb3gwUAj"
   },
   "source": [
    "## What Developers Actually Want\n",
    "\n",
    "* \"Just fix the disk issue and generate the PDF from the research you already have.\"\n",
    "* \"Don't make me pay for the same LLM call twice!\"\n",
    "* \"Don't lose my work because of a simple file system error!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_cCDze0xOZv"
   },
   "source": [
    "## What Normal Execution Gives Us\n",
    "\n",
    "* Every failure means restarting from scratch\n",
    "* Expensive LLM calls are repeated unnecessarily\n",
    "* User experience becomes frustrating and unreliable\n",
    "* No way to resume from where you left off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyO36uq0xUBF"
   },
   "source": [
    "## What We Need\n",
    "\n",
    "A way to make our AI agents resilient to these failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPwO4YKQxkgP"
   },
   "source": [
    "## This is Durable Execution\n",
    "\n",
    "<!-- This is a big slide in the middle with only a title for effect -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAlIqew1aQhp"
   },
   "source": [
    "## What Is Durable Execution?\n",
    "\n",
    "* Crash-proof execution\n",
    "* Retries upon failure\n",
    "* Maintains application state, resuming after a crash at the point of failure\n",
    "* Can run across a multitude of processes, even on different machines\n",
    "  * Virtualizes execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyhiqZsp0j5v"
   },
   "source": [
    "## Temporal Provides Durable Execution\n",
    "\n",
    "* It removes the pain of plumbing your distributed system by handling state, retries, timeouts, state preservation right out of the box\n",
    "* Open-Source MIT Licensed\n",
    "* Code based approach to Workflow design\n",
    "* Use your own tools, processes, and libraries\n",
    "* Support for 7 languages\n",
    "  * Python, TypeScript, Ruby, Java, Go, PHP, .NET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyAg5UjQwaof"
   },
   "source": [
    "## Demo (Expand for instructor notes or to run on your own)\n",
    "\n",
    "<!--\n",
    "Normal Execution Demo:\n",
    "1. To demonstrate the power of durable execution, we'll first show the power of running the app with no durable execution. This is the code that we showed in the first notebook.\n",
    "2. Clone this repository: `https://github.com/temporalio/edu-ai-workshop`. The instructions will also be in the README.\n",
    "2. From the `src/module_one_01_ai_agent/app.py` directory, run `app.py` with `python app.py`.\n",
    "3. When prompted, provide the prompt you want to prompt OpenAI in the command line.\n",
    "4. Before the process generates a PDF, kill the process.\n",
    "5. Rerun the application again with `python app.py` and show that the process restarted and you have to have your agent start the research again. Emphasize that from a cost perspective, this could be very costly, because you could have to re-run through many tokens to get to where you left off.\n",
    "\n",
    "Durable Execution Demo:\n",
    "1. Now show the durable version by switching into the ``src/module_one_02_adding_durability` directory.\n",
    "2. Run the Worker with `python worker.py`.\n",
    "3. Run the Workflow with `python workflow.py`.\n",
    "4. When prompted, provide the prompt you want to prompt OpenAI in the command line.\n",
    "5. Before the process generates a PDF, kill the Worker.\n",
    "6. Rerun the Worker and show that you continue right where you left off.\n",
    "7. Emphasize that you lost no progress or data. The Workflow will continue by generating the PDF (available in the same directory) and completing the process successfully.\n",
    "10. Show the Workflow Execution completion in the Web UI.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjVtkEtCbcGX"
   },
   "source": [
    "## Durable Execution Requirements\n",
    "\n",
    "Temporal relies on a Replay mechanism to recover from failure.\n",
    "As your program progresses, Temporal saves the input and output from function calls to the history.\n",
    "This allows a failed program to restart right where it left off.\n",
    "\n",
    "For example:\n",
    "\n",
    "User request: \"Research sustainable energy trends\"\n",
    "✓ Step 1: LLM research call → Output saved to history\n",
    "✓ Step 2: Generate summary → Output saved to history  \n",
    "✗ Step 3: Create PDF → CRASH!\n",
    "\n",
    "On restart:\n",
    "- Temporal replays Steps 1 & 2 from history (no actual execution)\n",
    "- Continues from Step 3 with the same inputs\n",
    "\n",
    "**Because of this, Temporal requires your workflow to be deterministic**\n",
    "\n",
    "A Workflow is deterministic if it produces the same output given the same input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhjwg4tTcEnk"
   },
   "source": [
    "## _Wait, how can AI code be deterministic?_\n",
    "\n",
    "Your **workflow** needs to be deterministic, not the entire application.\n",
    "\n",
    "The key is understanding Temporal's separation of concerns.\n",
    "\n",
    "1. **Non-deterministic parts** - Run arbitrary code that has the potential to fail due to external conditions\n",
    "  * Ex: Calling LLMs, accessing the file system, writing to a database.\n",
    "  * Take 1 minute to discuss with your neighbor any other examples, then we'll share answers\n",
    "2. **Deterministic parts** - Orchestrate the non-deterministic parts\n",
    "  * Ex: Branching, looping, mathematical operations, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJCi-5xlhMH3"
   },
   "source": [
    "## Consider the Following Example\n",
    "\n",
    "* Depending on the time of day, a different decision is made\n",
    "* If it's 5:00pm, it's dinner time\n",
    "* If it's 9:30am, it's breakfast time\n",
    "\n",
    "**What would happen if a user ran this application at 11:59am, it crashed and was replayed at 12:01pm? What would the user expect?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "o68o8I0BgHPA",
    "outputId": "c489afad-1422-47ee-b8e8-0c44bc21f1fa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRECiAgICBBWyJHZXQgQ3VycmVudCBUaW1lIl0gLS0+IEJbIklzIGFtIG9yIHBtPyJdCiAgICBCIC0tPiBDWyJUaW1lIGZvciBicmVha2Zhc3QiXQogICAgQiAtLT4gRFsiVGltZSBmb3IgZGlubmVyIl0K\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diagram = \"\"\"\n",
    "graph TD\n",
    "    A[\"Get Current Time\"] --> B[\"Is am or pm?\"]\n",
    "    B --> C[\"Time for breakfast\"]\n",
    "    B --> D[\"Time for dinner\"]\n",
    "\"\"\"\n",
    "render_mermaid(diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOI8Mhmhhujv"
   },
   "source": [
    "## _What Does This Have to Do with AI?_\n",
    "\n",
    "_Common Misconception_: \"Since workflows need to be deterministic, your AI agents will always behave the same way and follow identical paths.\"\n",
    "\n",
    "_Reality_: **This statement is completely wrong.**\n",
    "\n",
    "### **Determistic != predetermined**\n",
    "Deterministic means your workflow makes the same decisions when replayed with the same inputs and external responses. Your AI agent can still be dynamic and adaptive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxb6hx9bymHs"
   },
   "source": [
    "## AI Agent Reality Check\n",
    "\n",
    "**Common Fear:** \"If my workflow is deterministic, my AI agent will always do the same thing.\"\n",
    "\n",
    "**Reality:** Your agent can be completely dynamic while still being deterministic.\n",
    "\n",
    "Example: User asks \"Research best Italian restaurant in New York City.\"\n",
    "LLM returns product information → Agent follows restaurant research path\n",
    "\n",
    "**The deterministic guarantee**: If any of these workflows need to replay because of a network outage or an application crash, they will execute the exact same steps with the exact same LLM responses recorded in their history. But each scenario can produce completely different, contextually dynamic results.\n",
    "\n",
    "**Bottom line**: Determinism ensures reliability, not rigidity. Your AI remains as smart and adaptive as you design it to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQ0StPpCyteS"
   },
   "source": [
    "## AI Research Agent Examples\n",
    "\n",
    "**Each run is completely different** (dynamic), but **each individual run is reproducible** (deterministic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "Y0rH6AEijBdF",
    "outputId": "0c2dba73-454f-4635-999e-b9311e6dcef9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRECiAgICBBWyJBc2sgQUkgZm9yIGEgUGxhbiJdIC0tPiBCeyJNYWtlIFRlYSJ9CiAgICBCIC0tPiBDWyJCb2lsIFdhdGVyIl0KICAgIEMgLS0+IERbIlN0ZWVwIHRlYSJdCiAgICBEIC0tPiBFWyJSZW1vdmUgYW5kIGVuam95Il0K\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diagram = \"\"\"\n",
    "graph TD\n",
    "    A[\"Ask AI for a Plan\"] --> B{\"Make Tea\"}\n",
    "    B --> C[\"Boil Water\"]\n",
    "    C --> D[\"Steep tea\"]\n",
    "    D --> E[\"Remove and enjoy\"]\n",
    "\"\"\"\n",
    "render_mermaid(diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "id": "-p3e4gLrjqV-",
    "outputId": "a359ed1b-d9d5-42ad-dc72-db5360f0ec1c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRECiAgICBBWyJBc2sgQUkgZm9yIGEgUGxhbiJdIC0tPiBCeyJTbGF5IGEgZHJhZ29uIn0KICAgIEIgLS0+IENbIkZpbmQgdGhlIHdlYWsgc3BvdCJdCiAgICBDIC0tPiBEWyJBY3F1aXJlIHRoZSBjb3JyZWN0IHdlYXBvbiJdCiAgICBEIC0tPiBFWyJDYXJyeSBvdXQgeW91ciBhdHRhY2siXQo=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diagram = \"\"\"\n",
    "graph TD\n",
    "    A[\"Ask AI for a Plan\"] --> B{\"Slay a dragon\"}\n",
    "    B --> C[\"Find the weak spot\"]\n",
    "    C --> D[\"Acquire the correct weapon\"]\n",
    "    D --> E[\"Carry out your attack\"]\n",
    "\"\"\"\n",
    "render_mermaid(diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 587
    },
    "id": "yJicN8wkjrmo",
    "outputId": "3590255c-4693-4f5e-aac5-05f7b6f23e0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRECiAgICBBWyJBc2sgQUkgZm9yIGEgUGxhbiJdIC0tPiBCeyJXcml0ZSBDb2RlIn0KICAgIEIgLS0+IENbIkxvY2F0ZSBmaWxlcyJdCiAgICBDIC0tPiBEWyJXcml0ZSBjb2RlIl0KICAgIEQgLS0+IEVbIkV2YWx1YXRlIHJlc3VsdCJdCg==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diagram = \"\"\"\n",
    "graph TD\n",
    "    A[\"Ask AI for a Plan\"] --> B{\"Write Code\"}\n",
    "    B --> C[\"Locate files\"]\n",
    "    C --> D[\"Write code\"]\n",
    "    D --> E[\"Evaluate result\"]\n",
    "\"\"\"\n",
    "render_mermaid(diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFdMr3gjkb6y"
   },
   "source": [
    "## How Deterministic Workflows Are *Essential* for AI Workflows\n",
    "\n",
    "* The Agentic Loop: AI agents follow a repeatable pattern of reasoning and action:\n",
    "  * Evaluate goal - What am I trying to accomplish?\n",
    "  * Locate tools - What capabilities do I need to use?\n",
    "  * Execute tools - Perform the actual work (API calls, file operations, etc.)\n",
    "  * Evaluate completion - Did I achieve the goal or need to continue?\n",
    "* Tools that the LLM decides to call become **dynamic**, not **non-deterministic**.\n",
    "* **Deterministic, not predetermined**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jE-mLgDTy_I8"
   },
   "source": [
    "## Let's Make Your Agent Durable\n",
    "\n",
    "We're about to transform your simple research agent into a durable one. Here's what changes:\n",
    "\n",
    "* Your tools will become crash-proof\n",
    "* Automatic retries and recovery\n",
    "* State persistence\n",
    "* Automatic retries and recovery\n",
    "\n",
    "This results in a process such as:\n",
    "LLM Decision → Tool A → Result X (Saved in history, then on replay, same result X will result in the same next decision) → Next Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nt8y5WgdzL1e"
   },
   "source": [
    "## What stays the same\n",
    "\n",
    "* Your core logic (LLM call → PDF generation)\n",
    "* Your inputs and outputs\n",
    "* Your business requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtDXSdICK0QV"
   },
   "source": [
    "## Package Our Inputs & Outputs for Ease of Management\n",
    "\n",
    "For ease of use, evolution of parameters, and type checking, Temporal recommends passing and returing a single object from functions. `dataclass` is the recommended structure here, but anything serializable will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtTsi1b9K86m"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LLMCallInput:\n",
    "    prompt: str\n",
    "    llm_api_key: str\n",
    "    llm_model: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PDFGenerationInput:\n",
    "    content: str\n",
    "    filename: str = \"research_pdf.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whhJ6z6P1hXm"
   },
   "source": [
    "## What is an Activity?\n",
    "\n",
    "* An Activity is a function/method that is prone to failure and/or non-deterministic.\n",
    "* Temporal requires all non-deterministic code be run in an Activity\n",
    "\n",
    "Examples:\n",
    "  - External API calls - LLM requests, web scraping, database queries\n",
    "  - File system operations - Reading documents, writing reports, managing storage\n",
    "  - Network operations - HTTP requests, email sending, data transfers\n",
    "  - Resource-intensive computations - Image processing, data analysis, model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vx-Q6RTU19NU"
   },
   "source": [
    "## What Activities Give You\n",
    "\n",
    "* **Automatic retries** when external code fails\n",
    "* **Timeout handling** for slow operations and detecting failures\n",
    "* **Detailed visibility** of execution, including inputs/outputs for debugging\n",
    "* **Automatic checkpoints** - if your workflow crashes, Activities aren't re-executed. Instead, your Workflow continues from the last known good state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Duu7FpnzadyW"
   },
   "source": [
    "## Tasks/Tools become Activities\n",
    "\n",
    "To turn a function/method into an Activity, add the `@activity.defn` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doN_2Wzganj5"
   },
   "outputs": [],
   "source": [
    "from temporalio import activity\n",
    "from litellm import completion, ModelResponse\n",
    "\n",
    "\n",
    "@activity.defn\n",
    "def llm_call(input: LLMCallInput) -> ModelResponse:\n",
    "    response = completion(\n",
    "        model=input.llm_model, api_key=input.llm_api_key, messages=[{\"content\": input.prompt, \"role\": \"user\"}]\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OOx3-Y4lkkW"
   },
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.units import inch\n",
    "\n",
    "\n",
    "@activity.defn\n",
    "def create_pdf_activity(input: PDFGenerationInput) -> str:\n",
    "    print(\"Creating PDF document...\")\n",
    "\n",
    "    doc = SimpleDocTemplate(input.filename, pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    title_style = ParagraphStyle(\"CustomTitle\", parent=styles[\"Heading1\"], fontSize=24, spaceAfter=30, alignment=1)\n",
    "\n",
    "    story = []\n",
    "    title = Paragraph(\"Research Report\", title_style)\n",
    "    story.append(title)\n",
    "    story.append(Spacer(1, 20))\n",
    "    paragraphs = input.content.split(\"\\n\\n\")\n",
    "    for para in paragraphs:\n",
    "        if para.strip():\n",
    "            p = Paragraph(para.strip(), styles[\"Normal\"])\n",
    "            story.append(p)\n",
    "            story.append(Spacer(1, 12))\n",
    "\n",
    "    doc.build(story)\n",
    "\n",
    "    print(f\"SUCCESS! PDF created: {input.filename}\")\n",
    "    return input.filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gW4eZ3Q-2xF-"
   },
   "source": [
    "## Your Code\n",
    "\n",
    "**Your LLM call is now:**\n",
    "* Protected against API timeouts\n",
    "* Automatically retried with backoff\n",
    "* Observable for debugging\n",
    "\n",
    "**Your PDF generation is now:**\n",
    "* Protected against file system errors\n",
    "* Automatically retried if temporary failures\n",
    "* Tracked for completion verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KF98wlHSKnak"
   },
   "source": [
    "## Activities Are Called from Workflows\n",
    "\n",
    "- You orchestrate the execution of your Activities from within a Workflow.\n",
    "- Workflows contain the decision-making flow, but Activities perform the actual work.\n",
    "- Each Activity call is recorded in the workflow history with inputs and outputs\n",
    "- Workflows can wait for activity completion, handle failures, and make decisions based on results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763
    },
    "id": "6FB7t6qYEiN5",
    "outputId": "54267e3b-692e-4c12-bcfb-7fffa4ff2060"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRECiAgICBBWyJSZXNlYXJjaCBBZ2VudCBXb3JrZmxvdyJdIC0tPiBCWyJMTE0gQ2FsbCBBY3Rpdml0eSJdCiAgICBCIC0tPiBDWyJXb3JrZmxvdzogUHJvY2VzcyBMTE0gUmVzcG9uc2UiXQogICAgQyAtLT4gRFsiR2VuZXJhdGUgSW1hZ2UgQWN0aXZpdHkiXQogICAgRCAtLT4gRVsiV29ya2Zsb3c6IENvbWJpbmUgQ29udGVudCAmIEltYWdlIl0KICAgIEUgLS0+IEZbIlBERiBDcmVhdGlvbiBBY3Rpdml0eSJdCiAgICBGIC0tPiBHWyJXb3JrZmxvdyBDb21wbGV0ZSJdCiAgICAKICAgIHN0eWxlIEEgZmlsbDojZTFmNWZlCiAgICBzdHlsZSBCIGZpbGw6I2ZmZjNlMAogICAgc3R5bGUgRCBmaWxsOiNmZmYzZTAKICAgIHN0eWxlIEYgZmlsbDojZmZmM2UwCiAgICBzdHlsZSBDIGZpbGw6I2U4ZjVlOAogICAgc3R5bGUgRSBmaWxsOiNlOGY1ZTgK\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diagram = \"\"\"\n",
    "graph TD\n",
    "    A[\"Research Agent Workflow\"] --> B[\"LLM Call Activity\"]\n",
    "    B --> C[\"Workflow: Process LLM Response\"]\n",
    "    C --> D[\"Generate Image Activity\"]\n",
    "    D --> E[\"Workflow: Combine Content & Image\"]\n",
    "    E --> F[\"PDF Creation Activity\"]\n",
    "    F --> G[\"Workflow Complete\"]\n",
    "\n",
    "    style A fill:#e1f5fe\n",
    "    style B fill:#fff3e0\n",
    "    style D fill:#fff3e0\n",
    "    style F fill:#fff3e0\n",
    "    style C fill:#e8f5e8\n",
    "    style E fill:#e8f5e8\n",
    "\"\"\"\n",
    "render_mermaid(diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlWk6DY63Ehg"
   },
   "source": [
    "## More Input/Output Packaging\n",
    "\n",
    "Just like with Activities, Temporal recommends passing a single object to the Workflow for input and returning a single object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mesIIXMMkNG"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GenerateReportInput:\n",
    "    prompt: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GenerateReportOutput:\n",
    "    result: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qEvPXcHOn80"
   },
   "source": [
    "## Creating the Workflow\n",
    "\n",
    "* Activities are orchestrated within a Temporal Workflow.\n",
    "* Workflows must **not** make API calls, file system calls, or anything non-deterministic. That is what Activities are for.\n",
    "* Workflows are async, and you define them as a class decorated with the `@workflow.defn` decorator.\n",
    "* Every Workflow has a **single** entry point, which is an `async` method decorated with `@workflow.run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tq0aUW3OQkS"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "\n",
    "from temporalio import workflow\n",
    "\n",
    "\n",
    "# sandboxed=False is a Notebook only requirement. You normally don't do this\n",
    "@workflow.defn(sandboxed=False)\n",
    "class GenerateReportWorkflow:\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "        llm_call_input = LLMCallInput(prompt=input.prompt, llm_api_key=LLM_API_KEY, llm_model=LLM_MODEL)\n",
    "\n",
    "        research_facts = await workflow.execute_activity(\n",
    "            llm_call,\n",
    "            llm_call_input,\n",
    "            start_to_close_timeout=timedelta(seconds=30),\n",
    "        )\n",
    "\n",
    "        workflow.logger.info(\"Research complete!\")\n",
    "\n",
    "        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "        pdf_filename = await workflow.execute_activity(\n",
    "            create_pdf_activity,\n",
    "            pdf_generation_input,\n",
    "            start_to_close_timeout=timedelta(seconds=10),\n",
    "        )\n",
    "\n",
    "        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0gKu3eSQxbO"
   },
   "source": [
    "## Running a Worker\n",
    "\n",
    "* Temporal Workflows are run on Workers\n",
    "* Workers wait for tasks to do, such as an Activity or Workflow Task, and execute them.\n",
    "* Workers find tasks by listenting on a Task Queue\n",
    "* Workers have Workflows and Activities registered to them so the Worker knows what to execute.\n",
    "* This makes the execution of work indirect; _any_ Worker can pick up a registered Workflow or Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZB5mZ3HxOk6u"
   },
   "outputs": [],
   "source": [
    "from temporalio.client import Client\n",
    "from temporalio.worker import Worker\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "async def run_worker() -> None:\n",
    "    # Create client connected to server at the given address\n",
    "    client = await Client.connect(\"localhost:7233\", namespace=\"default\")\n",
    "\n",
    "    # Run the Worker\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as activity_executor:\n",
    "        worker = Worker(\n",
    "            client,\n",
    "            task_queue=\"research\",  # the task queue the Worker is polling\n",
    "            workflows=[GenerateReportWorkflow],  # register the Workflow\n",
    "            activities=[llm_call, create_pdf_activity],  # register the Activities\n",
    "            activity_executor=activity_executor,\n",
    "        )\n",
    "\n",
    "        print(f\"Starting the worker....\")\n",
    "        await worker.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8-AKv5nOYmA"
   },
   "source": [
    "## Running a Temporal Service\n",
    "\n",
    "* The Temporal Service brings it all together\n",
    "* The Temporal Service can be run locally, self-hosted, or you can use Temporal Cloud\n",
    "* The service acts as the supervisor of your Workflows, Activities, and everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfMzAaJ2Sfeh"
   },
   "outputs": [],
   "source": [
    "# Start the Temporal Dev Server\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "command = \"/root/.temporalio/bin/temporal server start-dev --ui-port 8000\"\n",
    "temporal_server = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, preexec_fn=os.setsid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syr9AZyG1mAP"
   },
   "outputs": [],
   "source": [
    "# Uncomment to kill the dev server\n",
    "# Use this if you need to restart the Temporal Service\n",
    "# Kill the Temporal Dev Server\n",
    "\n",
    "# import signal\n",
    "\n",
    "# os.killpg(os.getpgid(temporal_server.pid), signal.SIGTERM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhjgaQBbNZum"
   },
   "source": [
    "## Starting the Worker\n",
    "\n",
    "* A Workflow can't execute if a Worker isn't running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FB4oDvu7Nbwm"
   },
   "outputs": [],
   "source": [
    "# Due to the limitation of Jupyter Notebooks and Google Collab, this is how\n",
    "# you must start the worker in a Notebook environment\n",
    "worker = asyncio.create_task(run_worker())\n",
    "\n",
    "\n",
    "# If you are running this code in a typical Python environment, you can start\n",
    "# the Worker by just calling `asyncio.run`\n",
    "# if __name__ == \"__main__\":\n",
    "#    asyncio.run(run_worker())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZiLuWj_Q0XR"
   },
   "source": [
    "## Executing the Workflow\n",
    "\n",
    "* Temporal Workflows are executed indirectly\n",
    "* You **don't** just execute the file, you request execution from the Temporal Service\n",
    "* You do this using a Temporal Client\n",
    "* In the client you specfiy the Workflow to run, the data, a Workflow ID to identify the execution, and the Task Queue to request on\n",
    "  * This Task Queue **must exactly match** the Task Queue specified in the Worker\n",
    "* Workflows can be started asynchonously or synchronously\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QTOOaYd8UnBL",
    "outputId": "debd6278-447b-4f78-9392-cfe699b87399"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Research Report Generator!\n",
      "Enter your research topic or question: Give me 2 facts about dogs\n",
      "Started workflow. Workflow ID: generate-research-report-workflow, RunID 0198f418-8507-7624-bdce-159d39f17fd9\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from temporalio.client import Client\n",
    "\n",
    "# Create client connected to server at the given address\n",
    "client = await Client.connect(\"localhost:7233\", namespace=\"default\")\n",
    "\n",
    "print(\"Welcome to the Research Report Generator!\")\n",
    "prompt = input(\"Enter your research topic or question: \").strip()\n",
    "\n",
    "if not prompt:\n",
    "    prompt = \"Give me 5 fun and fascinating facts about tardigrades. Make them interesting and educational!\"\n",
    "    print(f\"No prompt entered. Using default: {prompt}\")\n",
    "\n",
    "# Asynchronous start of a Workflow\n",
    "handle = await client.start_workflow(\n",
    "    GenerateReportWorkflow.run,\n",
    "    GenerateReportInput(prompt=prompt),\n",
    "    id=\"generate-research-report-workflow\",  # user-defined Workflow identifier, which typically has some business meaning\n",
    "    task_queue=\"research\",  # the task-queue that your Worker is polling\n",
    ")\n",
    "\n",
    "print(f\"Started workflow. Workflow ID: {handle.id}, RunID {handle.result_run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvM082EAtXZv"
   },
   "source": [
    "## Getting the Result\n",
    "\n",
    "The example above uses async execution. You can `await` the handle to get the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VODi5uGZAj3j",
    "outputId": "a345db2e-3923-4cbb-d297-22d9d94ee86f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PDF document...\n",
      "SUCCESS! PDF created: research_pdf.pdf\n",
      "Result: GenerateReportOutput(result='Successfully created research report PDF: research_pdf.pdf')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/temporalio/converter.py:573: UserWarning: If you're using Pydantic v2, use temporalio.contrib.pydantic.pydantic_data_converter. If you're using Pydantic v1 and cannot upgrade, refer to https://github.com/temporalio/samples-python/tree/main/pydantic_converter_v1 for better v1 support.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get the result\n",
    "result = await handle.result()\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4gwjNk6wx-L"
   },
   "source": [
    "## Temporal Web UI\n",
    "\n",
    "- Temporal provides a robust Web UI for managing Workflow Executions\n",
    "- Can gain insights like responses from Activities, execution time, and failures\n",
    "- Great for debugging and understanding what's happening during your Workflow Executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "qc120QlcSiom",
    "outputId": "930674db-6c43-4eb5-e0b3-3902f3e53ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://8000-m-s-38fbof756iukf-a.asia-east1-0.prod.colab.dev\n"
     ]
    }
   ],
   "source": [
    "# Get the Temporal Web UI URL\n",
    "from google.colab.output import eval_js\n",
    "\n",
    "print(eval_js(\"google.colab.kernel.proxyPort(8000)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsOEZDrgGefo"
   },
   "source": [
    "## Exploring the Web UI\n",
    "\n",
    "Can you locate the following items on the Web UI?\n",
    "\n",
    "- The name of the Task Queue\n",
    "- The name of the two Activities called\n",
    "- The inputs and outputs of the called Activities\n",
    "- Input and output of the Workflow Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZIR7OS4Nk-U"
   },
   "source": [
    "## Simulating Failure\n",
    "\n",
    "What happens if the Worker process were to crash during execution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stu--jIeNwUY"
   },
   "source": [
    "## Adding a Durable Timer\n",
    "\n",
    "- Timers introduce delays in your Workflow with guaranteed execution.\n",
    "- Durable timers will fire even if there is no Worker running, and persists despite restarts and infrastructure failures\n",
    "- Let's add one to the Workflow to give us time to kill the Worker in the middle of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbkxjA4TNvXm"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "\n",
    "from temporalio import workflow\n",
    "\n",
    "\n",
    "# sandboxed=False is a Notebook only requirement. You normally don't do this\n",
    "@workflow.defn(sandboxed=False)\n",
    "class GenerateReportWorkflow:\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "        llm_call_input = LLMCallInput(prompt=input.prompt, llm_api_key=LLM_API_KEY, llm_model=LLM_MODEL)\n",
    "\n",
    "        research_facts = await workflow.execute_activity(\n",
    "            llm_call,\n",
    "            llm_call_input,\n",
    "            start_to_close_timeout=timedelta(seconds=30),\n",
    "        )\n",
    "\n",
    "        workflow.logger.info(\"Research complete!\")\n",
    "\n",
    "        # Adding a Timer here to pause the Workflow Execution\n",
    "        await workflow.sleep(timedelta(seconds=20))\n",
    "\n",
    "        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "        pdf_filename = await workflow.execute_activity(\n",
    "            create_pdf_activity,\n",
    "            pdf_generation_input,\n",
    "            start_to_close_timeout=timedelta(seconds=10),\n",
    "        )\n",
    "\n",
    "        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEMJDMaYOpek"
   },
   "source": [
    "## Restart the Worker\n",
    "\n",
    "- After a Workflow change, you must restart the Worker for the change to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "miEibF-kT5Ap",
    "outputId": "5220f30b-c168-4d7d-f1a5-101f0b7022ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker killed\n"
     ]
    }
   ],
   "source": [
    "# Run this to kill the current Worker\n",
    "x = worker.cancel()\n",
    "\n",
    "if x:\n",
    "    print(\"Worker killed\")\n",
    "else:\n",
    "    print(\"Worker was not running. Nothing to kill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2AlPpWHBOx4-",
    "outputId": "5e4036ca-ea7b-479f-aba3-8dc36bcead70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task is currently active.\n"
     ]
    }
   ],
   "source": [
    "# Starting the Worker again\n",
    "worker = asyncio.create_task(run_worker())\n",
    "\n",
    "# Check if the task is in the set of all tasks\n",
    "if worker in asyncio.all_tasks():\n",
    "    # The sleep is necessary because of the async task scheduling in Jupyter\n",
    "    print(\"Task is currently active.\")  # The Worker now registers the updated Workflow changes\n",
    "else:\n",
    "    print(\"Task is not found in active tasks (might have finished or not yet scheduled).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfGUkFh_O4wI"
   },
   "source": [
    "## Start the Workflow and Simulate an Error\n",
    "\n",
    "Start the Workflow again, prompt the LLM, wait about ~8 seconds to let the `llm_call` Activity complete, then kill the Worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9U5eLhjPFzj",
    "outputId": "247cea10-54cd-4912-f7de-d366512f034e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Research Report Generator!\n",
      "Enter your research topic or question: Give me 2 facts about sharks\n",
      "Started workflow. Wait about ~10 seconds to let the first Activity (llm_call) to complete, then kill the Worker.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "client = await Client.connect(\"localhost:7233\", namespace=\"default\")\n",
    "\n",
    "print(\"Welcome to the Research Report Generator!\")\n",
    "prompt = input(\"Enter your research topic or question: \").strip()\n",
    "\n",
    "if not prompt:\n",
    "    prompt = \"Give me 5 fun and fascinating facts about tardigrades. Make them interesting and educational!\"\n",
    "    print(f\"No prompt entered. Using default: {prompt}\")\n",
    "\n",
    "# Asynchronous start of a Workflow\n",
    "handle = await client.start_workflow(\n",
    "    GenerateReportWorkflow.run,\n",
    "    GenerateReportInput(prompt=prompt),\n",
    "    id=\"generate-research-report-workflow\",\n",
    "    task_queue=\"research\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Started workflow. Wait about ~8 seconds to let the first Activity (llm_call) to complete, then kill the Worker.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hFrX4DPmxYrc",
    "outputId": "679d461c-181a-4cf0-a9a2-4877aa9bfaf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker killed\n"
     ]
    }
   ],
   "source": [
    "# After about 8 seconds, run this to kill the current Worker\n",
    "x = worker.cancel()\n",
    "\n",
    "if x:\n",
    "    print(\"Worker killed\")\n",
    "else:\n",
    "    print(\"Worker was not running. Nothing to kill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0xnVjylPj6k"
   },
   "source": [
    "## Watch the Progress in the Web UI\n",
    "\n",
    "Go to the Web UI and watch the progress. What do you observe? Does the Timer complete despite the Worker being kiled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "q-EVnqlwPrWy",
    "outputId": "940542a8-b480-4afa-b918-5a4cc9d2dd15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://8000-m-s-38fbof756iukf-a.asia-east1-0.prod.colab.dev\n"
     ]
    }
   ],
   "source": [
    "# Get the Temporal Web UI URL\n",
    "from google.colab.output import eval_js\n",
    "\n",
    "print(eval_js(\"google.colab.kernel.proxyPort(8000)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99gwelS1P4iz"
   },
   "source": [
    "## Restart the Worker to Resume Execution\n",
    "\n",
    "- Restart the Worker and return to the WebUI.\n",
    "  * What do you think will happen? *\n",
    "- You will see the Workflow pick up where it left off as if nothing happened!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6czqh6bP-u5"
   },
   "outputs": [],
   "source": [
    "# Starting the Worker again\n",
    "worker = asyncio.create_task(run_worker())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbw85OliybvN"
   },
   "source": [
    "---\n",
    "# Exercise 2 - Adding Durability\n",
    "\n",
    "* In these exercises you will:\n",
    "  * Transform your LLM calls and your execution of tools to Activities\n",
    "  * Use a Temporal Workflow to orchestrate your Activities\n",
    "  * Observe how Temporal handles your errors\n",
    "  * Debug your error and observe your Workflow Execution successfully complete\n",
    "* Go to the **Exercise** Directory in the Google Drive and open the **Practice** Directory\n",
    "* Open _02-Adding-Durability.ipynb_ and follow the instructions\n",
    "* If you get stuck, raise your hand and someone will come by and help. You can also check the `Solution` directory for the answers\n",
    "* **You have 5 mins**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}