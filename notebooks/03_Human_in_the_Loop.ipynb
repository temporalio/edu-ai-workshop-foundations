{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42Spdn4_DuBH"
   },
   "source": [
    "# Human in the Loop\n",
    "\n",
    "Your durable research application now survives crashes and automatically retries failures. But there's a critical gap: **it runs completely autonomously**.\n",
    "\n",
    "Imagine this scenario: Your AI generates research on \"best travel spots for summer 2025\", your application generates a PDF, and sends it to your client—all automatically.\n",
    "\n",
    "Then your client calls: \"I want cheaper options!\"\n",
    "\n",
    "Your application had no way to pause and ask for clarification or approval. It just executed straight through to completion.\n",
    "\n",
    "**Real-world AI applications need human interaction**—to provide feedback, approve decisions, or clarify requirements. But adding human input to automated workflows introduces new challenges: What if the user's browser crashes while they're reviewing? What if they close the tab and come back later? How do you prevent losing expensive LLM work while waiting for approval?\n",
    "\n",
    "In this section, we'll solve these problems by making your AI application interactive and durable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most GenAI Apps Still Engage Humans\n",
    "\n",
    "While some AI applications may operate entirely autonomously, many require human intervention. They may provide input on launch or at various points throughout the execution.\n",
    "\n",
    "Examples:\n",
    "  - Validation at critical decision points\n",
    "  - Final review before implementation\n",
    "  - Feedback loops\n",
    "\n",
    "This can happen in a chain workflow like if the human wants to confirm the research before outputting it into a PDF:\n",
    "\n",
    "<img src=\"https://images.ctfassets.net/0uuz8ydxyd9p/70SBemKQHnqfLxoHgPovQX/33f3a0b6cfc96eae2d17d1a463079560/Screenshot_2025-07-08_at_10.26.26%C3%A2__AM.png\" />\n",
    "\n",
    "\n",
    "Or there can be continuous feedback cycle in a human-in-the-loop system. The AI agent performs work based on input or previous feedback, then pauses to wait for human review. This feedback is fed back into the AI agent, which then continues execution based on the human's decision:\n",
    "\n",
    "<img src=\"https://i.postimg.cc/43pm5mfw/hitl-loop.png\" width=\"300\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Customer Support Agent with Human Oversight\n",
    "\n",
    "The Scenario:\n",
    "Customer contacts support: \"My order hasn't arrived and I need it urgently\"\n",
    "\n",
    "**How Human Interaction Works:**\n",
    "\n",
    "1. **Initial Input** - Customer provides problem; human sets constraints like maximum refund amounts or escalation thresholds\n",
    "\n",
    "2. **AI Analysis** - LLM checks order status and finds \"delayed in transit,\" then proposes: \"Process a $75 expedited replacement\"\n",
    "\n",
    "3. **Human Decision Point** - System pauses for approval. Human can:\n",
    "   - Approve the proposed action\n",
    "   - Modify the approach (\"Offer refund instead\")\n",
    "   - Add context (\"VIP customer - upgrade to overnight\")\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Cost control** - Expensive decisions require human approval\n",
    "- **Security** - Prevent unauthorized refunds or account changes\n",
    "- **Risk mitigation** - Human judgment for edge cases and exceptions\n",
    "- **Compliance** - Legal/regulatory requirements for certain actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges in Non-Durable Human in the Loop Processes\n",
    "\n",
    "While human interaction points are valuable for AI applications, implementing them reliably presents significant technical challenges. Without durable execution, human input can be lost during system failures, leading to unpredictable behavior.\n",
    "\n",
    "**Consider the following scenario**\n",
    "\n",
    "- A user needs to approve a transaction.\n",
    "- As they are doing this, the website goes down.\n",
    "- How do you mitigate this?\n",
    "  - Do we notify the user to approve the payment again? (creating confusion since the user already 'approved')\n",
    "  - Do we assume approval and risk processing an unauthorized payment?\n",
    "\n",
    "Without durable processes, you're forced to choose between security, user experience, and reliability.\n",
    "\n",
    "<img src=\"https://i.postimg.cc/C5G3rpBy/Screenshot-2025-10-08-at-9-19-32-AM.png\" />\n",
    "\n",
    "#### It's **distributed system challenges** all over again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But with **Durable Execution**, the Workflow LIVES Through These Interactions!\n",
    "\n",
    "With Temporal's durable execution, the workflow instance **persists throughout the entire human interaction**:\n",
    "\n",
    "**What this means in practice:**\n",
    "- **User's approval is durably stored** - When a user clicks \"approve\", that decision is saved in the workflow history\n",
    "- **No re-approval needed** - If the website crashes after approval, the workflow resumes with the approval already recorded\n",
    "- **Automatic recovery** - System failures don't lose progress; the workflow picks up exactly where it left off\n",
    "- **User can walk away** - Close the browser, shut down the laptop—the workflow continues running on the server\n",
    "\n",
    "**The key insight:** Instead of managing complex coordination between services, queues, and databases to handle human input, you write straightforward code that waits for human decisions. Temporal handles all the reliability, state management, and recovery automatically.\n",
    "\n",
    "<img src=\"https://i.postimg.cc/bYR8R1QC/hitl-durable-execution.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signals in Temporal\n",
    "\n",
    "In Temporal, human interaction in Temporal systems is achieved through a [Temporal Signal](https://docs.temporal.io/develop/python/message-passing#signals).\n",
    "\n",
    "A Signal is a:\n",
    "* Message sent asynchronously to a running Workflow Execution\n",
    "* Used to to change the state and control the flow of a Workflow Execution.\n",
    "\n",
    "*Take 2 minutes to discuss with your neighbor when we might use a Signal, then we'll share answers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Signal Usage\n",
    "\n",
    "1. The user initiates a workflow with an initial request\n",
    "2. The workflow processes the request and determines what information or approval is needed\n",
    "3. The workflow pauses and waits for user input via Signal, such as:\n",
    "    - Additional information or clarification\n",
    "    - Permission to proceed with an action\n",
    "    - Selection between multiple options\n",
    "4. The user sends a Signal with their response\n",
    "5. The workflow resumes execution based on the Signal received\n",
    "6. Steps repeat as needed until the workflow completes its task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Durably Storing Human Interactions\n",
    "\n",
    "Let's go back to the user approving a payment example now. With Temporal, when the user clicks \"approve\" in the finance portal, the approval decision gets durably stored.\n",
    "\n",
    "The user can close the browser, go to lunch, and the Workflow will continue running in the background.\n",
    "\n",
    "If the payment gateway times out, returns an error or becomes unavailable, Temporal automatically retries the payment step. IT does not need to re-ask the user for approval, because that decision is already durably stored in the Workflow state.\n",
    "\n",
    "- **No duplicate work** (user does not have to re-approve the same expense)\n",
    "- **No manual intervention** (does not need to manually reconcile failed payments or investigate whether an expense was actually approved)\n",
    "- **Reliable processing** (business can count on approved expenses being paid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Implement a Signal!\n",
    "\n",
    "Let's update our application. After receiving a Signal, your Workflow's main execution logic must evaluate the Signal data and determine the appropriate response. This is where the Workflow's business logic intersects with human input.\n",
    "\n",
    "1. We will now give the user the opportunity to either \n",
    "    - Accept the research results from the LLM, or\n",
    "    - Refine the output with an additional prompt\n",
    "2. So we'll loop\n",
    "\n",
    "<img src=\"https://i.postimg.cc/BZPYWhHZ/signal-goal.png\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo (Expand for instructor notes or to run on your own)\n",
    "<!--\n",
    "1. Clone this repository: `https://github.com/temporalio/edu-ai-workshop-agentic-loop`. The instructions will also be in the README.\n",
    "2. From the `src/module_one_03_human_in_the_loop` directory, run `app.py` with `python app.py`.\n",
    "3. In one terminal window, run your Worker with `python worker.py`.\n",
    "4. In another terminal window, execute your Workflow with `python starter.py`.\n",
    "5. You'll be prompted to enter a research topic or question in the CLI.\n",
    "6. Once you do, you'll be prompted with the ability to Signal or Query the Workflow.\n",
    "7. Type 'query' and you'll see the output in the terminal window where you started your Workflow Execution.\n",
    "8. Time to demonstrate Signals. Back in the terminal window when you started your Workflow Execution, you'll see that you are prompted to choose one of the two options:\n",
    "    a. Approve of this research and if you would like it to create a PDF (type 'keep' to send a Signal to the Workflow to create the PDF).\n",
    "    b. Modify the research by adding extra info to the prompt (type 'edit' to modify the prompt and send another Signal to the Workflow to prompt the LLM again).\n",
    "9. Demonstrate the modification by typing `edit`.\n",
    "10. Enter additional instructions (e.g.: \"turn this into a poem\") and see the new output in the terminal window by typing `query` again.\n",
    "11. Finally, show that you can keep changing the execution path of your Workflow Execution by typing `keep`. Show that the PDF has appeared in your `module_one_03_human_in_the_loop` directory.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Notebook\n",
    "\n",
    "Run the following code blocks to install various packages and tools necessary to run this notebook\n",
    "\n",
    "**Be sure to add your .env file again. It doesn't persist across notebooks or sesions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll first install the necessary packages for this workshop.\n",
    "\n",
    "%pip install --quiet temporalio litellm reportlab python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .env file\n",
    "with open(\".env\", \"w\") as fh:\n",
    "  fh.write(\"LLM_API_KEY = YOUR_API_KEY\\nLLM_MODEL = openai/gpt-4o\")\n",
    "\n",
    "  # Now open the file and replace YOUR_API_KEY with your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables and configure LLM settings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Get LLM_API_KEY environment variable and print it to make sure that your .env file is properly loaded.\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"openai/gpt-4o\")\n",
    "LLM_API_KEY = os.getenv(\"LLM_API_KEY\", None)\n",
    "print(\"LLM API Key\", LLM_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This allows us to run the Temporal Asyncio event loop within the event loop of Jupyter Notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Running this will download the Temporal CLI\n",
    "\n",
    "!curl -sSf https://temporal.download/cli.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Sure Your Temporal Web UI is Running\n",
    "\n",
    "1. You should have the Temporal Server running in your terminal (run `temporal server start-dev` if not).\n",
    "2. Then in your `Ports` tab on the bottom of this screen, find `8233` and click on the Globe icon to open the Temporal Web UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these code blocks to load our dataclasses and Activities (from the previous workshop) into the program\n",
    "from dataclasses import dataclass\n",
    "from temporalio import activity\n",
    "from litellm import completion, ModelResponse\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "\n",
    "# Dataclasses\n",
    "\n",
    "@dataclass\n",
    "class LLMCallInput:\n",
    "  prompt: str\n",
    "  llm_api_key: str\n",
    "  llm_model: str\n",
    "\n",
    "@dataclass\n",
    "class PDFGenerationInput:\n",
    "  content: str\n",
    "  filename: str = \"research_pdf.pdf\"\n",
    "\n",
    "@dataclass\n",
    "class GenerateReportInput:\n",
    "    prompt: str\n",
    "    llm_api_key: str\n",
    "    llm_research_model: str = \"openai/gpt-4o\"\n",
    "    llm_image_model: str = \"dall-e-3\"\n",
    "\n",
    "@dataclass\n",
    "class GenerateReportOutput:\n",
    "    result: str\n",
    "\n",
    "@activity.defn\n",
    "def llm_call(input: LLMCallInput) -> ModelResponse:\n",
    "    response = completion(\n",
    "      model=input.llm_model,\n",
    "      api_key=input.llm_api_key,\n",
    "      messages=[{ \"content\": input.prompt,\"role\": \"user\"}]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Activities\n",
    "\n",
    "@activity.defn\n",
    "def create_pdf(input: PDFGenerationInput) -> str:\n",
    "    print(\"Creating PDF document...\")\n",
    "\n",
    "    doc = SimpleDocTemplate(input.filename, pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    title_style = ParagraphStyle(\n",
    "        'CustomTitle',\n",
    "        parent=styles['Heading1'],\n",
    "        fontSize=24,\n",
    "        spaceAfter=30,\n",
    "        alignment=1\n",
    "    )\n",
    "\n",
    "    story = []\n",
    "    title = Paragraph(\"Research Report\", title_style)\n",
    "    story.append(title)\n",
    "    story.append(Spacer(1, 20))\n",
    "    paragraphs = input.content.split('\\n\\n')\n",
    "    for para in paragraphs:\n",
    "        if para.strip():\n",
    "          p = Paragraph(para.strip(), styles['Normal'])\n",
    "          story.append(p)\n",
    "          story.append(Spacer(1, 12))\n",
    "\n",
    "    doc.build(story)\n",
    "\n",
    "    print(f\"SUCCESS! PDF created: {input.filename}\")\n",
    "    return input.filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Create Our Loop\n",
    "\n",
    "After generating the research, and before creating a PDF report, the user can review the research. The user has two choices to send to the Workflow:\n",
    "\n",
    "- Keep the research and generate a PDF report by signaling \"keep\"\n",
    "- Modify the research before generating a PDF report by signaling \"edit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Set the condition to check if the user decision is `UserDecision.EDIT`\n",
    "\n",
    "from temporalio import workflow\n",
    "from datetime import timedelta\n",
    "\n",
    "# sandboxed=False is a Notebook only requirement. You normally don't do this\n",
    "@workflow.defn(sandboxed=False)\n",
    "class GenerateReportWorkflow:\n",
    "    def __init__(self) -> None:\n",
    "        self._current_prompt: str = \"\"\n",
    "\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "        self._current_prompt = input.prompt\n",
    "\n",
    "    llm_call_input = LLMCallInput(\n",
    "        prompt=self._current_prompt,\n",
    "        llm_api_key=input.llm_api_key,\n",
    "        llm_model=input.llm_research_model,\n",
    "    )\n",
    "    \n",
    "    # Continue looping until the user approves the research\n",
    "    continue_agent_loop = True\n",
    "\n",
    "    # Execute the LLM call to generate research based on the current prompt\n",
    "    while continue_agent_loop:\n",
    "        research_facts = await workflow.execute_activity(\n",
    "            llm_call,\n",
    "            llm_call_input,\n",
    "            start_to_close_timeout=timedelta(seconds=30),\n",
    "        )\n",
    "\n",
    "        # User approved the research - exit the loop and proceed to PDF generation\n",
    "        if self._user_decision.decision == UserDecision.KEEP:\n",
    "            workflow.logger.info(\"User approved the research. Creating PDF...\")\n",
    "            continue_agent_loop = False\n",
    "        # User wants to edit the research - update the prompt and loop again\n",
    "        elif self._user_decision.decision == \"\": # TODO: Set the condition to check if the user decision is `EDIT`\n",
    "            workflow.logger.info(\"User requested research modification.\")\n",
    "            if self._user_decision.additional_prompt != \"\":\n",
    "                # Append the user's additional instructions to the existing prompt\n",
    "                self._current_prompt = (\n",
    "                    f\"{self._current_prompt}\\n\\nAdditional instructions: {self._user_decision.additional_prompt}\"\n",
    "                )\n",
    "            else:\n",
    "                workflow.logger.info(\"No additional instructions provided. Regenerating with original prompt.\")\n",
    "            # Update the Activity input with the modified prompt for the next iteration\n",
    "            llm_call_input.prompt = self._current_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load and display the solution\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "notebook_dir = Path(os.getcwd())\n",
    "solution_file = notebook_dir / \"Solutions_03_Human_in_the_Loop\" / \"loop_solution.py\"\n",
    "\n",
    "code = solution_file.read_text()\n",
    "\n",
    "print(\"Solution loaded:\")\n",
    "display(Markdown(f\"```python\\n{code}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Implement a Signal!\n",
    "\n",
    "We have now added the Workflow main logic that checks the state and reacts accordingly! Now, we need to do three things:\n",
    "\n",
    "1. **Define the structure for the data being supplied by the user** \n",
    "2. **Implement the point(s) in the flow where user input is sought**          \n",
    "3. **Implement a Signal handler that is called when the user supplies the information.**\n",
    "    - Implement a Signal handler method decorated with `@workflow signal`\n",
    "    - This signal handler will write into the above defined data structure\n",
    "\n",
    "Let's first look at storing the Signal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Create a Model\n",
    "\n",
    "- Create a model for the Signal to be stored in\n",
    "- Similar to Activities and Workflows, `dataclasses` are recommended here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This defines a string enum and a dataclass for handling user decisions in a Temporal workflow.\n",
    "# We will send a `UserDecision` as a Signal to our Research Workflow letting the Workflow know if \n",
    "# we want to keep or edit the research or if we want to wait for further decision\n",
    "\n",
    "# Step 1: Set the additional prompt to be a string that default as an empty string\n",
    "# Step 2: Run this code block\n",
    "from dataclasses import dataclass\n",
    "from enum import StrEnum\n",
    "\n",
    "class UserDecision(StrEnum):\n",
    "    KEEP = \"KEEP\"\n",
    "    EDIT = \"EDIT\"\n",
    "    WAIT = \"WAIT\"\n",
    "    \n",
    "@dataclass\n",
    "class UserDecisionSignal: # A data structure to send user decisions via Temporal Signals\n",
    "    decision: UserDecision\n",
    "    additional_prompt: # TODO Set this to be a string that defaults as an empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load and display the solution\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "notebook_dir = Path(os.getcwd())\n",
    "solution_file = notebook_dir / \"Solutions_03_Human_in_the_Loop\" / \"model_solution.py\"\n",
    "\n",
    "code = solution_file.read_text()\n",
    "\n",
    "print(\"Solution loaded:\")\n",
    "display(Markdown(f\"```python\\n{code}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Storing the Signal State\n",
    "\n",
    "Remember, the workflow needs a place to remember what Signals it has received.\n",
    "\n",
    "- Use instance variables to persist signal data across Workflow Execution\n",
    "- Can be a simple variable, or a Queue for handling many Signals\n",
    "- Initialize with default values that indicates \"no Signal received yet\". In the example above, `WAIT` is the default state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Set the instance variable of the user decision to default to WAIT\n",
    "from datetime import timedelta\n",
    "from temporalio import workflow\n",
    "\n",
    "# sandboxed=False is a Notebook only requirement. You normally don't do this\n",
    "@workflow.defn(sandboxed=False)\n",
    "class GenerateReportWorkflow:\n",
    "    def __init__(self) -> None:\n",
    "        self._current_prompt: str = \"\"\n",
    "        # Instance variable to store Signal data\n",
    "        self._user_decision: UserDecisionSignal = UserDecisionSignal(decision=UserDecision.____) # TODO Set the default state of the user decision to be WAIT\n",
    "\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "        self._current_prompt = input.prompt\n",
    "\n",
    "        llm_call_input = LLMCallInput(\n",
    "            prompt=self._current_prompt,\n",
    "            llm_api_key=input.llm_api_key,\n",
    "            llm_model=input.llm_research_model,\n",
    "        )\n",
    "\n",
    "        # Continue looping until the user approves the research\n",
    "        continue_agent_loop = True\n",
    "\n",
    "        # Execute the LLM call to generate research based on the current prompt\n",
    "        while continue_agent_loop:\n",
    "            research_facts = await workflow.execute_activity(\n",
    "                llm_call,\n",
    "                llm_call_input,\n",
    "                start_to_close_timeout=timedelta(seconds=30),\n",
    "            )\n",
    "\n",
    "            # User approved the research - exit the loop and proceed to PDF generation\n",
    "            if self._user_decision.decision == UserDecision.KEEP:\n",
    "                workflow.logger.info(\"User approved the research. Creating PDF...\")\n",
    "                continue_agent_loop = False\n",
    "            # User wants to edit the research - update the prompt and loop again\n",
    "            elif self._user_decision.decision == UserDecision.EDIT:\n",
    "                workflow.logger.info(\"User requested research modification.\")\n",
    "                if self._user_decision.additional_prompt != \"\":\n",
    "                    # Append the user's additional instructions to the existing prompt\n",
    "                    self._current_prompt = (\n",
    "                        f\"{self._current_prompt}\\n\\nAdditional instructions: {self._user_decision.additional_prompt}\"\n",
    "                    )\n",
    "                else:\n",
    "                    workflow.logger.info(\"No additional instructions provided. Regenerating with original prompt.\")\n",
    "                # Update the Activity input with the modified prompt for the next iteration\n",
    "                llm_call_input.prompt = self._current_prompt\n",
    "\n",
    "        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "        pdf_filename: str = await workflow.execute_activity(\n",
    "            create_pdf,\n",
    "            pdf_generation_input,\n",
    "            start_to_close_timeout=timedelta(seconds=20),\n",
    "        )\n",
    "\n",
    "        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load and display the solution\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "notebook_dir = Path(os.getcwd())\n",
    "solution_file = notebook_dir / \"Solutions_03_Human_in_the_Loop\" / \"store_signal_solution.py\"\n",
    "\n",
    "code = solution_file.read_text()\n",
    "\n",
    "print(\"Solution loaded:\")\n",
    "display(Markdown(f\"```python\\n{code}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Signal Handler\n",
    "- A Signal is defined in your code and handled in your Workflow Definition\n",
    "- A given Workflow Definition can support multiple Signals\n",
    "- To define a Signal, set the Signal decorator `@workflow.signal` on the Signal function inside your Workflow class\n",
    "- Signal methods define what happens when the Signal is received\n",
    "\n",
    "In this example, when a client sends a Signal, this handler receives the UserDecisionSignal data and updates the `self._user_decision` in the Workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Add the `@workflow.signal` decorator above the `user_decision_signal` method\n",
    "# Step 2: Update the instance variable `self._user_decision` by setting it to `decision_data`\n",
    "# Reflection Question: What's still missing for this Signal implementation to work properly?\n",
    "from temporalio import workflow\n",
    "\n",
    "@workflow.defn(sandboxed=False) # sandboxed=False is a Notebook only requirement. You normally don't do this)\n",
    "class GenerateReportWorkflow:\n",
    "    def __init__(self) -> None:\n",
    "        self._current_prompt: str = \"\"\n",
    "        self._user_decision: UserDecisionSignal = UserDecisionSignal(decision=UserDecision.WAIT) # UserDecision Signal starts with WAIT as the default state\n",
    "\n",
    "    # TODO Define the Signal handler with @workflow.signal\n",
    "    async def user_decision_signal(self, decision_data: UserDecisionSignal) -> None:\n",
    "        self._user_decision = # Update instance variable when Signal is received to decision_data\n",
    "\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "      self._current_prompt = input.prompt\n",
    "\n",
    "      llm_call_input = LLMCallInput(\n",
    "          prompt=self._current_prompt,\n",
    "          llm_api_key=input.llm_api_key,\n",
    "          llm_model=input.llm_research_model,\n",
    "      )\n",
    "      # rest of code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load and display the solution\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "notebook_dir = Path(os.getcwd())\n",
    "solution_file = notebook_dir / \"Solutions_03_Human_in_the_Loop\" / \"signal_handler_solution.py\"\n",
    "\n",
    "code = solution_file.read_text()\n",
    "\n",
    "print(\"Solution loaded:\")\n",
    "display(Markdown(f\"```python\\n{code}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waiting for a Signal\n",
    "Now that we have a Signal handler to receive data, we need a way for the Workflow to pause and wait for that Signal to arrive. This is where workflow.wait_condition() comes in.\n",
    "\n",
    "- Use workflow.wait_condition() to pause until Signal is received (user decides the next step)\n",
    "- Creates a blocking checkpoint where the Workflow stops and waits\n",
    "- Resumes execution only when specified condition becomes true\n",
    "- Optionally accepts a timeout parameter: `workflow.wait_condition(lambda: condition, timeout=timedelta(hours=24))` - waits until Signal received OR timeout elapsed, whichever happens first\n",
    "\n",
    "**Benefits**:\n",
    "- **Resource efficiency**: During waiting periods, the Workflow instance consumes no CPU or memory. The Worker only \"wakes up\" when a Signal is received or when replaying history.\n",
    "- **Durability**: If systems crash while waiting, the Workflow resumes exactly where it left off when systems recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Within the `await workflow.wait_condition(lambda: )` code, \n",
    "# set the lambda to when `self._user_decision.decision` is not set to `UserDecision.WAIT`\n",
    "# Step 2: Reset the Signal state back to WAIT so we can receive the next user decision\n",
    "# Step 3: Run this code block\n",
    "from temporalio import workflow\n",
    "\n",
    "@workflow.defn(sandboxed=False)\n",
    "class GenerateReportWorkflow:\n",
    "    def __init__(self) -> None:\n",
    "        self._current_prompt: str = \"\"\n",
    "        # Instance variable to store the Signal in\n",
    "        self._user_decision: UserDecisionSignal = UserDecisionSignal(decision=UserDecision.WAIT)\n",
    "\n",
    "    # Method to handle the Signal\n",
    "    @workflow.signal\n",
    "    async def user_decision_signal(self, decision_data: UserDecisionSignal) -> None:\n",
    "        # Update the instance variable with the received Signal data\n",
    "        self._user_decision = decision_data\n",
    "\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "        self._current_prompt = input.prompt\n",
    "\n",
    "        llm_call_input = LLMCallInput(\n",
    "            prompt=self._current_prompt,\n",
    "            llm_api_key=input.llm_api_key,\n",
    "            llm_model=input.llm_research_model,\n",
    "        )\n",
    "\n",
    "        continue_agent_loop = True\n",
    "\n",
    "        while continue_agent_loop:\n",
    "            research_facts = await workflow.execute_activity(\n",
    "                llm_call,\n",
    "                llm_call_input,\n",
    "                start_to_close_timeout=timedelta(seconds=30),\n",
    "            )\n",
    "\n",
    "            self._research_result = research_facts[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "            # Waiting for Signal with user decision\n",
    "            await workflow.wait_condition(lambda: ) # TODO set the lambda to when `self._user_decision.decision` is not set to `UserDecision.WAIT`\n",
    "\n",
    "            if self._user_decision.decision == UserDecision.KEEP:\n",
    "                workflow.logger.info(\"User approved the research. Creating PDF...\")\n",
    "                continue_agent_loop = False\n",
    "            elif self._user_decision.decision == UserDecision.EDIT:\n",
    "                workflow.logger.info(\"User requested research modification.\")\n",
    "                if self._user_decision.additional_prompt != \"\":\n",
    "                    self._current_prompt = (\n",
    "                        f\"{self._current_prompt}\\n\\nAdditional instructions: {self._user_decision.additional_prompt}\"\n",
    "                    )\n",
    "                else:\n",
    "                    workflow.logger.info(\"No additional instructions provided. Regenerating with original prompt.\")\n",
    "                llm_call_input.prompt = self._current_prompt\n",
    "\n",
    "                # TODO Reset the Signal state back to WAIT so we can receive the next user decision\n",
    "\n",
    "        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "        pdf_filename: str = await workflow.execute_activity(\n",
    "            create_pdf,\n",
    "            pdf_generation_input,\n",
    "            start_to_close_timeout=timedelta(seconds=20),\n",
    "        )\n",
    "\n",
    "        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load and display the solution\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "notebook_dir = Path(os.getcwd())\n",
    "solution_file = notebook_dir / \"Solutions_03_Human_in_the_Loop\" / \"wait_condition_solution.py\"\n",
    "\n",
    "code = solution_file.read_text()\n",
    "\n",
    "print(\"Solution loaded:\")\n",
    "display(Markdown(f\"```python\\n{code}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending Signals\n",
    "\n",
    "- We can now run the Workflow and send our Signal\n",
    "- There are multiple ways to send a Signal\n",
    "  - Using a Temporal Client in an SDK\n",
    "  - Using the Web UI\n",
    "  - Using the `temporal` cli\n",
    "- For this example, we will use a Temporal Client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Test our Signal!\n",
    "\n",
    "Now that your Signal is implemented, let's run the Worker and start the Workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set up our Worker. Run this code block to load it into the program\n",
    "import concurrent.futures\n",
    "from temporalio.client import Client\n",
    "from temporalio.worker import Worker\n",
    "\n",
    "async def run_worker() -> None:\n",
    "    # Create client connected to server at the given address\n",
    "    client = await Client.connect(\"localhost:7233\", namespace=\"default\")\n",
    "\n",
    "    # Run the Worker\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as activity_executor:\n",
    "        worker = Worker(\n",
    "            client,\n",
    "            task_queue=\"research\",\n",
    "            workflows=[GenerateReportWorkflow],\n",
    "            activities=[llm_call, create_pdf],\n",
    "            activity_executor=activity_executor\n",
    "        )\n",
    "        print(f\"Starting the worker..\")\n",
    "        await worker.run()\n",
    "\n",
    "print(\"Worker loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to run the Worker\n",
    "import asyncio\n",
    "worker = asyncio.create_task(run_worker())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Workflow! Run this codeblock\n",
    "import uuid\n",
    "from temporalio.client import Client\n",
    "\n",
    "client = await Client.connect(\"localhost:7233\", namespace=\"default\")\n",
    "\n",
    "print(\"Welcome to the Research Report Generator!\")\n",
    "prompt = input(\"Enter your research topic or question: \").strip()\n",
    "\n",
    "if not prompt:\n",
    "    prompt = \"Give me 5 fun and fascinating facts about tardigrades. Make them interesting and educational!\"\n",
    "    print(f\"No prompt entered. Using default: {prompt}\")\n",
    "\n",
    "# Asynchronous start of a Workflow\n",
    "handle = await client.start_workflow(\n",
    "    GenerateReportWorkflow.run,\n",
    "    GenerateReportInput(prompt=prompt, llm_api_key=LLM_API_KEY),\n",
    "    id=f\"generate-research-report-workflow-{uuid.uuid4()}\",\n",
    "    task_queue=\"research\",\n",
    ")\n",
    "\n",
    "print(f\"Started workflow. Workflow ID: {handle.id}, RunID {handle.result_run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing Signals in the Web UI\n",
    "\n",
    "Refresh your Web UI. Look for the `Workflow Execution Signaled` Event. What was the input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the current Worker so we can start fresh with Query support\n",
    "x = worker.cancel()\n",
    "\n",
    "if x:\n",
    "  print(\"Worker killed!\")\n",
    "else:\n",
    "  print(\"Worker was not running. Nothing to kill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Adding Query Support in Temporal\n",
    "\n",
    "Now let's add **[Query](https://docs.temporal.io/develop/python/message-passing#queries)** support to our Workflow. Queries:\n",
    "\n",
    "- Extract state to show the user\n",
    "- Can be done during or even after the Workflow Execution has completed\n",
    "    - In either case, there must be at least one running Worker for the Task Queue to which that Workflow belongs.\n",
    "- Are a synchronous operation that that retrieve state from a Workflow Execution. Once a Query is issued, the Client waits for a response from the Workflow. \n",
    "\n",
    "Examples:\n",
    "- **Monitor Progress of Long-Running Workflows**: A Client might want to receive updates on the progress, like the percentage of the task completed.\n",
    "- **Retrieve Results**: Queries can be used to fetch the results of Activities without waiting for the entire Workflow to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Handle a Query\n",
    "\n",
    "Let's create a Query which will allow external clients to read the current research content from a running Workflow without interrupting its execution.\n",
    "\n",
    "You can handle Queries by annotating a function within your Workflow with `@workflow.query`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a Query into our Workflow code\n",
    "# Step 1: Initialize the _research_result to be an empty string\n",
    "# Step 2: In your Query handler, returns the `_research_result` field\n",
    "import asyncio\n",
    "from datetime import timedelta\n",
    "\n",
    "from temporalio import workflow\n",
    "\n",
    "@workflow.defn(sandboxed=False)\n",
    "class GenerateReportWorkflow:\n",
    "    def __init__(self) -> None:\n",
    "        self._current_prompt: str = \"\"\n",
    "        self._user_decision: UserDecisionSignal = UserDecisionSignal(decision=UserDecision.WAIT)\n",
    "        self._research_result: # TODO Initialize the _research_result to be an empty string\n",
    "\n",
    "    # Method to handle the Signal\n",
    "    @workflow.signal\n",
    "    async def user_decision_signal(self, decision_data: UserDecisionSignal) -> None:\n",
    "        # Update the instance variable with the received Signal data\n",
    "        self._user_decision = decision_data\n",
    "    \n",
    "    @workflow.query # Query to get the current research result\n",
    "    def get_research_result(self) -> str | None:\n",
    "        # TODO: Return the `_research_result` field\n",
    "\n",
    "    # Rest of code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load and display the solution\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "notebook_dir = Path(os.getcwd())\n",
    "solution_file = notebook_dir / \"Solutions_03_Human_in_the_Loop\" / \"handling_query_solution.py\"\n",
    "\n",
    "code = solution_file.read_text()\n",
    "\n",
    "print(\"Solution loaded:\")\n",
    "display(Markdown(f\"```python\\n{code}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### React to Query\n",
    "\n",
    "We now want to fill out the contents of the research result so that the contents can be sent back to the user when the user queries for the research result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a Query into our Workflow code\n",
    "# Step 1: After the LLM call, set `_research_result` field to be the contents of the research (research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
    "# Step 2: Run this code block to load it into the program\n",
    "import asyncio\n",
    "from datetime import timedelta\n",
    "\n",
    "from temporalio import workflow\n",
    "\n",
    "@workflow.defn(sandboxed=False)\n",
    "class GenerateReportWorkflow:\n",
    "    def __init__(self) -> None:\n",
    "        self._current_prompt: str = \"\"\n",
    "        # Instance variable to store the Signal in\n",
    "        self._user_decision: UserDecisionSignal = UserDecisionSignal(decision=UserDecision.WAIT)\n",
    "        self._research_result: str = \"\"\n",
    "\n",
    "    # Method to handle the Signal\n",
    "    @workflow.signal\n",
    "    async def user_decision_signal(self, decision_data: UserDecisionSignal) -> None:\n",
    "        # Update the instance variable with the received Signal data\n",
    "        self._user_decision = decision_data\n",
    "\n",
    "    @workflow.query # Query to get the current research result\n",
    "    def get_research_result(self) -> str | None:\n",
    "        return self._research_result\n",
    "\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "        self._current_prompt = input.prompt\n",
    "\n",
    "        llm_call_input = LLMCallInput(\n",
    "            prompt=self._current_prompt,\n",
    "            llm_api_key=input.llm_api_key,\n",
    "            llm_model=input.llm_research_model,\n",
    "        )\n",
    "\n",
    "        continue_agent_loop = True\n",
    "\n",
    "        while continue_agent_loop:\n",
    "            research_facts = await workflow.execute_activity(\n",
    "                llm_call,\n",
    "                llm_call_input,\n",
    "                start_to_close_timeout=timedelta(seconds=30),\n",
    "            )\n",
    "\n",
    "            # TODO: Set `_research_result` field to be the contents of the research (research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "            # Waiting for Signal with user decision\n",
    "            await workflow.wait_condition(lambda: self._user_decision.decision != UserDecision.WAIT)\n",
    "\n",
    "            if self._user_decision.decision == UserDecision.KEEP:\n",
    "                workflow.logger.info(\"User approved the research. Creating PDF...\")\n",
    "                continue_agent_loop = False\n",
    "            elif self._user_decision.decision == UserDecision.EDIT:\n",
    "                workflow.logger.info(\"User requested research modification.\")\n",
    "                if self._user_decision.additional_prompt != \"\":\n",
    "                    self._current_prompt = (\n",
    "                        f\"{self._current_prompt}\\n\\nAdditional instructions: {self._user_decision.additional_prompt}\"\n",
    "                    )\n",
    "                    workflow.logger.info(f\"Regenerating research with updated prompt: {self._current_prompt}\")\n",
    "                else:\n",
    "                    workflow.logger.info(\"No additional instructions provided. Regenerating with original prompt.\")\n",
    "                llm_call_input.prompt = self._current_prompt\n",
    "\n",
    "                self._user_decision = UserDecisionSignal(decision=UserDecision.WAIT)\n",
    "\n",
    "        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "        pdf_filename: str = await workflow.execute_activity(\n",
    "            create_pdf,\n",
    "            pdf_generation_input,\n",
    "            start_to_close_timeout=timedelta(seconds=20),\n",
    "        )\n",
    "\n",
    "        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load and display the solution\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "notebook_dir = Path(os.getcwd())\n",
    "solution_file = notebook_dir / \"Solutions_03_Human_in_the_Loop\" / \"react_to_query_solution.py\"\n",
    "\n",
    "code = solution_file.read_text()\n",
    "\n",
    "print(\"Solution loaded:\")\n",
    "display(Markdown(f\"```python\\n{code}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending a Signal and Query with the Client\n",
    "\n",
    "To send a Signal with the Temporal Client, we need to get a \"handle\" to a specific Workflow Execution, which will be used to interact with that Workflow.\n",
    "\n",
    "We'll do this with the `get_workflow_handle` method.\n",
    "\n",
    "```python\n",
    "handle = client.get_workflow_handle(workflow_id)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending a Query wiht the Client\n",
    "\n",
    "Let's send a Query first. \n",
    "\n",
    "1. Get a handle of the Workflow Execution we will query\n",
    "2. Send a query with the `query` method.\n",
    "\n",
    "```python\n",
    "research_result = await handle.query(GenerateReportWorkflow.get_research_result)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's send our Query from the Client code\n",
    "# TODO: Run this codeblock\n",
    "\n",
    "async def query_research_result(client: Client, workflow_id: str) -> None:\n",
    "    handle = client.get_workflow_handle(workflow_id)\n",
    "\n",
    "    try:\n",
    "        research_result = await handle.query(GenerateReportWorkflow.get_research_result)\n",
    "        if research_result:\n",
    "            print(f\"Research Result: {research_result}\")\n",
    "        else:\n",
    "            print(\"Research Result: Not yet available\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Query failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's send our Signal from the Client code\n",
    "# Step 1: If the input decision was to edit the the prompt, send a Signal to your Workflow Handle with the new signal data\n",
    "# Step 2: Run this code block to load it into the program\n",
    "async def send_user_decision_signal(client: Client, workflow_id: str) -> None:\n",
    "  loop = asyncio.get_running_loop() # We usually do not need this\n",
    "  handle = client.get_workflow_handle(workflow_id) # Get a handle on the Workflow Execution we want to send a Signal to.\n",
    "\n",
    "  while True:\n",
    "      print(\"\\n\" + \"=\" * 80)\n",
    "      print(\n",
    "          \"Calling LLM! See the response in your Web UI in the output of the `llm_call` Activity. Would you like to keep or edit it?\"\n",
    "      )\n",
    "      print(\"1. Type 'keep' to approve the output and create PDF\")\n",
    "      print(\"2. Type 'edit' to modify the output\")\n",
    "      print(\"=\" * 80)\n",
    "\n",
    "      # When running input in async code, run in an executor to not block the event loop\n",
    "      decision = await loop.run_in_executor(None, input, \"Your decision (keep/edit): \")\n",
    "      decision = decision.strip().lower()\n",
    "\n",
    "      if decision in {\"keep\", \"1\"}:\n",
    "          signal_data = UserDecisionSignal(decision=UserDecision.KEEP)\n",
    "          await handle.signal(\"user_decision_signal\", signal_data) # Send our Keep Signal to our Workflow Execution we have a handle on\n",
    "          print(\"Signal sent to keep output and create PDF\")\n",
    "          break\n",
    "      if decision in {\"edit\", \"2\"}:\n",
    "          additional_prompt_input = input(\"Enter additional instructions to edit the output (optional): \").strip()\n",
    "          additional_prompt = additional_prompt_input if additional_prompt_input else \"\"\n",
    "          signal_data = UserDecisionSignal(decision=UserDecision.EDIT, additional_prompt=additional_prompt)\n",
    "          # TODO Send our Signal to our Workflow Execution we have a handle on\n",
    "          print(\"Signal sent to regenerate output\")\n",
    "\n",
    "      else:\n",
    "          print(\"Please enter either 'keep', 'edit'\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load and display the solution\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "notebook_dir = Path(os.getcwd())\n",
    "solution_file = notebook_dir / \"Solutions_03_Human_in_the_Loop\" / \"sending_signal_solution.py\"\n",
    "\n",
    "code = solution_file.read_text()\n",
    "\n",
    "print(\"Solution loaded:\")\n",
    "display(Markdown(f\"```python\\n{code}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the Worker again\n",
    "worker = asyncio.create_task(run_worker())\n",
    "\n",
    " # Check if the task is in the set of all tasks\n",
    "if worker in asyncio.all_tasks():\n",
    "    # The sleep is necessary because of the async task scheduling in Jupyter\n",
    "    print(\"Task is currently active.\") # The Worker now registers the updated Workflow changes\n",
    "else:\n",
    "    print(\"Task is not found in active tasks (might have finished or not yet scheduled).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now send your Signal by running this code block.\n",
    "send_signal = await send_user_decision_signal(client, handle.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSU6nnwcKbY6"
   },
   "source": [
    "## Putting it Together\n",
    "\n",
    "You can now make a research call, check your results by querying your Workflow, then choose to edit or keep the research!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbw85OliybvN"
   },
   "source": [
    "---\n",
    "# Exercise 3 - Human in the Loop\n",
    "\n",
    "* In these exercises you will:\n",
    "  * Review a modified version of the previous exercise and investigate the results in the Web UI\n",
    "  * Add a Signal to the exercise to provide the filename you wish to save the research report as\n",
    "  * Add a Query to the exercise to extract the character length of the research request\n",
    "* Go to the **Exercise** Directory in the Google Drive and open the **Practice** Directory\n",
    "* Open _01-An-AI-Agent-Practice.ipynb_ and follow the instructions\n",
    "* If you get stuck, raise your hand and someone will come by and help. You can also check the `Solution` directory for the answers\n",
    "* **You have 5 mins**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "This workshop introduced you to the **interacting with workflows** in Temporal. Further your learning with these resources:\n",
    "\n",
    "- Our [free course](https://learn.temporal.io/courses/interacting_with_workflows/python/) on Signals and Queries in Temporal and other ways to interact with Workflows\n",
    "- A [Python tutorial](https://learn.temporal.io/tutorials/python/build-an-email-drip-campaign/) to practice handling Signals and Queries\n",
    "- Documentation on [Updates](https://docs.temporal.io/develop/python/message-passing#updates), a feature which combine both Signals and Queries"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (edu-ai-workshop-mcp)",
   "language": "python",
   "name": "edu-ai-workshop-mcp"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
