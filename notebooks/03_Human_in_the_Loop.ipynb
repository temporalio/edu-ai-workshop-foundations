{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42Spdn4_DuBH"
   },
   "source": [
    "# Human in the Loop\n",
    "\n",
    "Your durable research application now survives crashes and automatically retries failures. But there's a critical gap: **it runs completely autonomously**.\n",
    "\n",
    "Imagine this scenario: Your AI generates research on \"best travel spots for summer 2025\", your application generates a PDF, and sends it to your client—all automatically.\n",
    "\n",
    "Then your client calls: \"I want cheaper options!\"\n",
    "\n",
    "Your application had no way to pause and ask for clarification or approval. It just executed straight through to completion.\n",
    "\n",
    "**Real-world AI applications need human interaction**—to provide feedback, approve decisions, or clarify requirements. But adding human input to automated workflows introduces new challenges: What if the user's browser crashes while they're reviewing? What if they close the tab and come back later? How do you prevent losing expensive LLM work while waiting for approval?\n",
    "\n",
    "In this section, we'll solve these problems by making your AI application interactive and durable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfYOsDC1Jc5M"
   },
   "source": [
    "## Setup Notebook\n",
    "\n",
    "Run the following code blocks to install various packages and tools necessary to run this notebook\n",
    "\n",
    "**Be sure to add your .env file again. It doesn't persist across notebooks or sesions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ZqFOSwtoJv1L",
    "outputId": "b3df6731-a0b6-429b-cf0f-519cc441378e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.4/278.4 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# We'll first install the necessary packages for this workshop.\n",
    "\n",
    "%pip install --quiet temporalio litellm reportlab python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rjtlX8lJ8rZ"
   },
   "outputs": [],
   "source": [
    "# Create .env file\n",
    "with open(\".env\", \"w\") as fh:\n",
    "  fh.write(\"LLM_API_KEY = YOUR_API_KEY\\nLLM_MODEL = openai/gpt-4o\")\n",
    "\n",
    "  # Now open the file and replace YOUR_API_KEY with your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fy4s0KTRdWxL"
   },
   "outputs": [],
   "source": [
    "# Load environment variables and configure LLM settings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Get LLM_API_KEY environment variable and print it to make sure that your .env file is properly loaded.\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"openai/gpt-4o\")\n",
    "LLM_API_KEY = os.getenv(\"LLM_API_KEY\", None)\n",
    "print(\"LLM API Key\", LLM_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "at3q7U8wGuSp"
   },
   "outputs": [],
   "source": [
    "# Mermaid renderer, run at the beginning to setup rendering of diagrams\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def render_mermaid(graph_definition):\n",
    "    \"\"\"\n",
    "    Renders a Mermaid diagram in Google Colab using mermaid.ink.\n",
    "\n",
    "    Args:\n",
    "        graph_definition (str): The Mermaid diagram code (e.g., \"graph LR; A-->B;\").\n",
    "    \"\"\"\n",
    "    graph_bytes = graph_definition.encode(\"ascii\")\n",
    "    base64_bytes = base64.b64encode(graph_bytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLHoJlR4R8Au"
   },
   "outputs": [],
   "source": [
    "# This allows us to run the Temporal Asyncio event loop within the event loop of Jupyter Notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etzQbb8rphcH"
   },
   "outputs": [],
   "source": [
    " # Running this will download the Temporal CLI\n",
    "\n",
    "!curl -sSf https://temporal.download/cli.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Sure Your Temporal Web UI is Running\n",
    "\n",
    "1. You should have the Temporal Server running in your terminal (run `temporal server start-dev` if not).\n",
    "2. Then in your `Ports` tab on the bottom of this screen, find `8233` and click on the Globe icon to open the Temporal Web UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmBTPDc1K1Oc"
   },
   "source": [
    "## Review the Previous Workflow\n",
    "\n",
    "Let's quickly review the previous Workflow below to refresh your memory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMzXQ7ivLClZ"
   },
   "source": [
    "### Data Models\n",
    "\n",
    "* Temporal recommends passing data to and from Activities and Workflows as a single object.\n",
    "* Use a dataclass for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtTsi1b9K86m"
   },
   "outputs": [],
   "source": [
    "# Run this code block to load it into the program\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class LLMCallInput:\n",
    "  prompt: str\n",
    "  llm_api_key: str\n",
    "  llm_model: str\n",
    "\n",
    "@dataclass\n",
    "class PDFGenerationInput:\n",
    "  content: str\n",
    "  filename: str = \"research_pdf.pdf\"\n",
    "\n",
    "@dataclass\n",
    "class GenerateReportInput:\n",
    "    prompt: str\n",
    "    llm_api_key: str\n",
    "    llm_research_model: str = \"openai/gpt-4o\"\n",
    "    llm_image_model: str = \"dall-e-3\"\n",
    "\n",
    "@dataclass\n",
    "class GenerateReportOutput:\n",
    "    result: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9X5Xh50LFJs"
   },
   "source": [
    "### Activities\n",
    "\n",
    "* An Activity is a function/method that is prone to failure and/or non-deterministic.\n",
    "* Temporal requires all non-deterministic code be run in an Activity\n",
    "* Activities retry over and over until they succeed or until your customized retry or timeout configuration is hit.\n",
    "* You define an Activity by adding the `@activity.defn` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doN_2Wzganj5"
   },
   "outputs": [],
   "source": [
    "# Run this code block to load it into the program\n",
    "from temporalio import activity\n",
    "from litellm import completion, ModelResponse\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "\n",
    "@activity.defn\n",
    "def llm_call(input: LLMCallInput) -> ModelResponse:\n",
    "    response = completion(\n",
    "      model=input.llm_model,\n",
    "      api_key=input.llm_api_key,\n",
    "      messages=[{ \"content\": input.prompt,\"role\": \"user\"}]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "@activity.defn\n",
    "def create_pdf(input: PDFGenerationInput) -> str:\n",
    "    print(\"Creating PDF document...\")\n",
    "\n",
    "    doc = SimpleDocTemplate(input.filename, pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    title_style = ParagraphStyle(\n",
    "        'CustomTitle',\n",
    "        parent=styles['Heading1'],\n",
    "        fontSize=24,\n",
    "        spaceAfter=30,\n",
    "        alignment=1\n",
    "    )\n",
    "\n",
    "    story = []\n",
    "    title = Paragraph(\"Research Report\", title_style)\n",
    "    story.append(title)\n",
    "    story.append(Spacer(1, 20))\n",
    "    paragraphs = input.content.split('\\n\\n')\n",
    "    for para in paragraphs:\n",
    "        if para.strip():\n",
    "          p = Paragraph(para.strip(), styles['Normal'])\n",
    "          story.append(p)\n",
    "          story.append(Spacer(1, 12))\n",
    "\n",
    "    doc.build(story)\n",
    "\n",
    "    print(f\"SUCCESS! PDF created: {input.filename}\")\n",
    "    return input.filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NyXfwzyLHB9"
   },
   "source": [
    "### Workflow\n",
    "\n",
    "* Activities are orchestrated within a Temporal Workflow.\n",
    "* Workflows must **not** make API calls, file system calls, or anything non-deterministic. That is what Activities are for.\n",
    "* Workflows are async, and you define them as a class decorated with the `@workflow.defn` decorator.\n",
    "* Every Workflow has a **single** entry point, which is an `async` method decorated with `@workflow.run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnJltM-3LIMg"
   },
   "outputs": [],
   "source": [
    "# Run this code block to load it into the program\n",
    "from datetime import timedelta\n",
    "from temporalio import workflow\n",
    "\n",
    "# sandboxed=False is a Notebook only requirement. You normally don't do this\n",
    "@workflow.defn(sandboxed=False)\n",
    "class GenerateReportWorkflow:\n",
    "\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "\n",
    "        llm_call_input = LLMCallInput(prompt=input.prompt, llm_api_key=LLM_API_KEY, llm_model=LLM_MODEL)\n",
    "\n",
    "        research_facts = await workflow.execute_activity(\n",
    "            llm_call,\n",
    "            llm_call_input,\n",
    "            start_to_close_timeout=timedelta(seconds=30),\n",
    "        )\n",
    "\n",
    "        workflow.logger.info(\"Research complete!\")\n",
    "\n",
    "        # Uncomment to add delay\n",
    "        # await workflow.sleep(timedelta(seconds=20))\n",
    "\n",
    "        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "        pdf_filename = await workflow.execute_activity(\n",
    "            create_pdf,\n",
    "            pdf_generation_input,\n",
    "            start_to_close_timeout=timedelta(seconds=10),\n",
    "        )\n",
    "\n",
    "        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBfUn_5wEfvE"
   },
   "source": [
    "## Human Interactions in Your AI Applications\n",
    "\n",
    "While some AI applications may operate entirely autonomously, many require human intervention. They may provide input on launch or at various points throughout the execution.\n",
    "\n",
    "Examples:\n",
    "  a. Validation at critical decision points\n",
    "  b. Final review before implementation\n",
    "  c. Feedback loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cFySLL0sagn"
   },
   "source": [
    "### Example: Customer Support Agent with Human Oversight\n",
    "\n",
    "The Scenario:\n",
    "Customer contacts support: \"My order hasn't arrived and I need it urgently\"\n",
    "\n",
    "**How Human Interaction Works:**\n",
    "\n",
    "1. **Initial Input** - Customer provides problem; human sets constraints like maximum refund amounts or escalation thresholds\n",
    "\n",
    "2. **AI Analysis** - LLM checks order status and finds \"delayed in transit,\" then proposes: \"Process a $75 expedited replacement\"\n",
    "\n",
    "3. **Human Decision Point** - System pauses for approval. Human can:\n",
    "   - Approve the proposed action\n",
    "   - Modify the approach (\"Offer refund instead\")\n",
    "   - Add context (\"VIP customer - upgrade to overnight\")\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Cost control** - Expensive decisions require human approval\n",
    "- **Security** - Prevent unauthorized refunds or account changes\n",
    "- **Risk mitigation** - Human judgment for edge cases and exceptions\n",
    "- **Compliance** - Legal/regulatory requirements for certain actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bB1hUId-Nolr"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIExSCiAgICBVMVtVc2VyIEludGVyYWN0aW9uIG9wdGlvbmFsXSAtLi0+IExbTExNIG1ha2VzIGRlY2lzaW9ucyBvbiB3aGF0IHRvIGRvIG5leHRdCiAgICBMIC0tPiBBW0FjdGlvbiBkb2VzIHdoYXQgdGhlIExMTSBkZWNpZGVkXQogICAgQSAtLT4gTAogICAgVTJbVXNlciBJbnRlcmFjdGlvbiBvcHRpb25hbF0gLS4tPiBBCg==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diagram = \"\"\"\n",
    "graph LR\n",
    "    U1[User Interaction optional] -.-> L[LLM makes decisions on what to do next]\n",
    "    L --> A[Action does what the LLM decided]\n",
    "    A --> L\n",
    "    U2[User Interaction optional] -.-> A\n",
    "\"\"\"\n",
    "render_mermaid(diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_a8_7myY80C"
   },
   "source": [
    "## Challenges in Non-Durable Human in the Loop Processes\n",
    "\n",
    "While human interaction points are valuable for AI applications, implementing them reliably presents significant technical challenges. Without durable execution, human input can be lost during system failures, leading to unpredictable behavior.\n",
    "\n",
    "**Consider the following scenario**\n",
    "\n",
    "- A user needs to approve a transaction.\n",
    "- As they are doing this, the website goes down.\n",
    "- How do you mitigate this?\n",
    "  - Do we notify the user to approve the payment again? (creating confusion since the user already 'approved')\n",
    "  - Do we assume approval and risk processing an unauthorized payment?\n",
    "\n",
    "Without durable processes, you're forced to choose between security, user experience, and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_R22lxWTNs1"
   },
   "source": [
    "## Signals in Temporal\n",
    "\n",
    "In Temporal, human interaction in Temporal systems is achieved through a [Temporal Signal](https://docs.temporal.io/encyclopedia/workflow-message-passing).\n",
    "\n",
    "A Signal is a:\n",
    "* Message sent asynchronously to a running Workflow Execution\n",
    "* Used to to change the state and control the flow of a Workflow Execution.\n",
    "\n",
    "*Take 2 minutes to discuss with your neighbor when we might use a Signal, then we'll share answers*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FM27DzFXKQMJ"
   },
   "source": [
    "## Example Signal Usage\n",
    "\n",
    "1. The user initiates a workflow with an initial request\n",
    "2. The workflow processes the request and determines what information or approval is needed\n",
    "3. The workflow pauses and waits for user input via Signal, such as:\n",
    "    - Additional information or clarification\n",
    "    - Permission to proceed with an action\n",
    "    - Selection between multiple options\n",
    "4. The user sends a Signal with their response\n",
    "5. The workflow resumes execution based on the Signal received\n",
    "6. Steps repeat as needed until the workflow completes its task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULS1W9BeZt7X"
   },
   "source": [
    "## Durably Storing Human Interactions\n",
    "\n",
    "Let's go back to the user approving a payment example now. With Temporal, when the user clicks \"approve\" in the finance portal, the approval decision gets durably stored.\n",
    "\n",
    "The user can close the browser, go to lunch, and the Workflow will continue running in the background.\n",
    "\n",
    "If the payment gateway times out, returns an error or becomes unavailable, Temporal automatically retries the payment step. IT does not need to re-ask the user for approval, because that decision is already durably stored in the Workflow state.\n",
    "\n",
    "- **No duplicate work** (user does not have to re-approve the same expense)\n",
    "- **No lost approvals** (Signal persists and processing resumes automatically when system recovers)\n",
    "- **No manual intervention** (does not need to manually reconcile failed payments or investigate whether an expense was actually approved)\n",
    "- **Reliable processing** (business can count on approved expenses being paid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1RStPXFT2OA"
   },
   "source": [
    "## Developing Signals in Temporal\n",
    "\n",
    "There are two steps for adding support for a Signal to your Workflow code:\n",
    "\n",
    "1. **Defining the Signal** - You specify the name and data structure used by Temporal Clients when sending the Signal.\n",
    "2. **Handling the Signal** - You write code that will be invoked when the Signal is received from a Temporal Client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GR_jscEAAaAt"
   },
   "source": [
    "## Demo (Expand for instructor notes or to run on your own)\n",
    "<!--\n",
    "1. Clone this repository: `https://github.com/temporalio/edu-ai-workshop-agentic-loop`. The instructions will also be in the README.\n",
    "2. From the `src/module_one_03_human_in_the_loop` directory, run `app.py` with `python app.py`.\n",
    "3. In one terminal window, run your Worker with `python worker.py`.\n",
    "4. In another terminal window, execute your Workflow with `python starter.py`.\n",
    "5. You'll be prompted to enter a research topic or question in the CLI.\n",
    "6. Once you do, you'll be prompted with the ability to Signal or Query the Workflow.\n",
    "7. Type 'query' and you'll see the output in the terminal window where you started your Workflow Execution.\n",
    "8. Time to demonstrate Signals. Back in the terminal window when you started your Workflow Execution, you'll see that you are prompted to choose one of the two options:\n",
    "    a. Approve of this research and if you would like it to create a PDF (type 'keep' to send a Signal to the Workflow to create the PDF).\n",
    "    b. Modify the research by adding extra info to the prompt (type 'edit' to modify the prompt and send another Signal to the Workflow to prompt the LLM again).\n",
    "9. Demonstrate the modification by typing `edit`.\n",
    "10. Enter additional instructions (e.g.: \"turn this into a poem\") and see the new output in the terminal window by typing `query` again.\n",
    "11. Finally, show that you can keep changing the execution path of your Workflow Execution by typing `keep`. Show that the PDF has appeared in your `module_one_03_human_in_the_loop` directory.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZfKmHu5UBO2"
   },
   "source": [
    "## Implementing a Signal\n",
    "\n",
    "To add Signal support to your Workflow, you need to complete four key steps:\n",
    "\n",
    "1. **Create a custom model for the Signal data** \n",
    "     - Define the structure of information that will be sent via Signal \n",
    "     - This model determines what information can be passed to your running Workflow \n",
    "2. **Initialize instance variables in your Workflow's `__init__` method** \n",
    "     - Add instance variables to store incoming Signal data\n",
    "    - These variables persist throughout the Workflow's execution\n",
    "    - Set default values that represent the \"no Signal received yet state         \n",
    "3. **Implement a Signal handler method decorated with `@workflow signal`**\n",
    "    - Define a method that will be called when a Signal is received\n",
    "    - This method updates your instance variables with the incoming Signal data\n",
    "4. **React to the Signal in your Workflow logic** \n",
    "    - Use `workflow.wait_condition()` to pause execution until a Signal is received\n",
    "    - Check the Signal data and branch your Workflow logic accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLAQZtPMe813"
   },
   "source": [
    "## Let's Add a Signal to Our Workflow\n",
    "\n",
    "We are going to create a Signal to send to our `GenerateReportWorkflow`. After generating the research, and before creating a PDF report, the user can review the research. The user has two choices to send to the Workflow:\n",
    "  - Keep the research and generate a PDF report by signaling \"keep\"\n",
    "  - Modify the research before generating a PDF report by signaling \"edit\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4ATexy5Cfsx"
   },
   "source": [
    "### Create a Model\n",
    "\n",
    "- Create a model for the Signal to be stored in\n",
    "- Similar to Activities and Workflows, `dataclasses` are recommended here\n",
    "- The model can be nested of other classes, such as `StrEnum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9b4ZxmVYnO7"
   },
   "outputs": [],
   "source": [
    "# This defines a string enum and a dataclass for handling user decisions in a Temporal workflow.\n",
    "# We will send a `UserDecision` as a Signal to our Research Workflow letting the Workflow know if we want to keep or edit the research\n",
    "# Or if we want to wait for further decision\n",
    "# Run this code block to load it into the program\n",
    "from enum import StrEnum\n",
    "\n",
    "class UserDecision(StrEnum):\n",
    "    KEEP = \"KEEP\"\n",
    "    EDIT = \"EDIT\"\n",
    "    WAIT = \"WAIT\"\n",
    "    \n",
    "@dataclass\n",
    "class UserDecisionSignal: # A data structure to send user decisions via Temporal Signals\n",
    "    decision: UserDecision\n",
    "    additional_prompt: str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6j6_1ggB2D13"
   },
   "source": [
    "## Handle the Signal\n",
    "\n",
    "Signals are asynchronous messages sent to running workflows from external systems (like a user clicking a button or an API call). Once a Signal is received, your Workflow needs to process that data and take appropriate action based on the Signal handler function.\n",
    "\n",
    "Remember, there are three main components:\n",
    "\n",
    "1. **Store Signal State**: Instance variables to store incoming Signal data that persist across Workflow Executions and Replays.\n",
    "2. **Signal Handler Method**: The handler is a method decorated with `@workflow.signal` that receives incoming Signals and updates your stored state.\n",
    "3. **React to Signal in Workflow Logic**: After storing the signal, your workflow's main logic needs to check the state and react accordingly.\n",
    "\n",
    "Let's first look at storing the Signal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zb1UaTYPCssj"
   },
   "source": [
    "### Storing the Signal state\n",
    "\n",
    "Remember, the workflow needs a place to remember what Signals it has received:\n",
    "\n",
    "```python\n",
    "@workflow.defn\n",
    "class GenerateReportWorkflow:\n",
    "    def __init__(self):\n",
    "        # Instance variable - persists across workflow execution\n",
    "        self._user_decision = UserDecisionSignal(decision=UserDecision.WAIT)\n",
    "```\n",
    "\n",
    "- Can be a simple variable, or a Queue for handling many Signals\n",
    "- Initialize with default values that indicates \"no Signal received yet\". In the example above, `WAIT` is the default state.\n",
    "\n",
    "Now let's define a Signal handler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UE3GaTs9fiTN"
   },
   "source": [
    "## Defining a Signal Handler\n",
    "\n",
    "A method within your Workflow class that receives and processes incoming Signals.\n",
    "\n",
    "```python\n",
    "@workflow.signal\n",
    "async def user_decision_signal(self, decision_data: UserDecisionSignal) -> None:\n",
    "    self._user_decision = decision_data\n",
    "```\n",
    "\n",
    "In this example, when a client sends a Signal, this handler receives the `UserDecisionSignal` data and updates the `self._user_decision` in the Workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ebvjx4ZWhIHl"
   },
   "outputs": [],
   "source": [
    "# Putting it Together\n",
    "# Reflection Questions:\n",
    "# 1. What's still missing for this Signal implementation to work properly?\n",
    "# 2. How will the Workflow know when to check for a Signal?\n",
    "from temporalio import workflow\n",
    "\n",
    "@workflow.defn(sandboxed=False) # sandboxed=False is a Notebook only requirement. You normally don't do this)\n",
    "class GenerateReportWorkflow:\n",
    "    def __init__(self) -> None:\n",
    "        # Instance variable to store Signal data\n",
    "        self._user_decision: UserDecisionSignal = UserDecisionSignal(decision=UserDecision.WAIT) # UserDecision Signal starts with WAIT as the default state\n",
    "\n",
    "    # Define the Signal handler\n",
    "    @workflow.signal\n",
    "    async def user_decision_signal(self, decision_data: UserDecisionSignal) -> None:\n",
    "        # Update instance variable when Signal is received\n",
    "        self._user_decision = decision_data\n",
    "\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "      self._current_prompt = input.prompt\n",
    "\n",
    "      llm_call_input = LLMCallInput(\n",
    "          prompt=self._current_prompt,\n",
    "          llm_api_key=input.llm_api_key,\n",
    "          llm_model=input.llm_research_model,\n",
    "      )\n",
    "      # rest of code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDWk-Ca39mmO"
   },
   "source": [
    "## Waiting for a Signal\n",
    "\n",
    "Now that we have a Signal handler to receive data, we need a way for the Workflow to pause and wait for that Signal to arrive. This is where `workflow.wait_condition()` comes in.\n",
    "\n",
    "**How `workflow.wait_condition()` works:**\n",
    "- Pauses Workflow execution until a specified condition becomes true\n",
    "- Creates a durable checkpoint where the Workflow stops and waits\n",
    "- Resumes execution only when the condition evaluates to `True`\n",
    "- Optionally accepts a timeout parameter: `workflow.wait_condition(lambda: condition, timeout=timedelta(hours=24))` - waits until Signal received OR timeout elapsed, whichever happens first\n",
    "\n",
    "**Benefits:**\n",
    "- **Resource efficiency**: During waiting periods, the Workflow instance consumes no CPU or memory. The Worker only \"wakes up\" when a Signal is received or when replaying history.\n",
    "- **Durability**: If systems crash while waiting, the Workflow resumes exactly where it left off when systems recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INGtRIkm96bZ"
   },
   "outputs": [],
   "source": [
    "# Let's add a wait_condition to our code\n",
    "# TODO: Within the `await workflow.wait_condition(lambda: )` code, \n",
    "# set the lambda to when `self._user_decision.decision` is not set to `UserDecision.WAIT`\n",
    "from temporalio import workflow\n",
    "\n",
    "@workflow.defn(sandboxed=False)\n",
    "class GenerateReportWorkflow:\n",
    "    def __init__(self) -> None:\n",
    "        self._current_prompt: str = \"\"\n",
    "        # Instance variable to store the Signal in\n",
    "        self._user_decision: UserDecisionSignal = UserDecisionSignal(decision=UserDecision.WAIT) # UserDecision Signal starts with WAIT as the default state\n",
    "\n",
    "    # Method to handle the Signal\n",
    "    @workflow.signal\n",
    "    async def user_decision_signal(self, decision_data: UserDecisionSignal) -> None:\n",
    "        # Update the instance variable with the received Signal data\n",
    "        self._user_decision = decision_data\n",
    "\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "        self._current_prompt = input.prompt\n",
    "\n",
    "        llm_call_input = LLMCallInput(\n",
    "            prompt=self._current_prompt,\n",
    "            llm_api_key=input.llm_api_key,\n",
    "            llm_model=input.llm_research_model,\n",
    "        )\n",
    "\n",
    "        continue_agent_loop = True\n",
    "\n",
    "        while continue_agent_loop:\n",
    "            research_facts = await workflow.execute_activity(\n",
    "                llm_call,\n",
    "                llm_call_input,\n",
    "                start_to_close_timeout=timedelta(seconds=30),\n",
    "            )\n",
    "\n",
    "            self._research_result = research_facts[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "            # Waiting for Signal with user decision\n",
    "            await workflow.wait_condition(lambda: ) # TODO set the lambda to when `self._user_decision.decision` is not set to `UserDecision.WAIT`\n",
    "\n",
    "            # rest of code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load and display the solution\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "notebook_dir = Path(os.getcwd())\n",
    "solution_file = notebook_dir / \"Solutions_03_Human_in_the_Loop\" / \"wait_condition_solution.py\"\n",
    "\n",
    "code = solution_file.read_text()\n",
    "\n",
    "print(\"Solution loaded:\")\n",
    "display(Markdown(f\"```python\\n{code}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3m4wCPrASVc"
   },
   "source": [
    "## Handle the Signal: React to Signal in Workflow Logic\n",
    "\n",
    "After receiving a Signal, your Workflow's main execution logic must evaluate the Signal data and determine the appropriate response. This is where the Workflow's business logic intersects with human input.\n",
    "\n",
    "In this example, we are branching execution based on Signal content.\n",
    "\n",
    "```\n",
    "if self._user_decision.decision == UserDecision.KEEP:\n",
    "    continue_agent_loop = False\n",
    "elif self._user_decision.decision == UserDecision.EDIT:\n",
    "    # Modify prompt and reset for next iteration\n",
    "    self._user_decision = UserDecisionSignal(decision=UserDecision.WAIT)\n",
    "```\n",
    "\n",
    "- If the Workflow receives `KEEP` as the `UserDecision`, then the Workflow exits the research loop and proceeds to PDF generation.\n",
    "- If the Workflow receives `EDIT` as the `UserDecision`, then the Workflow incorporates any additional feedback into the prompt, updates the research parameters, and resets the Signal state back to WAIT so it can loop again to regenerate the research and wait for the next user decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWfa8im4YWLv"
   },
   "outputs": [],
   "source": [
    "# Putting it together, we see we have set the Signal state, created the Signal handler method, \n",
    "# and have logic in the Workflow to how it will react to the Signal.\n",
    "# Step 1. If the user chooses to edit the research, after they supply a new prompt,\n",
    "# set the user decision back to WAIT for the next loop\n",
    "# Step 2: Run this code block to load it into your program\n",
    "from datetime import timedelta\n",
    "from temporalio import workflow\n",
    "\n",
    "@workflow.defn(sandboxed=False)\n",
    "class GenerateReportWorkflow:\n",
    "    def __init__(self) -> None:\n",
    "        self._current_prompt: str = \"\"\n",
    "        # Instance variable to store the Signal in\n",
    "        self._user_decision: UserDecisionSignal = UserDecisionSignal(decision=UserDecision.WAIT)\n",
    "        self._research_result: str = \"\"\n",
    "\n",
    "    # Method to handle the Signal\n",
    "    @workflow.signal\n",
    "    async def user_decision_signal(self, decision_data: UserDecisionSignal) -> None:\n",
    "        # Update the instance variable with the received Signal data\n",
    "        self._user_decision = decision_data\n",
    "\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "        self._current_prompt = input.prompt\n",
    "\n",
    "        llm_call_input = LLMCallInput(\n",
    "            prompt=self._current_prompt,\n",
    "            llm_api_key=input.llm_api_key,\n",
    "            llm_model=input.llm_research_model,\n",
    "        )\n",
    "\n",
    "        continue_agent_loop = True\n",
    "\n",
    "        while continue_agent_loop:\n",
    "            research_facts = await workflow.execute_activity(\n",
    "                llm_call,\n",
    "                llm_call_input,\n",
    "                start_to_close_timeout=timedelta(seconds=30),\n",
    "            )\n",
    "\n",
    "            self._research_result = research_facts[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "            # Waiting for Signal with user decision\n",
    "            await workflow.wait_condition(lambda: self._user_decision.decision != UserDecision.WAIT)\n",
    "\n",
    "            if self._user_decision.decision == UserDecision.KEEP:\n",
    "                workflow.logger.info(\"User approved the research. Creating PDF...\")\n",
    "                continue_agent_loop = False\n",
    "            elif self._user_decision.decision == UserDecision.EDIT:\n",
    "                workflow.logger.info(\"User requested research modification.\")\n",
    "                if self._user_decision.additional_prompt != \"\":\n",
    "                    self._current_prompt = (\n",
    "                        f\"{self._current_prompt}\\n\\nAdditional instructions: {self._user_decision.additional_prompt}\"\n",
    "                    )\n",
    "                else:\n",
    "                    workflow.logger.info(\"No additional instructions provided. Regenerating with original prompt.\")\n",
    "                llm_call_input.prompt = self._current_prompt\n",
    "\n",
    "                # TODO Set the user decision back to WAIT for the next loop\n",
    "                self._user_decision = UserDecisionSignal(decision=UserDecision.WAIT)\n",
    "\n",
    "        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "        pdf_filename: str = await workflow.execute_activity(\n",
    "            create_pdf,\n",
    "            pdf_generation_input,\n",
    "            start_to_close_timeout=timedelta(seconds=20),\n",
    "        )\n",
    "\n",
    "        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load and display the solution\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "notebook_dir = Path(os.getcwd())\n",
    "solution_file = notebook_dir / \"Solutions_03_Human_in_the_Loop\" / \"putting_it_together_solution.py\"\n",
    "\n",
    "code = solution_file.read_text()\n",
    "\n",
    "print(\"Solution loaded:\")\n",
    "display(Markdown(f\"```python\\n{code}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set up our Worker. Run this code block to load it into the program\n",
    "import concurrent.futures\n",
    "from temporalio.client import Client\n",
    "from temporalio.worker import Worker\n",
    "\n",
    "async def run_worker() -> None:\n",
    "    # Create client connected to server at the given address\n",
    "    client = await Client.connect(\"localhost:7233\", namespace=\"default\")\n",
    "\n",
    "    # Run the Worker\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as activity_executor:\n",
    "        worker = Worker(\n",
    "            client,\n",
    "            task_queue=\"research\",\n",
    "            workflows=[GenerateReportWorkflow],\n",
    "            activities=[llm_call, create_pdf],\n",
    "            activity_executor=activity_executor\n",
    "        )\n",
    "        print(f\"Starting the worker..\")\n",
    "        await worker.run()\n",
    "\n",
    "print(\"Worker loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0fstw-TE8Ck"
   },
   "source": [
    "## Test the Workflow\n",
    "\n",
    "Now that your Signal is implemented, let's run the Worker and start the Workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_knBkUXcEUuR"
   },
   "source": [
    "### Run a Worker\n",
    "\n",
    "As always, code won't execute if a Worker isn't running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPjwyhKPEUEY"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "worker = asyncio.create_task(run_worker())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tS5s2xtiFPst"
   },
   "source": [
    "### Start the Workflow\n",
    "\n",
    "Now start the Workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TO81vymKFV8x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Research Report Generator!\n",
      "No prompt entered. Using default: Give me 5 fun and fascinating facts about tardigrades. Make them interesting and educational!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GenerateReportWorkflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo prompt entered. Using default: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Asynchronous start of a Workflow\u001b[39;00m\n\u001b[32m     14\u001b[39m handle = \u001b[38;5;28;01mawait\u001b[39;00m client.start_workflow(\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[43mGenerateReportWorkflow\u001b[49m.run,\n\u001b[32m     16\u001b[39m     GenerateReportInput(prompt=prompt, llm_api_key=LLM_API_KEY),\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mid\u001b[39m=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgenerate-research-report-workflow-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muuid.uuid4()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m     task_queue=\u001b[33m\"\u001b[39m\u001b[33mresearch\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarted workflow. Workflow ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhandle.id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, RunID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhandle.result_run_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'GenerateReportWorkflow' is not defined"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from temporalio.client import Client\n",
    "\n",
    "client = await Client.connect(\"localhost:7233\", namespace=\"default\")\n",
    "\n",
    "print(\"Welcome to the Research Report Generator!\")\n",
    "prompt = input(\"Enter your research topic or question: \").strip()\n",
    "\n",
    "if not prompt:\n",
    "    prompt = \"Give me 5 fun and fascinating facts about tardigrades. Make them interesting and educational!\"\n",
    "    print(f\"No prompt entered. Using default: {prompt}\")\n",
    "\n",
    "# Asynchronous start of a Workflow\n",
    "handle = await client.start_workflow(\n",
    "    GenerateReportWorkflow.run,\n",
    "    GenerateReportInput(prompt=prompt, llm_api_key=LLM_API_KEY),\n",
    "    id=f\"generate-research-report-workflow-{uuid.uuid4()}\",\n",
    "    task_queue=\"research\",\n",
    ")\n",
    "\n",
    "print(f\"Started workflow. Workflow ID: {handle.id}, RunID {handle.result_run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUJ9n1rnFhZl"
   },
   "source": [
    "## Observing Signals in the Web UI\n",
    "\n",
    "Now, look at the Web UI. What do you observe?\n",
    "\n",
    "_**To run the Temporal Server in this exercise environment**:\n",
    "1. You should have the Temporal Server running in your terminal (run `temporal server start-dev` if not).\n",
    "2. Then in your `Ports` tab on the bottom of this screen, find `8233` and click on the Globe icon to open the Temporal Web UI._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMBrnpkfbZNU"
   },
   "source": [
    "## Sending Signals\n",
    "\n",
    "- We can now run the Workflow and send our Signal\n",
    "- There are multiple ways to send a Signal\n",
    "  - Using a Temporal Client in an SDK\n",
    "  - Using the Web UI\n",
    "  - Using the `temporal` cli\n",
    "- For this example, we will use a Temporal Client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYvPG66YERjv"
   },
   "source": [
    "## Sending a Signal with the Client\n",
    "\n",
    "To send a Signal with the Temporal Client, we need to get a \"handle\" to a specific Workflow Execution, which will be used to interact with that Workflow.\n",
    "\n",
    "We'll do this with the `get_workflow_handle` method.\n",
    "\n",
    "```python\n",
    "handle = client.get_workflow_handle(workflow_id)\n",
    "```\n",
    "\n",
    "With the handle on the Workflow Execution we want to Signal, we'll then pass in our Signal:\n",
    "\n",
    "```python\n",
    "signal_data = UserDecisionSignal(decision=UserDecision.KEEP)\n",
    "await handle.signal(\"user_decision_signal\", signal_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gw4QMExOK63s"
   },
   "outputs": [],
   "source": [
    "# Let's send our Signal from the Client code\n",
    "# Step 1: If the input decision was to edit the the prompt, send a Signal to your Workflow Handle with the new signal data\n",
    "# Step 2: Run this code block to load it into the program\n",
    "async def send_user_decision_signal(client: Client, workflow_id: str) -> None:\n",
    "  handle = client.get_workflow_handle(workflow_id) # Get a handle on the Workflow Execution we want to send a Signal to.\n",
    "\n",
    "  while True:\n",
    "      print(\"\\n\" + \"=\" * 80)\n",
    "      print(\n",
    "          \"Research is complete! The response will output in the terminal window with the Worker running. What would you like to do?\"\n",
    "      )\n",
    "      print(\"1. Type 'keep' to approve the research and create PDF\")\n",
    "      print(\"2. Type 'edit' to modify the research\")\n",
    "      print(\"=\" * 80)\n",
    "\n",
    "      decision = input(\"Your decision (keep/edit): \").strip().lower()\n",
    "\n",
    "      if decision in {\"keep\", \"1\"}:\n",
    "          signal_data = UserDecisionSignal(decision=UserDecision.KEEP)\n",
    "          await handle.signal(\"user_decision_signal\", signal_data) # Send our Keep Signal to our Workflow Execution we have a handle on\n",
    "          print(\"Signal sent to keep research and create PDF\")\n",
    "          break\n",
    "      if decision in {\"edit\", \"2\"}:\n",
    "          additional_prompt_input = input(\"Enter additional instructions for the research (optional): \").strip()\n",
    "          additional_prompt = additional_prompt_input if additional_prompt_input else \"\"\n",
    "\n",
    "          signal_data = UserDecisionSignal(decision=UserDecision.EDIT, additional_prompt=additional_prompt)\n",
    "          # TODO Send our Signal to our Workflow Execution we have a handle on\n",
    "          print(\"Signal sent to regenerate research\")\n",
    "\n",
    "      else:\n",
    "          print(\"Please enter either 'keep', 'edit'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load and display the solution\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "notebook_dir = Path(os.getcwd())\n",
    "solution_file = notebook_dir / \"Solutions_03_Human_in_the_Loop\" / \"sending_signal_solution.py\"\n",
    "\n",
    "code = solution_file.read_text()\n",
    "\n",
    "print(\"Solution loaded:\")\n",
    "display(Markdown(f\"```python\\n{code}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FmMj5yaLIJJ"
   },
   "source": [
    "## Part 2: Adding Query Support in Temporal\n",
    "\n",
    "Now let's add **Query** support to our Workflow. Queries allow you to read workflow state without interrupting execution. This can be done during or even after the Workflow Execution. For example, you might want to:\n",
    "\n",
    "- **Monitor Progress of Long-Running Workflows**: A Client might want to receive updates on the progress, like the percentage of the task completed.\n",
    "- **Retrieve Results**: Queries can be used to fetch the results of Activities without waiting for the entire Workflow to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the current Worker so we can start fresh with Query support\n",
    "x = worker.cancel()\n",
    "\n",
    "if x:\n",
    "  print(\"Worker killed - ready to start fresh with Query support!\")\n",
    "else:\n",
    "  print(\"Worker was not running. Nothing to kill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hbb6Z4uEEvFI"
   },
   "source": [
    "## Developing Queries in Temporal\n",
    "\n",
    "Once a Query is issued, the Client waits for a response from the Workflow. Although Queries are typically used to access the state of an open (running) Workflow Execution, it is also possible to send a Query to a closed Workflow Execution. In either case, there must be at least one running Worker for the Task Queue to which that Workflow belongs.\n",
    "\n",
    "Your Query should not include any logic that generates commands (such as executing Activities). Remember, Queries are intended to be **read-only operations** that do not alter the Workflow's state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBKk05TPFDmx"
   },
   "source": [
    "## Handling a Query\n",
    "\n",
    "Let's create a Query which will allow external clients to read the current research content from a running Workflow without interrupting its execution.\n",
    "\n",
    "Similar to Signals, in the Python SDK, you can handle Queries by annotating a function within your Workflow with `@workflow.query`:\n",
    "\n",
    "```\n",
    "@workflow.query\n",
    "def get_research_result(self) -> str | None:\n",
    "    \"\"\"Query to get the current research result\"\"\"\n",
    "    return self._research_result\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77ZzdbGgIt1s"
   },
   "outputs": [],
   "source": [
    "# Run this to kill the current Worker\n",
    "x = worker.cancel()\n",
    "\n",
    "if x:\n",
    "  print(\"Worker killed\")\n",
    "else:\n",
    "  print(\"Worker was not running. Nothing to kill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a New Workflow Execution\n",
    "\n",
    "Now that the Worker is running with Query support, start a **new** workflow execution to test Queries.\n",
    "\n",
    "**Run the cells from Part 1 again (cells 44, 50) but with the new Query-enabled workflow, then run cells 60-61 to send Queries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eIvD56wHhWO"
   },
   "outputs": [],
   "source": [
    "# Adding a Query into our Workflow code\n",
    "# Step 1: In your Query handler, returns the `_research_result` field\n",
    "# Step 2: After the LLM call, set `_research_result` field to be the contents of the research (research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
    "# Step 3: Run this code block to load it into the program\n",
    "import asyncio\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "\n",
    "from temporalio import workflow\n",
    "\n",
    "@workflow.defn(sandboxed=False)\n",
    "class GenerateReportWorkflow:\n",
    "    def __init__(self) -> None:\n",
    "        self._current_prompt: str = \"\"\n",
    "        # Instance variable to store the Signal in\n",
    "        self._user_decision: UserDecisionSignal = UserDecisionSignal(decision=UserDecision.WAIT)\n",
    "        self._research_result: str = \"\"\n",
    "\n",
    "    # Method to handle the Signal\n",
    "    @workflow.signal\n",
    "    async def user_decision_signal(self, decision_data: UserDecisionSignal) -> None:\n",
    "        # Update the instance variable with the received Signal data\n",
    "        self._user_decision = decision_data\n",
    "\n",
    "    @workflow.query # Query to get the current research result\n",
    "    def get_research_result(self) -> str | None:\n",
    "        # TODO: Return the `_research_result` field\n",
    "\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "        self._current_prompt = input.prompt\n",
    "\n",
    "        llm_call_input = LLMCallInput(\n",
    "            prompt=self._current_prompt,\n",
    "            llm_api_key=input.llm_api_key,\n",
    "            llm_model=input.llm_research_model,\n",
    "        )\n",
    "\n",
    "        continue_agent_loop = True\n",
    "\n",
    "        while continue_agent_loop:\n",
    "            research_facts = await workflow.execute_activity(\n",
    "                llm_call,\n",
    "                llm_call_input,\n",
    "                start_to_close_timeout=timedelta(seconds=30),\n",
    "            )\n",
    "\n",
    "            # TODO: Set `_research_result` field to be the contents of the research (research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "            # Waiting for Signal with user decision\n",
    "            await workflow.wait_condition(lambda: self._user_decision.decision != UserDecision.WAIT)\n",
    "\n",
    "            if self._user_decision.decision == UserDecision.KEEP:\n",
    "                workflow.logger.info(\"User approved the research. Creating PDF...\")\n",
    "                continue_agent_loop = False\n",
    "            elif self._user_decision.decision == UserDecision.EDIT:\n",
    "                workflow.logger.info(\"User requested research modification.\")\n",
    "                if self._user_decision.additional_prompt != \"\":\n",
    "                    self._current_prompt = (\n",
    "                        f\"{self._current_prompt}\\n\\nAdditional instructions: {self._user_decision.additional_prompt}\"\n",
    "                    )\n",
    "                    workflow.logger.info(f\"Regenerating research with updated prompt: {self._current_prompt}\")\n",
    "                else:\n",
    "                    workflow.logger.info(\"No additional instructions provided. Regenerating with original prompt.\")\n",
    "                llm_call_input.prompt = self._current_prompt\n",
    "\n",
    "                # Set the decision back to WAIT for the next loop\n",
    "                self._user_decision = UserDecisionSignal(decision=UserDecision.WAIT)\n",
    "\n",
    "        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "        pdf_filename: str = await workflow.execute_activity(\n",
    "            create_pdf,\n",
    "            pdf_generation_input,\n",
    "            start_to_close_timeout=timedelta(seconds=20),\n",
    "        )\n",
    "\n",
    "        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load and display the solution\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "notebook_dir = Path(os.getcwd())\n",
    "solution_file = notebook_dir / \"Solutions_03_Human_in_the_Loop\" / \"handling_query_solution.py\"\n",
    "\n",
    "code = solution_file.read_text()\n",
    "\n",
    "print(\"Solution loaded:\")\n",
    "display(Markdown(f\"```python\\n{code}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sg3Bw8fLJpHD"
   },
   "outputs": [],
   "source": [
    "# Starting the Worker again\n",
    "worker = asyncio.create_task(run_worker())\n",
    "\n",
    " # Check if the task is in the set of all tasks\n",
    "if worker in asyncio.all_tasks():\n",
    "    # The sleep is necessary because of the async task scheduling in Jupyter\n",
    "    print(\"Task is currently active.\") # The Worker now registers the updated Workflow changes\n",
    "else:\n",
    "    print(\"Task is not found in active tasks (might have finished or not yet scheduled).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xx6oZ4UqGGBV"
   },
   "source": [
    "## Sending a Query\n",
    "\n",
    "After defining and setting a handler for the Queries in your Workflow, the next step is to send a Query, which is sent from a Temporal Client. To do this, use the query method. To do this, we will again:\n",
    "\n",
    "1. Get a handle of the Workflow Execution we will query\n",
    "2. Send a query with the `query` method.\n",
    "\n",
    "```python\n",
    "research_result = await handle.query(GenerateReportWorkflow.get_research_result)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMmo3_0MIPM5"
   },
   "outputs": [],
   "source": [
    "# Let's send our Query from the Client code\n",
    "\n",
    "async def query_research_result(client: Client, workflow_id: str) -> None:\n",
    "    handle = client.get_workflow_handle(workflow_id)\n",
    "\n",
    "    try:\n",
    "        research_result = await handle.query(GenerateReportWorkflow.get_research_result)\n",
    "        if research_result:\n",
    "            print(f\"Research Result: {research_result}\")\n",
    "        else:\n",
    "            print(\"Research Result: Not yet available\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Query failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTW9HXxGJUQX"
   },
   "outputs": [],
   "source": [
    "# Run this to send your Query\n",
    "send_query = asyncio.run(query_research_result(client, handle.id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSU6nnwcKbY6"
   },
   "source": [
    "## Putting it Together\n",
    "\n",
    "You can now make a research call, check your results by querying your Workflow, then choose to edit or keep the research!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbw85OliybvN"
   },
   "source": [
    "---\n",
    "# Exercise 3 - Human in the Loop\n",
    "\n",
    "* In these exercises you will:\n",
    "  * Review a modified version of the previous exercise and investigate the results in the Web UI\n",
    "  * Add a Signal to the exercise to provide the filename you wish to save the research report as\n",
    "  * Add a Query to the exercise to extract the character length of the research request\n",
    "* Go to the **Exercise** Directory in the Google Drive and open the **Practice** Directory\n",
    "* Open _01-An-AI-Agent-Practice.ipynb_ and follow the instructions\n",
    "* If you get stuck, raise your hand and someone will come by and help. You can also check the `Solution` directory for the answers\n",
    "* **You have 5 mins**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (edu-ai-workshop-mcp)",
   "language": "python",
   "name": "edu-ai-workshop-mcp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
