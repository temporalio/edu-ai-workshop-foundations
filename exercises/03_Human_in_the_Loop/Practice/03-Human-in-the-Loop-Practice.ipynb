{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekQPgGVwQb1a"
   },
   "source": [
    "# Exercise #3 - Human in the Loop\n",
    "\n",
    "In the previous exercise, you called an Activity to get the topic of a sentence using an LLM and used an LLM to create an image of that topic, finally adding it to your research report.\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "1. Review a modified version of the previous exercise and investigate the results in the Web UI\n",
    "2. Add a Signal to the exercise to provide the filename you wish to save the research report as\n",
    "3. Add a Query to the exercise to extract the character length of the research request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_uA13X5SYWe"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Before doing the exercise, you need to:\n",
    "\n",
    "- Install necessary dependencies\n",
    "- Create your `.env` file and supply your API key\n",
    "- Load the environment variables\n",
    "- Download and start a local Temporal Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28453,
     "status": "ok",
     "timestamp": 1756831877581,
     "user": {
      "displayName": "Angela Zhou",
      "userId": "04211316506850475003"
     },
     "user_tz": 240
    },
    "id": "m8UH2Il_Sp50",
    "outputId": "55f13c73-76ab-4df3-8221-884da9d7f615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m680.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.4/278.4 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install --quiet temporalio litellm reportlab python-dotenv requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXp3VIAdZuhO"
   },
   "source": [
    "### Create a `.env` File\n",
    "\n",
    "Next you'll create a `.env` file to store your API keys.\n",
    "In the file browser on the left, create a new file and name it `.env`.\n",
    "\n",
    "**Note**: It may disappear as soon as you create it. This is because Google Collab hides hidden files (files that start with a `.`) by default.\n",
    "To make this file appear, click the icon that is a crossed out eye and hidden files will appear.\n",
    "\n",
    "Then double click on the `.env` file and add the following line with your API key.\n",
    "\n",
    "```\n",
    "LLM_API_KEY = YOUR_API_KEY\n",
    "LLM_MODEL = \"openai/gpt-4o\"\n",
    "```\n",
    "\n",
    "By default this notebook uses OpenAI's GPT-4o.\n",
    "If you want to use a different LLM provider, look up the appropriate model name [in their documentation](https://docs.litellm.ai/docs/providers) and change the `LLM_MODEL` field and provide your API key.\n",
    "\n",
    "**To perform image generation, you will need an OpenAI key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSl-K8ATXLJ3"
   },
   "outputs": [],
   "source": [
    "# Create .env file\n",
    "with open(\".env\", \"w\") as fh:\n",
    "  fh.write(\"LLM_API_KEY = YOUR_API_KEY\\nLLM_MODEL = openai/gpt-4o\")\n",
    "\n",
    "# Now open the file and replace YOUR_API_KEY with your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PU75hvN-Qed"
   },
   "source": [
    "## Add Your LLM API Key **Before** Running the Following Code Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1756831988539,
     "user": {
      "displayName": "Angela Zhou",
      "userId": "04211316506850475003"
     },
     "user_tz": 240
    },
    "id": "fy4s0KTRdWxL",
    "outputId": "2775a462-fc26-405a-8149-32ef6a66044e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM API Key:  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\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables and configure LLM settings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Get LLM_API_KEY environment variable and print it to make sure that your .env file is properly loaded.\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"openai/gpt-4o\")\n",
    "LLM_API_KEY = os.getenv(\"LLM_API_KEY\", None)\n",
    "print(\"LLM API Key: \", LLM_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90AW0nTB0P2V"
   },
   "source": [
    "### Setting Up the Temporal Service\n",
    "\n",
    "Run the following blocks to setup & enable a local Temporal Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLHoJlR4R8Au"
   },
   "outputs": [],
   "source": [
    "# allows us to run the Temporal Asyncio event loop within the event loop of Jupyter Notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4647,
     "status": "ok",
     "timestamp": 1756832017373,
     "user": {
      "displayName": "Angela Zhou",
      "userId": "04211316506850475003"
     },
     "user_tz": 240
    },
    "id": "scrsqIOy0a_8",
    "outputId": "a11a7203-4564-4866-ef05-17ea2bbc7d03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mtemporal:\u001b[0m Downloading Temporal CLI latest\n",
      "\u001b[1mtemporal:\u001b[0m Temporal CLI installed at /root/.temporalio/bin/temporal\n",
      "\u001b[1mtemporal:\u001b[0m For convenience, we recommend adding it to your PATH\n",
      "\u001b[1mtemporal:\u001b[0m If using bash, run echo export PATH=\"\\$PATH:/root/.temporalio/bin\" >> ~/.bashrc\n"
     ]
    }
   ],
   "source": [
    "# Download the Temporal CLI.\n",
    "\n",
    "!curl -sSf https://temporal.download/cli.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Sure Your Temporal Web UI is Running\n",
    "\n",
    "1. You should have the Temporal Server running in your terminal (run `temporal server start-dev` if not).\n",
    "2. Then in your `Ports` tab on the bottom of this screen, find `8233` and click on the Globe icon to open the Temporal Web UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iw22wN9wX1Ok"
   },
   "source": [
    "## Part 1 - Run the below code blocks to load them into the proram\n",
    "\n",
    "Review this code and run it to understand what it does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSWFzYyb2UBJ"
   },
   "source": [
    "### Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMHdjdjL2FlT"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from enum import StrEnum\n",
    "\n",
    "class UserDecision(StrEnum):\n",
    "    KEEP = \"KEEP\"\n",
    "    EDIT = \"EDIT\"\n",
    "    WAIT = \"WAIT\"\n",
    "\n",
    "@dataclass\n",
    "class LLMCallInput:\n",
    "  prompt: str\n",
    "  llm_api_key: str\n",
    "  llm_model: str\n",
    "\n",
    "@dataclass\n",
    "class PDFGenerationInput:\n",
    "  content: str\n",
    "  image_url: str | None = None\n",
    "  filename: str = \"research_pdf\"\n",
    "\n",
    "@dataclass\n",
    "class GenerateReportInput:\n",
    "    prompt: str\n",
    "    llm_api_key: str\n",
    "    llm_research_model: str = \"openai/gpt-4o\"\n",
    "    llm_image_model: str = \"dall-e-3\"\n",
    "\n",
    "@dataclass\n",
    "class GenerateReportOutput:\n",
    "    result: str\n",
    "\n",
    "@dataclass\n",
    "class GenerateImageInput:\n",
    "    topic: str\n",
    "    llm_api_key: str\n",
    "    llm_model: str = \"dall-e-3\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UserDecisionSignal:\n",
    "    decision: UserDecision\n",
    "    additional_prompt: str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ys60mB8o2Vib"
   },
   "source": [
    "### Activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FEDm6EiYKB2"
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "import requests\n",
    "from litellm import completion, ModelResponse, image_generation\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image as RLImage\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.units import inch\n",
    "\n",
    "from temporalio import activity\n",
    "\n",
    "@activity.defn\n",
    "def llm_call(input: LLMCallInput) -> ModelResponse:\n",
    "    response = completion(\n",
    "      model=input.llm_model,\n",
    "      api_key=input.llm_api_key,\n",
    "      messages=[{ \"content\": input.prompt,\"role\": \"user\"}]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "@activity.defn\n",
    "def generate_ai_image(input: GenerateImageInput) -> ModelResponse:\n",
    "\n",
    "    image_prompt = f\"A cute, natural image of {input.topic}.\"\n",
    "\n",
    "    response = image_generation(\n",
    "        prompt=image_prompt,\n",
    "        model=input.llm_model,\n",
    "        api_key=input.llm_api_key\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "@activity.defn\n",
    "def create_pdf_activity(input: PDFGenerationInput) -> str:\n",
    "    doc = SimpleDocTemplate(f\"{input.filename}.pdf\", pagesize=letter)\n",
    "\n",
    "    styles = getSampleStyleSheet()\n",
    "    title_style = ParagraphStyle(\n",
    "        'CustomTitle',\n",
    "        parent=styles['Heading1'],\n",
    "        fontSize=24,\n",
    "        spaceAfter=30,\n",
    "        alignment=1\n",
    "    )\n",
    "\n",
    "    story = []\n",
    "    title = Paragraph(\"Research Report\", title_style)\n",
    "    story.append(title)\n",
    "    story.append(Spacer(1, 20))\n",
    "\n",
    "    if input.image_url is not None:\n",
    "      img_response = requests.get(input.image_url)\n",
    "      img_buffer = BytesIO(img_response.content)\n",
    "      img = RLImage(img_buffer, width=5*inch, height=5*inch)\n",
    "      story.append(img)\n",
    "      story.append(Spacer(1, 20))\n",
    "\n",
    "    paragraphs = input.content.split('\\n\\n')\n",
    "    for para in paragraphs:\n",
    "        if para.strip():\n",
    "            p = Paragraph(para.strip(), styles['Normal'])\n",
    "            story.append(p)\n",
    "            story.append(Spacer(1, 12))\n",
    "\n",
    "    doc.build(story)\n",
    "    return input.filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BE_tNaFU_dP"
   },
   "source": [
    "## Part 2 - Implementing a Signal\n",
    "\n",
    "Next you'll implement a Signal yourself. This Signal will act as a prompt that provides the filename to save the file as. However, if the user doesn't provide a response within twenty seconds, a default will be used instead.\n",
    "\n",
    "To do this, we be using the [wait_condition](https://python.temporal.io/temporalio.workflow.html#wait_condition) method.\n",
    "\n",
    "1. Add a new attribute, called `filename` and default it to a string set at `research_report`.\n",
    "2. Decorate the `filename_signal` Signal with `@workflow.signal`.\n",
    "3. We want to wait until the `filename_signal` is received or 20 seconds has elapsed, whichever happens first. Set the `wait_condition` to take in a timeout parameter of 20 seconds.\n",
    "4. Run the code block to load it into the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPKrNe3JqgQa"
   },
   "outputs": [],
   "source": [
    "# Run this codeblock\n",
    "@dataclass\n",
    "class FilenameSave:\n",
    "  filename: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-vJaE232x1h"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "\n",
    "from temporalio import workflow\n",
    "\n",
    "# sandboxed=False is a Notebook only requirement. You normally don't do this\n",
    "@workflow.defn(sandboxed=False)\n",
    "class GenerateReportWorkflow:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._current_prompt: str = \"\"\n",
    "        self._user_decision: UserDecisionSignal = UserDecisionSignal(decision=UserDecision.WAIT)\n",
    "        self._research_result: str | None = None\n",
    "        self._filename: # TODO: Set this to a string that has the default value of \"research_report\"\n",
    "\n",
    "    @workflow.signal\n",
    "    async def user_decision_signal(self, decision_data: UserDecisionSignal) -> None:\n",
    "        self._user_decision = decision_data\n",
    "\n",
    "    # TODO: Decorate this Signal with @workflow.signal\n",
    "    async def filename_signal(self, input: FilenameSave) -> None:\n",
    "        self._filename = input.filename\n",
    "\n",
    "    @workflow.query\n",
    "    def get_research_result(self) -> str | None:\n",
    "        return self._research_result\n",
    "\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "\n",
    "        self._current_prompt = input.prompt\n",
    "\n",
    "        llm_call_input = LLMCallInput(\n",
    "            prompt=self._current_prompt,\n",
    "            llm_api_key=input.llm_api_key,\n",
    "            llm_model=input.llm_research_model,\n",
    "        )\n",
    "\n",
    "        continue_agent_loop = True\n",
    "\n",
    "        while continue_agent_loop:\n",
    "            research_facts = await workflow.execute_activity(\n",
    "                llm_call,\n",
    "                llm_call_input,\n",
    "                start_to_close_timeout=timedelta(seconds=30),\n",
    "            )\n",
    "\n",
    "            # Store the research result for queries\n",
    "            self._research_result = research_facts[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "            print(\"Research complete!\")\n",
    "\n",
    "            print(\"Waiting for user decision. Send signal with 'keep' to create PDF or 'edit' to modify prompt.\")\n",
    "\n",
    "            await workflow.wait_condition(\n",
    "                lambda: self._user_decision.decision != UserDecision.WAIT\n",
    "            )\n",
    "            user_decision = self._user_decision\n",
    "\n",
    "            if user_decision.decision == UserDecision.KEEP:\n",
    "                print(\"User approved the research. Creating PDF...\")\n",
    "                continue_agent_loop = False\n",
    "            elif user_decision.decision == UserDecision.EDIT:\n",
    "                print(\"User requested research modification.\")\n",
    "                if user_decision.additional_prompt != \"\":\n",
    "                    self._current_prompt = f\"{self._current_prompt}\\n\\nAdditional instructions: {user_decision.additional_prompt}\"\n",
    "                    print(\n",
    "                        f\"Regenerating research with updated prompt: {self._current_prompt}\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        \"No additional instructions provided. Regenerating with original prompt.\"\n",
    "                    )\n",
    "                llm_call_input.prompt = self._current_prompt\n",
    "                self._user_decision = UserDecisionSignal(decision=UserDecision.WAIT)\n",
    "\n",
    "        subject_prompt = f\"What is the main topic of this sentence? Respond with only the topic in a single word or short phrase if multiple topics are detected. This response will be used for generating an AI image. No explanation. The sentence is: {self._current_prompt}\"\n",
    "        subject_input = LLMCallInput(prompt=subject_prompt, llm_api_key=LLM_API_KEY, llm_model=LLM_MODEL)\n",
    "\n",
    "        topic_call = await workflow.execute_activity(\n",
    "            llm_call,\n",
    "            subject_input,\n",
    "            start_to_close_timeout=timedelta(seconds=30),\n",
    "        )\n",
    "\n",
    "        topic = topic_call[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        # Used the new GenerateImageInput dataclass to create the input object for the Activity\n",
    "        image_input = GenerateImageInput(topic=topic, llm_api_key=LLM_API_KEY)\n",
    "\n",
    "        # Called the new generate_ai_image Activity, passing in the image_input parameter made above\n",
    "        ai_image = await workflow.execute_activity(\n",
    "            generate_ai_image,\n",
    "            image_input,\n",
    "            start_to_close_timeout=timedelta(seconds=30),\n",
    "        )\n",
    "\n",
    "        # Exctract the image_url form the Activity call\n",
    "        image_url = ai_image[\"data\"][0][\"url\"]\n",
    "\n",
    "        try:\n",
    "          await workflow.wait_condition(\n",
    "              lambda: self._filename != \"research_report\",\n",
    "              timeout=# TODO Set the timeout to be 20 seconds. It should take a format of something like `timedelta(seconds=5)`.\n",
    "          )\n",
    "        except asyncio.TimeoutError:\n",
    "          print(\"20 seconds have passed. The program will continue and your file will automatically be named 'research_paper.pdf'.\")\n",
    "\n",
    "        # Add the image_url parameter to the PDF Generation so the image is included\n",
    "        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"], image_url=image_url, filename=self._filename)\n",
    "\n",
    "        pdf_filename = await workflow.execute_activity(\n",
    "            create_pdf_activity,\n",
    "            pdf_generation_input,\n",
    "            start_to_close_timeout=timedelta(seconds=10),\n",
    "        )\n",
    "\n",
    "        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Prepare your Worker\n",
    "\n",
    "1. Add your `generate_ai_image` Activity to your list of Activities registered on the Worker\n",
    "2. Run the codeblock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNZZ71MHBwqp"
   },
   "outputs": [],
   "source": [
    "from temporalio.client import Client\n",
    "from temporalio.worker import Worker\n",
    "import concurrent.futures\n",
    "\n",
    "async def run_worker() -> None:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "\n",
    "    client = await Client.connect(\"localhost:7233\", namespace=\"default\")\n",
    "\n",
    "    # Run the Worker\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as activity_executor:\n",
    "        worker = Worker(\n",
    "            client,\n",
    "            task_queue=\"research\",\n",
    "            workflows=[GenerateReportWorkflow],\n",
    "            activities=[llm_call, create_pdf_activity], # TODO: Add your create_pdf_activity\n",
    "            activity_executor=activity_executor\n",
    "        )\n",
    "\n",
    "        print(f\"Starting the worker....\")\n",
    "        await worker.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUpctA0H56gl"
   },
   "outputs": [],
   "source": [
    "# Start a new worker\n",
    "\n",
    "# If you didn't kill the previous worker, uncomment this and run it first\n",
    "#worker.cancel()\n",
    "\n",
    "worker = asyncio.create_task(run_worker())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start your Workflow\n",
    "# Run this code block\n",
    "import datetime\n",
    "import uuid\n",
    "\n",
    "client = await Client.connect(\"localhost:7233\")\n",
    "\n",
    "print(\"Welcome to the Research Report Generator!\")\n",
    "prompt = input(\"Enter your research topic or question: \").strip()\n",
    "\n",
    "if not prompt:\n",
    "    prompt = \"Give me 5 fun and fascinating facts about tardigrades. Make them interesting and educational!\"\n",
    "    print(f\"No prompt entered. Using default: {prompt}\")\n",
    "\n",
    "handle = await client.start_workflow(\n",
    "    GenerateReportWorkflow.run,\n",
    "    GenerateReportInput(prompt=prompt, llm_api_key=LLM_API_KEY),\n",
    "    id=f\"generate-research-report-workflow-{uuid.uuid4()}\",\n",
    "    task_queue=\"research\",\n",
    ")\n",
    "\n",
    "print(f\"Started workflow. Workflow ID: {handle.id}, RunID {handle.result_run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 Sending a Signal with the Client\n",
    "\n",
    "1. After we get the filename from the input, send the `filename_signal` to the Workflow handle, passing in `FilenameSave(filename=filename))`. Don't forget to use await.\n",
    "2. Run the code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7134,
     "status": "ok",
     "timestamp": 1756834635141,
     "user": {
      "displayName": "Angela Zhou",
      "userId": "04211316506850475003"
     },
     "user_tz": 240
    },
    "id": "stv5t7sL4JH2",
    "outputId": "b41fcd83-5b9b-4f96-8cce-789a18a2f9be"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import uuid\n",
    "\n",
    "async def send_user_decision_signal(client: Client, workflow_id: str) -> None:\n",
    "    handle = client.get_workflow_handle(workflow_id)\n",
    "    loop = asyncio.get_running_loop()\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\n",
    "            \"Calling LLM! See the response in your Web UI in the output of the `llm_call` Activity. Would you like to keep or edit it?\"\n",
    "        )\n",
    "        print(\"1. Type 'keep' to approve the output and create PDF\")\n",
    "        print(\"2. Type 'edit' to modify the output\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # When running input in async code, run in an executor to not block the event loop\n",
    "        decision = await loop.run_in_executor(None, input, \"Your decision (keep/edit): \")\n",
    "        decision = decision.strip().lower()\n",
    "\n",
    "        if decision in {\"keep\", \"1\"}:\n",
    "            signal_data = UserDecisionSignal(decision=UserDecision.KEEP)\n",
    "            await handle.signal(\"user_decision_signal\", signal_data) # Send our Keep Signal to our Workflow Execution we have a handle on\n",
    "            print(\"Signal sent to keep output and create PDF\")\n",
    "            break\n",
    "        if decision in {\"edit\", \"2\"}:\n",
    "            additional_prompt_input = input(\"Enter additional instructions to edit the output (optional): \").strip()\n",
    "            additional_prompt = additional_prompt_input if additional_prompt_input else \"\"\n",
    "            signal_data = UserDecisionSignal(decision=UserDecision.EDIT, additional_prompt=additional_prompt)\n",
    "            await handle.signal(\"user_decision_signal\", signal_data)\n",
    "            print(\"Signal sent to regenerate research\")\n",
    "\n",
    "        else:\n",
    "            print(\"Please enter either 'keep', 'edit'\")\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "    print(\"What do you want to name the file? After 20 seconds, the program will continue and your file will automatically be named 'research_paper.pdf'.\")\n",
    "    filename = await loop.run_in_executor(None, input, \"Enter the filename: \")\n",
    "    # TODO: Send the `filename_signal` to the Workflow handle, passing in `FilenameSave(filename=filename))`. Don't forget to use await.\n",
    "    end_time = datetime.datetime.now()\n",
    "    if end_time - start_time > datetime.timedelta(seconds=20):\n",
    "        print(\"20 seconds have passed. The program will continue and your file will automatically be named 'research_paper.pdf'.\")\n",
    "    else:\n",
    "        print(f\"Your file will be saved as {filename}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaU2t6GERaN3"
   },
   "source": [
    "## Test Your Signal\n",
    "\n",
    "Run the code, and provide a filename for it to be saved as within the 20 seconds timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23237,
     "status": "ok",
     "timestamp": 1756834665102,
     "user": {
      "displayName": "Angela Zhou",
      "userId": "04211316506850475003"
     },
     "user_tz": 240
    },
    "id": "GXJjTpMo5mHe",
    "outputId": "23e48e47-20f4-4032-9aa7-a7041cecd84b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Research is complete!\n",
      "1. Type 'keep' to approve the research and create PDF\n",
      "2. Type 'edit' to modify the research\n",
      "3. Type 'query' to query for research result. If querying, wait for `Reserch Complete` to appear in terminal window with Worker running first.\n",
      "==================================================\n",
      "Your decision (keep/edit/query): query\n",
      "Research Result: Certainly! Here are two facts about Pikachu:\n",
      "\n",
      "1. **Iconic Pokémon**: Pikachu is arguably the most famous Pokémon and serves as the franchise's official mascot. It was first introduced in Pokémon Red and Blue (or Pokémon Red and Green in Japan) and quickly became beloved by fans worldwide due to its adorable appearance and prominent role in the Pokémon animated series as Ash Ketchum's primary Pokémon companion.\n",
      "\n",
      "2. **Electric Type Abilities**: Pikachu is an Electric-type Pokémon known for its ability to generate electricity. Its signature move is \"Thunderbolt,\" where it unleashes a powerful electric attack against its opponents. Pikachu's cheeks are capable of storing electricity, which it can release to shock other Pokémon or defend itself.\n",
      "\n",
      "==================================================\n",
      "Research is complete!\n",
      "1. Type 'keep' to approve the research and create PDF\n",
      "2. Type 'edit' to modify the research\n",
      "3. Type 'query' to query for research result. If querying, wait for `Reserch Complete` to appear in terminal window with Worker running first.\n",
      "==================================================\n",
      "Your decision (keep/edit/query): keep\n",
      "Signal sent to keep research and create PDF\n",
      "What do you want to name the file? After 120 seconds, the program will continue and your file will automatically be named 'research_paper.pdf'.\n",
      "User approved the research. Creating PDF...\n",
      "Enter the filename: pikachu-party\n",
      "Your file will be saved as pikachu-party.pdf\n",
      "Result: GenerateReportOutput(result='Successfully created research report PDF: pikachu-party.pdf')\n"
     ]
    }
   ],
   "source": [
    "signal_task = asyncio.create_task(send_user_decision_signal(client, handle.id))\n",
    "\n",
    "try:\n",
    "    result = await handle.result()\n",
    "    signal_task.cancel()\n",
    "    print(f\"Result: {result}\")\n",
    "except Exception as e:\n",
    "    signal_task.cancel()\n",
    "    print(f\"Workflow failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check your file explorer to see your new file\n",
    "\n",
    "It should be named the file you gave it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4iN3KNdRkPG"
   },
   "source": [
    "## Test Your Signal - Timeout Path\n",
    "\n",
    "Let's test what happens when you don't give your file a name in 20 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill any previous workers that may still be running\n",
    "x = worker.cancel()\n",
    "\n",
    "# Start a new worker\n",
    "worker = asyncio.create_task(run_worker())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start your Workflow\n",
    "# Run this code block\n",
    "import datetime\n",
    "import uuid\n",
    "\n",
    "client = await Client.connect(\"localhost:7233\")\n",
    "\n",
    "print(\"Welcome to the Research Report Generator!\")\n",
    "prompt = input(\"Enter your research topic or question: \").strip()\n",
    "\n",
    "if not prompt:\n",
    "    prompt = \"Give me 5 fun and fascinating facts about tardigrades. Make them interesting and educational!\"\n",
    "    print(f\"No prompt entered. Using default: {prompt}\")\n",
    "\n",
    "handle = await client.start_workflow(\n",
    "    GenerateReportWorkflow.run,\n",
    "    GenerateReportInput(prompt=prompt, llm_api_key=LLM_API_KEY),\n",
    "    id=f\"generate-research-report-workflow-{uuid.uuid4()}\",\n",
    "    task_queue=\"research\",\n",
    ")\n",
    "\n",
    "print(f\"Started workflow. Workflow ID: {handle.id}, RunID {handle.result_run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send your Signal. Run this code block.\n",
    "# Enter \"Keep\" to generate the PDF\n",
    "# After 20 seconds is up, you'll see \"research_result.pdf\" saved in your file explorer since you did not give it a filename\n",
    "# Click the Escape button\n",
    "signal_task = asyncio.create_task(send_user_decision_signal(client, handle.id))\n",
    "\n",
    "try:\n",
    "    result = await handle.result()\n",
    "    signal_task.cancel()\n",
    "    print(f\"Result: {result}\")\n",
    "except Exception as e:\n",
    "    signal_task.cancel()\n",
    "    print(f\"Workflow failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaIYuART6KKY"
   },
   "source": [
    "### Watch the Execuction in the Web UI\n",
    "\n",
    "Open the Web UI and compare the executions.\n",
    "\n",
    "- What was different in the Event History/Timeline view between the two?\n",
    "- How did the Activity know what to name the file? Can you find how this data is relayed in the Event History?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYhiKvcYzYpE"
   },
   "source": [
    "## Part 5 - Adding Queries\n",
    "\n",
    "We'll now add queries! You've already seen an example of looking at the research content in the last notebook. Let's now add in a query to see the character count of your generated research.\n",
    "\n",
    "1. We'll first add in a new attribute, called `character_count` that is set to an integer. It should default to 0.\n",
    "2. Handle a `get_research_stats` Query by anotating it with `@workflow.query`. Have it return its attribute `character_count`.\n",
    "3. After the line where your Workflow sets the `research_result` variable, set your `character_count` variable to `len(research_facts[\"choices\"][0][\"message\"][\"content\"])`.\n",
    "4. Run the codeblock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSs52gZ1za9J"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "\n",
    "from temporalio import workflow\n",
    "\n",
    "@workflow.defn(sandboxed=False)\n",
    "class GenerateReportWorkflow:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._current_prompt: str = \"\"\n",
    "        self._user_decision: UserDecisionSignal = UserDecisionSignal(decision=UserDecision.WAIT)\n",
    "        self._research_result: str = \"\"\n",
    "        # TODO: Add a new attribute, called `character_count` that is set to an integer. It should default to 0.\n",
    "\n",
    "    @workflow.signal\n",
    "    async def user_decision_signal(self, decision_data: UserDecisionSignal) -> None:\n",
    "        self._user_decision = decision_data\n",
    "\n",
    "    @workflow.query\n",
    "    def get_research_result(self) -> str | None:\n",
    "        return self._research_result\n",
    "\n",
    "    # TODO: Handle a `get_research_stats` Query by annotating it with `@workflow.query`.\n",
    "    # TODO: Have the `get_research_stats` Query return its attribute `character_count`.\n",
    "\n",
    "    @workflow.run\n",
    "    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n",
    "        self._current_prompt = input.prompt\n",
    "\n",
    "        llm_call_input = LLMCallInput(\n",
    "            prompt=self._current_prompt,\n",
    "            llm_api_key=input.llm_api_key,\n",
    "            llm_model=input.llm_research_model,\n",
    "        )\n",
    "\n",
    "        continue_agent_loop = True\n",
    "\n",
    "        while continue_agent_loop:\n",
    "            research_facts = await workflow.execute_activity(\n",
    "                llm_call,\n",
    "                llm_call_input,\n",
    "                start_to_close_timeout=timedelta(seconds=30),\n",
    "            )\n",
    "\n",
    "            self._research_result = research_facts[\"choices\"][0][\"message\"][\"content\"]\n",
    "            # TODO: Set your `character_count` attribute to `len(research_facts[\"choices\"][0][\"message\"][\"content\"])`.\n",
    "\n",
    "            await workflow.wait_condition(lambda: self._user_decision.decision != UserDecision.WAIT)\n",
    "\n",
    "            if self._user_decision.decision == UserDecision.KEEP:\n",
    "                print(\"User approved the research. Creating PDF...\")\n",
    "                continue_agent_loop = False\n",
    "            elif self._user_decision.decision == UserDecision.EDIT:\n",
    "                print(\"User requested research modification.\")\n",
    "                if self._user_decision.additional_prompt != \"\":\n",
    "                    self._current_prompt = (\n",
    "                        f\"{self._current_prompt}\\n\\nAdditional instructions: {self._user_decision.additional_prompt}\"\n",
    "                    )\n",
    "                    print(f\"Regenerating research with updated prompt: {self._current_prompt}\")\n",
    "                else:\n",
    "                    print(\"No additional instructions provided. Regenerating with original prompt.\")\n",
    "                llm_call_input.prompt = self._current_prompt\n",
    "                self._user_decision = UserDecisionSignal(decision=UserDecision.WAIT)\n",
    "\n",
    "        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "        pdf_filename: str = await workflow.execute_activity(\n",
    "            create_pdf_activity,\n",
    "            pdf_generation_input,\n",
    "            start_to_close_timeout=timedelta(seconds=20),\n",
    "        )\n",
    "\n",
    "        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bYqRtpUzyqN"
   },
   "outputs": [],
   "source": [
    "# Kill any previous workers that may still be running\n",
    "x = worker.cancel()\n",
    "\n",
    "# Start a new worker\n",
    "worker = asyncio.create_task(run_worker())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6 - Sending a Query with the Client\n",
    "\n",
    "1. In the `query_research_stats` function, in the `query` function, pass in `GenerateReportWorkflow.get_research_stats`.\n",
    "2. In the `print(f\"Research character count: {}\")` statement, print out your `stats` variable.\n",
    "3. Run the codeblock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGKgRZ4tz1Kc"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "async def query_research_stats(client: Client, workflow_id: str) -> None:\n",
    "    handle = client.get_workflow_handle(workflow_id)\n",
    "    try:\n",
    "        stats = await handle.query() # TODO: Pass in `GenerateReportWorkflow.get_research_stats` in the query method.\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(f\"Research character count: {}\") # TODO: Print out your `stats` variable.\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Research stats query failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start your Workflow\n",
    "# Run this code block\n",
    "import datetime\n",
    "import uuid\n",
    "\n",
    "client = await Client.connect(\"localhost:7233\")\n",
    "\n",
    "print(\"Welcome to the Research Report Generator!\")\n",
    "prompt = input(\"Enter your research topic or question: \").strip()\n",
    "\n",
    "if not prompt:\n",
    "    prompt = \"Give me 5 fun and fascinating facts about tardigrades. Make them interesting and educational!\"\n",
    "    print(f\"No prompt entered. Using default: {prompt}\")\n",
    "\n",
    "handle = await client.start_workflow(\n",
    "    GenerateReportWorkflow.run,\n",
    "    GenerateReportInput(prompt=prompt, llm_api_key=LLM_API_KEY),\n",
    "    id=f\"generate-research-report-workflow-{uuid.uuid4()}\",\n",
    "    task_queue=\"research\",\n",
    ")\n",
    "\n",
    "print(f\"Started workflow. Workflow ID: {handle.id}, RunID {handle.result_run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rpkXzFQz29d"
   },
   "outputs": [],
   "source": [
    "# Run this to send your Query\n",
    "send_query = asyncio.run(query_research_stats(client, handle.id))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
