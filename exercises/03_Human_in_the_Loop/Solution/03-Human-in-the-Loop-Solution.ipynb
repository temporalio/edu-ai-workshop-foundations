{"cells":[{"cell_type":"markdown","metadata":{"id":"ekQPgGVwQb1a"},"source":["# Exercise #3 - Human in the Loop\n","\n","In the previous exercise, you called an Activity to get the topic of a sentence using an LLM and used an LLM to create an image of that topic, finally adding it to your research report.\n","\n","In this exercise, you will:\n","\n","1. Review a modified version of the previous exercise and investigate the results in the Web UI\n","2. Add a Signal to the exercise to provide the filename you wish to save the research report as\n","3. Add a Query to the exercise to extract the character length of the research request\n","\n","Fill in the TODO instructions. Go to the `solution` directory if you need help."]},{"cell_type":"markdown","metadata":{"id":"t_uA13X5SYWe"},"source":["## Setup\n","\n","Before doing the exercise, you need to:\n","\n","- Install necessary dependencies\n","- Create your `.env` file and supply your API key\n","- Load the environment variables\n","- Download and start a local Temporal Service"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17937,"status":"ok","timestamp":1756905767186,"user":{"displayName":"Angela Zhou","userId":"04211316506850475003"},"user_tz":240},"id":"m8UH2Il_Sp50","outputId":"ab260520-5ad6-452e-fc53-247922d08c14"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.4/278.4 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["%pip install --quiet temporalio litellm reportlab python-dotenv requests"]},{"cell_type":"markdown","metadata":{"id":"HXp3VIAdZuhO"},"source":["### Create a `.env` File\n","\n","Next you'll create a `.env` file to store your API keys.\n","In the file browser on the left, create a new file and name it `.env`.\n","\n","**Note**: It may disappear as soon as you create it. This is because Google Collab hides hidden files (files that start with a `.`) by default.\n","To make this file appear, click the icon that is a crossed out eye and hidden files will appear.\n","\n","Then double click on the `.env` file and add the following line with your API key.\n","\n","```\n","LLM_API_KEY = YOUR_API_KEY\n","LLM_MODEL = \"openai/gpt-4o\"\n","```\n","\n","By default this notebook uses OpenAI's GPT-4o.\n","If you want to use a different LLM provider, look up the appropriate model name [in their documentation](https://docs.litellm.ai/docs/providers) and change the `LLM_MODEL` field and provide your API key.\n","\n","**To perform image generation, you will need an OpenAI key**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tSl-K8ATXLJ3"},"outputs":[],"source":["# Create .env file\n","with open(\".env\", \"w\") as fh:\n","  fh.write(\"LLM_API_KEY = YOUR_API_KEY\\nLLM_MODEL = openai/gpt-4o\")\n","\n","# Now open the file and replace YOUR_API_KEY with your API key"]},{"cell_type":"markdown","metadata":{"id":"4PU75hvN-Qed"},"source":["## Add Your LLM API Key **Before** Running the Following Code Block"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1756905796783,"user":{"displayName":"Angela Zhou","userId":"04211316506850475003"},"user_tz":240},"id":"fy4s0KTRdWxL","outputId":"755db2ac-5430-4439-db32-f37ee8916fed"},"outputs":[{"name":"stdout","output_type":"stream","text":["LLM API Key:  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\n"]}],"source":["# Load environment variables and configure LLM settings\n","import os\n","from dotenv import load_dotenv\n","\n","load_dotenv(override=True)\n","\n","\n","# Get LLM_API_KEY environment variable and print it to make sure that your .env file is properly loaded.\n","LLM_MODEL = os.getenv(\"LLM_MODEL\", \"openai/gpt-4o\")\n","LLM_API_KEY = os.getenv(\"LLM_API_KEY\", None)\n","print(\"LLM API Key: \", LLM_API_KEY)"]},{"cell_type":"markdown","metadata":{"id":"90AW0nTB0P2V"},"source":["### Setting Up the Temporal Service\n","\n","Run the following blocks to setup & enable a local Temporal Service"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TLHoJlR4R8Au"},"outputs":[],"source":["# allows us to run the Temporal Asyncio event loop within the event loop of Jupyter Notebooks\n","import nest_asyncio\n","nest_asyncio.apply()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2929,"status":"ok","timestamp":1756905801746,"user":{"displayName":"Angela Zhou","userId":"04211316506850475003"},"user_tz":240},"id":"scrsqIOy0a_8","outputId":"4e3190b2-02aa-4964-b9c9-29935cedca87"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1mtemporal:\u001b[0m Downloading Temporal CLI latest\n","\u001b[1mtemporal:\u001b[0m Temporal CLI installed at /root/.temporalio/bin/temporal\n","\u001b[1mtemporal:\u001b[0m For convenience, we recommend adding it to your PATH\n","\u001b[1mtemporal:\u001b[0m If using bash, run echo export PATH=\"\\$PATH:/root/.temporalio/bin\" >> ~/.bashrc\n"]}],"source":["# Download the Temporal CLI.\n","\n","!curl -sSf https://temporal.download/cli.sh | sh"]},{"cell_type":"markdown","metadata":{},"source":["## Make Sure Your Temporal Web UI is Running\n","\n","1. You should have the Temporal Server running in your terminal (run `temporal server start-dev` if not).\n","2. Then in your `Ports` tab on the bottom of this screen, find `8233` and click on the Globe icon to open the Temporal Web UI."]},{"cell_type":"markdown","metadata":{"id":"Iw22wN9wX1Ok"},"source":["## Part 1 - Running a Modified Version the Previous Exercise\n","\n","The code below is a modified version of the previous exercise, including the loop that was included in the previous section's notebook. Review this code and run it to understand what it does.\n","\n","**This code should run with 0 modifications.**"]},{"cell_type":"markdown","metadata":{"id":"lSWFzYyb2UBJ"},"source":["### Models\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pMHdjdjL2FlT"},"outputs":[],"source":["from dataclasses import dataclass\n","from enum import StrEnum\n","\n","\n","class UserDecision(StrEnum):\n","    KEEP = \"KEEP\"\n","    EDIT = \"EDIT\"\n","    WAIT = \"WAIT\"\n","\n","@dataclass\n","class LLMCallInput:\n","  prompt: str\n","  llm_api_key: str\n","  llm_model: str\n","\n","@dataclass\n","class PDFGenerationInput:\n","  content: str\n","  image_url: str | None = None\n","  filename: str = \"research_pdf\"\n","\n","@dataclass\n","class GenerateReportInput:\n","    prompt: str\n","    llm_api_key: str\n","    llm_research_model: str = \"openai/gpt-4o\"\n","    llm_image_model: str = \"dall-e-3\"\n","\n","@dataclass\n","class GenerateReportOutput:\n","    result: str\n","\n","@dataclass\n","class GenerateImageInput:\n","    topic: str\n","    llm_api_key: str\n","    llm_model: str = \"dall-e-3\"\n","\n","\n","@dataclass\n","class UserDecisionSignal:\n","    decision: UserDecision\n","    additional_prompt: str = \"\""]},{"cell_type":"markdown","metadata":{"id":"Ys60mB8o2Vib"},"source":["### Activities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FEDm6EiYKB2"},"outputs":[],"source":["from io import BytesIO\n","\n","import requests\n","from litellm import completion, ModelResponse, image_generation\n","from reportlab.lib.pagesizes import letter\n","from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image as RLImage\n","from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n","from reportlab.lib.units import inch\n","\n","from temporalio import activity\n","\n","@activity.defn\n","def llm_call(input: LLMCallInput) -> ModelResponse:\n","    response = completion(\n","      model=input.llm_model,\n","      api_key=input.llm_api_key,\n","      messages=[{ \"content\": input.prompt,\"role\": \"user\"}]\n","    )\n","    return response\n","\n","@activity.defn\n","def generate_ai_image(input: GenerateImageInput) -> ModelResponse:\n","\n","    image_prompt = f\"A cute, natural image of {input.topic}.\"\n","\n","    response = image_generation(\n","        prompt=image_prompt,\n","        model=input.llm_model,\n","        api_key=input.llm_api_key\n","    )\n","\n","    return response\n","\n","@activity.defn\n","def create_pdf_activity(input: PDFGenerationInput) -> str:\n","    doc = SimpleDocTemplate(f\"{input.filename}.pdf\", pagesize=letter)\n","\n","    styles = getSampleStyleSheet()\n","    title_style = ParagraphStyle(\n","        'CustomTitle',\n","        parent=styles['Heading1'],\n","        fontSize=24,\n","        spaceAfter=30,\n","        alignment=1\n","    )\n","\n","    story = []\n","    title = Paragraph(\"Research Report\", title_style)\n","    story.append(title)\n","    story.append(Spacer(1, 20))\n","\n","    if input.image_url is not None:\n","      img_response = requests.get(input.image_url)\n","      img_buffer = BytesIO(img_response.content)\n","      img = RLImage(img_buffer, width=5*inch, height=5*inch)\n","      story.append(img)\n","      story.append(Spacer(1, 20))\n","\n","    paragraphs = input.content.split('\\n\\n')\n","    for para in paragraphs:\n","        if para.strip():\n","            p = Paragraph(para.strip(), styles['Normal'])\n","            story.append(p)\n","            story.append(Spacer(1, 12))\n","\n","    doc.build(story)\n","    return input.filename"]},{"cell_type":"markdown","metadata":{"id":"uS8SgW472Xto"},"source":["### Workflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sUuRKXiXYNey"},"outputs":[],"source":["import asyncio\n","from datetime import timedelta\n","import logging\n","\n","from temporalio import workflow\n","\n","# sandboxed=False is a Notebook only requirement. You normally don't do this\n","@workflow.defn(sandboxed=False)\n","class GenerateReportWorkflow:\n","\n","    def __init__(self) -> None:\n","        self._current_prompt: str = \"\"\n","        self._user_decision: UserDecisionSignal = UserDecisionSignal(decision=UserDecision.WAIT)\n","        self._research_result: str | None = None\n","\n","    @workflow.signal\n","    async def user_decision_signal(self, decision_data: UserDecisionSignal) -> None:\n","        self._user_decision = decision_data\n","\n","    @workflow.query\n","    def get_research_result(self) -> str | None:\n","        return self._research_result\n","\n","    @workflow.run\n","    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n","\n","        self._current_prompt = input.prompt\n","\n","        llm_call_input = LLMCallInput(\n","            prompt=self._current_prompt,\n","            llm_api_key=input.llm_api_key,\n","            llm_model=input.llm_research_model,\n","        )\n","\n","        continue_agent_loop = True\n","\n","        while continue_agent_loop:\n","            research_facts = await workflow.execute_activity(\n","                llm_call,\n","                llm_call_input,\n","                start_to_close_timeout=timedelta(seconds=30),\n","            )\n","\n","            # Store the research result for queries\n","            self._research_result = research_facts[\"choices\"][0][\"message\"][\"content\"]\n","\n","            print(\"Research complete!\")\n","\n","            print(\"Waiting for user decision. Send signal with 'keep' to create PDF or 'edit' to modify prompt.\")\n","\n","            await workflow.wait_condition(\n","                lambda: self._user_decision.decision != UserDecision.WAIT\n","            )\n","            user_decision = self._user_decision\n","\n","            if user_decision.decision == UserDecision.KEEP:\n","                print(\"User approved the research. Creating PDF...\")\n","                continue_agent_loop = False\n","            elif user_decision.decision == UserDecision.EDIT:\n","                print(\"User requested research modification.\")\n","                if user_decision.additional_prompt != \"\":\n","                    self._current_prompt = f\"{self._current_prompt}\\nAdditional instructions: {user_decision.additional_prompt}\"\n","                    print(\n","                        f\"Regenerating research with updated prompt: {self._current_prompt}\"\n","                    )\n","                else:\n","                    print(\n","                        \"No additional instructions provided. Regenerating with original prompt.\"\n","                    )\n","                llm_call_input.prompt = self._current_prompt\n","                self._user_decision = UserDecisionSignal(decision=UserDecision.WAIT)\n","\n","        subject_prompt = f\"What is the main topic of this sentence? Respond with only the topic in a single word or short phrase if multiple topics are detected. This response will be used for generating an AI image. No explanation. The sentence is: {self._current_prompt}\"\n","        subject_input = LLMCallInput(prompt=subject_prompt, llm_api_key=LLM_API_KEY, llm_model=LLM_MODEL)\n","\n","        topic_call = await workflow.execute_activity(\n","            llm_call,\n","            subject_input,\n","            start_to_close_timeout=timedelta(seconds=30),\n","        )\n","\n","        topic = topic_call[\"choices\"][0][\"message\"][\"content\"]\n","\n","        # Used the new GenerateImageInput dataclass to create the input object for the Activity\n","        image_input = GenerateImageInput(topic=topic, llm_api_key=LLM_API_KEY)\n","\n","        # Called the new generate_ai_image Activity, passing in the image_input parameter made above\n","        ai_image = await workflow.execute_activity(\n","            generate_ai_image,\n","            image_input,\n","            start_to_close_timeout=timedelta(seconds=30),\n","        )\n","\n","        # Exctract the image_url form the Activity call\n","        image_url = ai_image[\"data\"][0][\"url\"]\n","\n","        # Add the image_url parameter to the PDF Generation so the image is included\n","        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"], image_url=image_url, filename=topic)\n","\n","        pdf_filename = await workflow.execute_activity(\n","            create_pdf_activity,\n","            pdf_generation_input,\n","            start_to_close_timeout=timedelta(seconds=10),\n","        )\n","\n","        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}\")"]},{"cell_type":"markdown","metadata":{"id":"qlbhDExL4C1n"},"source":["### Worker"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MRaj3kXa4Bpt"},"outputs":[],"source":["from temporalio.client import Client\n","from temporalio.worker import Worker\n","import concurrent.futures\n","\n","async def run_worker() -> None:\n","    logging.basicConfig(level=logging.INFO)\n","    logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n","\n","    client = await Client.connect(\"localhost:7233\", namespace=\"default\")\n","\n","    # Run the Worker\n","    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as activity_executor:\n","        worker = Worker(\n","            client,\n","            task_queue=\"research\",\n","            workflows=[GenerateReportWorkflow],\n","            activities=[llm_call, create_pdf_activity, generate_ai_image],\n","            activity_executor=activity_executor\n","        )\n","\n","        print(f\"Starting the worker....\")\n","        await worker.run()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BzJrRRh_IrhO"},"outputs":[],"source":["# Start a new worker\n","worker = asyncio.create_task(run_worker())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8117,"status":"ok","timestamp":1756905827399,"user":{"displayName":"Angela Zhou","userId":"04211316506850475003"},"user_tz":240},"id":"WX7t-oANslrY","outputId":"3cc9e489-8215-4266-f9ea-4046a344f695"},"outputs":[{"name":"stdout","output_type":"stream","text":["Welcome to the Research Report Generator!\n","Enter your research topic or question: Give me 2 facts about pikachus\n","Started workflow. Workflow ID: generate-research-report-workflow, RunID 01990fbf-6beb-74f9-aad8-c5bcd131aa8b\n"]}],"source":["import uuid\n","\n","async def query_research_result(client: Client, workflow_id: str) -> None:\n","    handle = client.get_workflow_handle(workflow_id)\n","\n","    try:\n","        research_result = await handle.query(GenerateReportWorkflow.get_research_result)\n","        if research_result:\n","            print(f\"Research Result: {research_result}\")\n","        else:\n","            print(\"Research Result: Not yet available\")\n","\n","    except Exception as e:\n","        print(f\"Query failed: {e}\")\n","\n","\n","async def send_user_decision_signal(client: Client, workflow_id: str) -> None:\n","    loop = asyncio.get_running_loop()\n","\n","    handle = client.get_workflow_handle(workflow_id)\n","\n","    while True:\n","        print(\"\\n\" + \"=\" * 50)\n","        print(\"Research is complete!\")\n","        print(\"1. Type 'keep' to approve the research and create PDF\")\n","        print(\"2. Type 'edit' to modify the research\")\n","        print(\n","            \"3. Type 'query' to query for research result..\"\n","        )\n","        print(\"=\" * 50)\n","\n","        # When running input in async code, run in an executor to not block the event loop\n","        decision = await loop.run_in_executor(None, input, \"Your decision (keep/edit/query): \")\n","        decision = decision.strip().lower()\n","\n","        if decision in {\"keep\", \"1\"}:\n","            signal_data = UserDecisionSignal(decision=UserDecision.KEEP)\n","            await handle.signal(\"user_decision_signal\", signal_data)\n","            print(\"Signal sent to keep research and create PDF\")\n","            break\n","        if decision in {\"edit\", \"2\"}:\n","            additional_prompt_input = await loop.run_in_executor(None, input, \"Enter additional instructions for the research (optional): \")\n","            additional_prompt = additional_prompt_input.strip() if additional_prompt_input else \"\"\n","\n","            signal_data = UserDecisionSignal(decision=UserDecision.EDIT, additional_prompt=additional_prompt)\n","            await handle.signal(\"user_decision_signal\", signal_data)\n","            print(\"Signal sent to regenerate research\")\n","        elif decision in {\"query\", \"3\"}:\n","            await query_research_result(client, workflow_id)\n","\n","        else:\n","            print(\"Please enter either 'keep', 'edit', or 'query'\")\n","\n","\n","client = await Client.connect(\"localhost:7233\")\n","\n","print(\"Welcome to the Research Report Generator!\")\n","prompt = input(\"Enter your research topic or question: \").strip()\n","\n","if not prompt:\n","    prompt = \"Give me 5 fun and fascinating facts about tardigrades. Make them interesting and educational!\"\n","    print(f\"No prompt entered. Using default: {prompt}\")\n","\n","handle = await client.start_workflow(\n","    GenerateReportWorkflow.run,\n","    GenerateReportInput(prompt=prompt, llm_api_key=LLM_API_KEY),\n","    id=f\"generate-research-report-workflow-{uuid.uuid4()}\",\n","    task_queue=\"research\",\n",")\n","\n","print(f\"Started workflow. Workflow ID: {handle.id}, RunID {handle.result_run_id}\")"]},{"cell_type":"markdown","metadata":{"id":"YTY339F9EuhQ"},"source":["### Review the Workflow Execution in the Web UI\n","\n","Give the execution a few seconds to run the first research Activity, then investigate in the web UI:\n","\n","- What is happening on the timeline view after the first LLM activity executes? Why is this?\n","- Did the LLM call succeed in one attempt? Or was the API flimsy and you see some retries?\n","- Can you find the output of the first call to the `llm_call` Activity?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":704,"status":"ok","timestamp":1756905833839,"user":{"displayName":"Angela Zhou","userId":"04211316506850475003"},"user_tz":240},"id":"Zn1LkXCo7ICC","outputId":"7f37ecc0-90fc-4e62-f170-19f092bf9eff"},"outputs":[{"name":"stdout","output_type":"stream","text":["https://8000-m-s-3up4z0k302t52-c.us-west4-0.prod.colab.dev\n"]}],"source":["# Get the Temporal Web UI URL\n","from google.colab.output import eval_js\n","print(eval_js(\"google.colab.kernel.proxyPort(8000)\"))"]},{"cell_type":"markdown","metadata":{"id":"VeN1-_t51t_m"},"source":["### Sending Signals and Queries\n","\n","Now you'll send Signals and Queries to potentially change the course of the code's execution."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22087,"status":"ok","timestamp":1756905857124,"user":{"displayName":"Angela Zhou","userId":"04211316506850475003"},"user_tz":240},"id":"_x8AcBor0Wh5","outputId":"906601f7-77fe-4032-ecfe-b537da14d528"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","==================================================\n","Research is complete!\n","1. Type 'keep' to approve the research and create PDF\n","2. Type 'edit' to modify the research\n","3. Type 'query' to query for research result..\n","==================================================\n","Your decision (keep/edit/query): Give me 2 facts about pikachus\n","Please enter either 'keep', 'edit', or 'query'\n","\n","==================================================\n","Research is complete!\n","1. Type 'keep' to approve the research and create PDF\n","2. Type 'edit' to modify the research\n","3. Type 'query' to query for research result..\n","==================================================\n","Your decision (keep/edit/query): keep\n","Signal sent to keep research and create PDF\n","User approved the research. Creating PDF...\n","Result: GenerateReportOutput(result='Successfully created research report PDF: Pikachus')\n"]}],"source":["signal_task = asyncio.create_task(send_user_decision_signal(client, handle.id))\n","\n","try:\n","    result = await handle.result()\n","    signal_task.cancel()\n","    print(f\"Result: {result}\")\n","except Exception as e:\n","    signal_task.cancel()\n","    print(f\"Workflow failed: {e}\")"]},{"cell_type":"markdown","metadata":{"id":"KGMtHnUW12XS"},"source":["### Review the Workflow Execution in the Web UI\n","\n","Finish the Workflow above and then observe the results in the Web UI\n","\n","- Can you see the Signals in the Web UI? How about the Queries?\n","- Did anything else interesting happen in the execution? Any flimsy APIs or retries you weren't exepecting?\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":136,"status":"ok","timestamp":1756905857259,"user":{"displayName":"Angela Zhou","userId":"04211316506850475003"},"user_tz":240},"id":"siIOF-l815SG","outputId":"1d42967e-1e42-44b3-f740-b4ed0bc7a883"},"outputs":[{"name":"stdout","output_type":"stream","text":["https://8000-m-s-3up4z0k302t52-c.us-west4-0.prod.colab.dev\n"]}],"source":["# Get the Temporal Web UI URL\n","from google.colab.output import eval_js\n","print(eval_js(\"google.colab.kernel.proxyPort(8000)\"))"]},{"cell_type":"markdown","metadata":{"id":"nliC3O2aEzdg"},"source":["### Kill the Worker to Prepare for the Next Exercise"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1756905857263,"user":{"displayName":"Angela Zhou","userId":"04211316506850475003"},"user_tz":240},"id":"YCVtY702qKyC","outputId":"ba8ed541-718b-4f96-9496-5d6ecf6e1bf9"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["worker.cancel()"]},{"cell_type":"markdown","metadata":{"id":"2BE_tNaFU_dP"},"source":["## Part 2 - Implementing a Signal\n","\n","Next you'll implement a Signal yourself. This Signal will act as a prompt that provides the f[link text](https://)ilename to save the file as. However, if the user doesn't provide a response within twenty seconds, a default will be used instead.\n","\n","To do this, research more options in the [wait_condition](https://python.temporal.io/temporalio.workflow.html#wait_condition) method.\n","\n","1, Add a new attribute, called `filename` and default it to a string set at `research_report`.\n","2. Decorate the `filename_signal` Signal with `@workflow.signal`.\n","3. We want to wait until the filename_signal is received or 20 seconds has elapsed, whichever happens first. Set the `wait_condition` to take in a timeout parameter of 20 seconds.\n","4. In the `send_user_decision_signal` function, send a Signal to the handle of the Workflow Execution passing in `filename_signal`, and `FilenameSave(filename=filename))`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BPKrNe3JqgQa"},"outputs":[],"source":["@dataclass\n","class FilenameSave:\n","  filename: str"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-vJaE232x1h"},"outputs":[],"source":["import asyncio\n","from datetime import timedelta\n","import logging\n","\n","from temporalio import workflow\n","\n","# sandboxed=False is a Notebook only requirement. You normally don't do this\n","@workflow.defn(sandboxed=False)\n","class GenerateReportWorkflow:\n","\n","    def __init__(self) -> None:\n","        self._current_prompt: str = \"\"\n","        self._user_decision: UserDecisionSignal = UserDecisionSignal(decision=UserDecision.WAIT)\n","        self._research_result: str | None = None\n","        self._filename: str = \"research_report\"\n","\n","    @workflow.signal\n","    async def user_decision_signal(self, decision_data: UserDecisionSignal) -> None:\n","        self._user_decision = decision_data\n","\n","    @workflow.signal\n","    async def filename_signal(self, input: FilenameSave) -> None:\n","        self._filename = input.filename\n","\n","    @workflow.query\n","    def get_research_result(self) -> str | None:\n","        return self._research_result\n","\n","    @workflow.run\n","    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n","\n","        self._current_prompt = input.prompt\n","\n","        llm_call_input = LLMCallInput(\n","            prompt=self._current_prompt,\n","            llm_api_key=input.llm_api_key,\n","            llm_model=input.llm_research_model,\n","        )\n","\n","        continue_agent_loop = True\n","\n","        while continue_agent_loop:\n","            research_facts = await workflow.execute_activity(\n","                llm_call,\n","                llm_call_input,\n","                start_to_close_timeout=timedelta(seconds=30),\n","            )\n","\n","            # Store the research result for queries\n","            self._research_result = research_facts[\"choices\"][0][\"message\"][\"content\"]\n","\n","            print(\"Research complete!\")\n","\n","            print(\"Waiting for user decision. Send signal with 'keep' to create PDF or 'edit' to modify prompt.\")\n","\n","            await workflow.wait_condition(\n","                lambda: self._user_decision.decision != UserDecision.WAIT\n","            )\n","            user_decision = self._user_decision\n","\n","            if user_decision.decision == UserDecision.KEEP:\n","                print(\"User approved the research. Creating PDF...\")\n","                continue_agent_loop = False\n","            elif user_decision.decision == UserDecision.EDIT:\n","                print(\"User requested research modification.\")\n","                if user_decision.additional_prompt != \"\":\n","                    self._current_prompt = f\"{self._current_prompt}\\n\\nAdditional instructions: {user_decision.additional_prompt}\"\n","                    print(\n","                        f\"Regenerating research with updated prompt: {self._current_prompt}\"\n","                    )\n","                else:\n","                    print(\n","                        \"No additional instructions provided. Regenerating with original prompt.\"\n","                    )\n","                llm_call_input.prompt = self._current_prompt\n","                self._user_decision = UserDecisionSignal(decision=UserDecision.WAIT)\n","\n","        subject_prompt = f\"What is the main topic of this sentence? Respond with only the topic in a single word or short phrase if multiple topics are detected. This response will be used for generating an AI image. No explanation. The sentence is: {self._current_prompt}\"\n","        subject_input = LLMCallInput(prompt=subject_prompt, llm_api_key=LLM_API_KEY, llm_model=LLM_MODEL)\n","\n","        topic_call = await workflow.execute_activity(\n","            llm_call,\n","            subject_input,\n","            start_to_close_timeout=timedelta(seconds=30),\n","        )\n","\n","        topic = topic_call[\"choices\"][0][\"message\"][\"content\"]\n","\n","        # Used the new GenerateImageInput dataclass to create the input object for the Activity\n","        image_input = GenerateImageInput(topic=topic, llm_api_key=LLM_API_KEY)\n","\n","        # Called the new generate_ai_image Activity, passing in the image_input parameter made above\n","        ai_image = await workflow.execute_activity(\n","            generate_ai_image,\n","            image_input,\n","            start_to_close_timeout=timedelta(seconds=30),\n","        )\n","\n","        # Exctract the image_url form the Activity call\n","        image_url = ai_image[\"data\"][0][\"url\"]\n","\n","        try:\n","          await workflow.wait_condition(\n","              lambda: self._filename != \"research_report\",\n","              timeout=timedelta(seconds=20)\n","          )\n","        except asyncio.TimeoutError:\n","          print(\"20 seconds have passed. The program will continue and your file will automatically be named 'research_paper.pdf'.\")\n","\n","        # Add the image_url parameter to the PDF Generation so the image is included\n","        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"], image_url=image_url, filename=self._filename)\n","\n","        pdf_filename = await workflow.execute_activity(\n","            create_pdf_activity,\n","            pdf_generation_input,\n","            start_to_close_timeout=timedelta(seconds=10),\n","        )\n","\n","        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}.pdf\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nNZZ71MHBwqp"},"outputs":[],"source":["from temporalio.client import Client\n","from temporalio.worker import Worker\n","import concurrent.futures\n","\n","async def run_worker() -> None:\n","    logging.basicConfig(level=logging.INFO)\n","    logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n","\n","    client = await Client.connect(\"localhost:7233\", namespace=\"default\")\n","\n","    # Run the Worker\n","    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as activity_executor:\n","        worker = Worker(\n","            client,\n","            task_queue=\"research\",\n","            workflows=[GenerateReportWorkflow],\n","            activities=[llm_call, create_pdf_activity, generate_ai_image],\n","            activity_executor=activity_executor\n","        )\n","\n","        print(f\"Starting the worker....\")\n","        await worker.run()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SUpctA0H56gl"},"outputs":[],"source":["# Start a new worker\n","\n","# If you didn't kill the previous worker, uncomment this and run it first\n","# worker.cancel()\n","\n","worker = asyncio.create_task(run_worker())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1776,"status":"ok","timestamp":1756905891012,"user":{"displayName":"Angela Zhou","userId":"04211316506850475003"},"user_tz":240},"id":"stv5t7sL4JH2","outputId":"6a84d524-e502-4c91-a74d-3bed8ac9b2e4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Welcome to the Research Report Generator!\n","Enter your research topic or question: Give me 2 facts about pikachus\n","Started workflow. Workflow ID: generate-research-report-workflow, RunID 01990fc0-648f-7569-9412-5deeed80c57a\n"]}],"source":["import datetime\n","import uuid\n","\n","async def query_research_result(client: Client, workflow_id: str) -> None:\n","    handle = client.get_workflow_handle(workflow_id)\n","\n","    try:\n","        research_result = await handle.query(GenerateReportWorkflow.get_research_result)\n","        if research_result:\n","            print(f\"Research Result: {research_result}\")\n","        else:\n","            print(\"Research Result: Not yet available\")\n","\n","    except Exception as e:\n","        print(f\"Query failed: {e}\")\n","\n","\n","async def send_user_decision_signal(client: Client, workflow_id: str) -> None:\n","    handle = client.get_workflow_handle(workflow_id)\n","    loop = asyncio.get_running_loop()\n","\n","    while True:\n","        print(\"\\n\" + \"=\" * 50)\n","        print(\"Research is complete!\")\n","        print(\"1. Type 'keep' to approve the research and create PDF\")\n","        print(\"2. Type 'edit' to modify the research\")\n","        print(\n","            \"3. Type 'query' to query for research result.\"\n","        )\n","        print(\"=\" * 50)\n","\n","        decision = await loop.run_in_executor(None, input, \"Your decision (keep/edit/query): \")\n","        decision = decision.strip().lower()\n","\n","        if decision in {\"keep\", \"1\"}:\n","            signal_data = UserDecisionSignal(decision=UserDecision.KEEP)\n","            await handle.signal(\"user_decision_signal\", signal_data)\n","            print(\"Signal sent to keep research and create PDF\")\n","            break\n","        if decision in {\"edit\", \"2\"}:\n","            additional_prompt_input = await loop.run_in_executor(None, input, \"Enter additional instructions for the research (optional): \")\n","            additional_prompt = additional_prompt_input.strip() if additional_prompt_input else \"\"\n","\n","            signal_data = UserDecisionSignal(decision=UserDecision.EDIT, additional_prompt=additional_prompt)\n","            await handle.signal(\"user_decision_signal\", signal_data)\n","            print(\"Signal sent to regenerate research\")\n","        elif decision in {\"query\", \"3\"}:\n","            await query_research_result(client, workflow_id)\n","\n","        else:\n","            print(\"Please enter either 'keep', 'edit', or 'query'\")\n","\n","    start_time = datetime.datetime.now()\n","    print(\"What do you want to name the file? After 20 seconds, the program will continue and your file will automatically be named 'research_paper.pdf'.\")\n","    filename = await loop.run_in_executor(None, input, \"Enter the filename: \")\n","    await handle.signal(\"filename_signal\", FilenameSave(filename=filename))\n","    end_time = datetime.datetime.now()\n","    if end_time - start_time > datetime.timedelta(seconds=20):\n","        print(\"20 seconds have passed. The program will continue and your file will automatically be named 'research_paper.pdf'.\")\n","    else:\n","        print(f\"Your file will be saved as {filename}.pdf\")\n","\n","\n","client = await Client.connect(\"localhost:7233\")\n","\n","print(\"Welcome to the Research Report Generator!\")\n","prompt = input(\"Enter your research topic or question: \").strip()\n","\n","if not prompt:\n","    prompt = \"Give me 5 fun and fascinating facts about tardigrades. Make them interesting and educational!\"\n","    print(f\"No prompt entered. Using default: {prompt}\")\n","\n","handle = await client.start_workflow(\n","    GenerateReportWorkflow.run,\n","    GenerateReportInput(prompt=prompt, llm_api_key=LLM_API_KEY),\n","    id=f\"generate-research-report-workflow-{uuid.uuid4()}\",\n","    task_queue=\"research\",\n",")\n","\n","print(f\"Started workflow. Workflow ID: {handle.id}, RunID {handle.result_run_id}\")"]},{"cell_type":"markdown","metadata":{"id":"MaU2t6GERaN3"},"source":["### Test Your Signal\n","\n","Run the code, and provide a filename for it to be saved as within the 20 seconds timeframe."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18877,"status":"ok","timestamp":1756905912307,"user":{"displayName":"Angela Zhou","userId":"04211316506850475003"},"user_tz":240},"id":"GXJjTpMo5mHe","outputId":"a4e272ce-3776-4196-9482-6562c60deda3"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","==================================================\n","Research is complete!\n","1. Type 'keep' to approve the research and create PDF\n","2. Type 'edit' to modify the research\n","3. Type 'query' to query for research result.\n","==================================================\n","Your decision (keep/edit/query): Give me 2 facts about pikachus\n","Please enter either 'keep', 'edit', or 'query'\n","\n","==================================================\n","Research is complete!\n","1. Type 'keep' to approve the research and create PDF\n","2. Type 'edit' to modify the research\n","3. Type 'query' to query for research result.\n","==================================================\n","Research complete!\n","Waiting for user decision. Send signal with 'keep' to create PDF or 'edit' to modify prompt.\n","Your decision (keep/edit/query): keep\n","Signal sent to keep research and create PDF\n","What do you want to name the file? After 20 seconds, the program will continue and your file will automatically be named 'research_paper.pdf'.\n","User approved the research. Creating PDF...\n","Enter the filename: adw\n","Your file will be saved as adw.pdf\n","Result: GenerateReportOutput(result='Successfully created research report PDF: adw.pdf')\n"]}],"source":["signal_task = asyncio.create_task(send_user_decision_signal(client, handle.id))\n","\n","try:\n","    result = await handle.result()\n","    signal_task.cancel()\n","    print(f\"Result: {result}\")\n","except Exception as e:\n","    signal_task.cancel()\n","    print(f\"Workflow failed: {e}\")"]},{"cell_type":"markdown","metadata":{"id":"s4iN3KNdRkPG"},"source":["### Test Your Signal - Timeout Path\n","\n","Scroll up and run the Workflow again, but this time, wait the twenty seconds for the Signal time to elapse. Once time has elapsed you will see the file named `research_report.pdf` get generated.\n","\n","Then enter a filename in the box (which should do nothing) to finish the cell execution."]},{"cell_type":"markdown","metadata":{"id":"zaIYuART6KKY"},"source":["### Watch the Execuction in the Web UI\n","\n","Open the Web UI and compare the executions.\n","\n","- What was different in the Event History/Timeline view between the two?\n","- How did the Activity know what to name the file? Can you find how this data is relayed in the Event History?\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":146,"status":"ok","timestamp":1756905912454,"user":{"displayName":"Angela Zhou","userId":"04211316506850475003"},"user_tz":240},"id":"OLg4s8yX6JPX","outputId":"adcb3a78-7842-4a1b-ce27-1f10dacc1e7b"},"outputs":[{"name":"stdout","output_type":"stream","text":["https://8000-m-s-3up4z0k302t52-c.us-west4-0.prod.colab.dev\n"]}],"source":["# Get the Temporal Web UI URL\n","from google.colab.output import eval_js\n","print(eval_js(\"google.colab.kernel.proxyPort(8000)\"))"]},{"cell_type":"markdown","metadata":{"id":"Hqb7u6GPKhAC"},"source":["## Adding Queries\n","\n","We'll now add queries! You've already seen an example of looking at the research content in the last notebook. Let's now add in a queryto see the character count of your generated research.\n","\n","1. We'll first add in a new attribute, called `character_count` that is set to an integer. It should default to 0.\n","2. Handle a `get_research_stats` Query by anotating it with `@workflow.query`. Have it return its attribute `character_count`.\n","3. After the line where your Workflow sets the `research_result` variable, set your `character_count` attribute to `len(research_facts[\"choices\"][0][\"message\"][\"content\"])`.\n","4. In the `query_research_stats` function, in the `query` function, pass in `GenerateReportWorkflow.get_research_stats`.\n","5. In the `print(f\"Research character count: {}\")` statement, print out your `stats` variable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OiBHlQdEKkV9"},"outputs":[],"source":["import asyncio\n","from datetime import timedelta\n","import logging\n","\n","from temporalio import workflow\n","\n","@workflow.defn(sandboxed=False)\n","class GenerateReportWorkflow:\n","\n","    def __init__(self) -> None:\n","        self._current_prompt: str = \"\"\n","        self._user_decision: UserDecisionSignal = UserDecisionSignal(decision=UserDecision.WAIT)\n","        self._research_result: str = \"\"\n","        self._character_count: int = 0\n","\n","    @workflow.signal\n","    async def user_decision_signal(self, decision_data: UserDecisionSignal) -> None:\n","        self._user_decision = decision_data\n","\n","    @workflow.query\n","    def get_research_result(self) -> str | None:\n","        return self._research_result\n","\n","    @workflow.query\n","    def get_research_stats(self) -> int:\n","        return self._character_count\n","\n","    @workflow.run\n","    async def run(self, input: GenerateReportInput) -> GenerateReportOutput:\n","        self._current_prompt = input.prompt\n","\n","        llm_call_input = LLMCallInput(\n","            prompt=self._current_prompt,\n","            llm_api_key=input.llm_api_key,\n","            llm_model=input.llm_research_model,\n","        )\n","\n","        continue_agent_loop = True\n","\n","        while continue_agent_loop:\n","            research_facts = await workflow.execute_activity(\n","                llm_call,\n","                llm_call_input,\n","                start_to_close_timeout=timedelta(seconds=30),\n","            )\n","\n","            self._research_result = research_facts[\"choices\"][0][\"message\"][\"content\"]\n","            self._character_count = len(research_facts[\"choices\"][0][\"message\"][\"content\"])\n","\n","            await workflow.wait_condition(lambda: self._user_decision.decision != UserDecision.WAIT)\n","\n","            if self._user_decision.decision == UserDecision.KEEP:\n","                print(\"User approved the research. Creating PDF...\")\n","                continue_agent_loop = False\n","            elif self._user_decision.decision == UserDecision.EDIT:\n","                print(\"User requested research modification.\")\n","                if self._user_decision.additional_prompt != \"\":\n","                    self._current_prompt = (\n","                        f\"{self._current_prompt}\\n\\nAdditional instructions: {self._user_decision.additional_prompt}\"\n","                    )\n","                    print(f\"Regenerating research with updated prompt: {self._current_prompt}\")\n","                else:\n","                    print(\"No additional instructions provided. Regenerating with original prompt.\")\n","                llm_call_input.prompt = self._current_prompt\n","                self._user_decision = UserDecisionSignal(decision=UserDecision.WAIT)\n","\n","        pdf_generation_input = PDFGenerationInput(content=research_facts[\"choices\"][0][\"message\"][\"content\"])\n","\n","        pdf_filename: str = await workflow.execute_activity(\n","            create_pdf_activity,\n","            pdf_generation_input,\n","            start_to_close_timeout=timedelta(seconds=20),\n","        )\n","\n","        return GenerateReportOutput(result=f\"Successfully created research report PDF: {pdf_filename}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ednmmCmzmPDZ"},"outputs":[],"source":["# Kill any previous workers that may still be running\n","x = worker.cancel()\n","\n","# Start a new worker\n","worker = asyncio.create_task(run_worker())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6116,"status":"ok","timestamp":1756906633025,"user":{"displayName":"Angela Zhou","userId":"04211316506850475003"},"user_tz":240},"id":"rRKGBkkqmp63","outputId":"60fa90cd-ad17-4a0e-8d5a-d318ca355afb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Welcome to the Research Report Generator!\n","Enter your research topic or question: Give me 2 facts about geese\n","Started workflow. Workflow ID: generate-research-report-workflow, RunID 01990fcb-b708-7435-8729-fd162d7aec87\n"]}],"source":["import uuid\n","\n","async def query_research_stats(client: Client, workflow_id: str) -> None:\n","    handle = client.get_workflow_handle(workflow_id)\n","    try:\n","        stats = await handle.query(GenerateReportWorkflow.get_research_stats)\n","\n","        print(\"\\n\" + \"=\" * 50)\n","        print(f\"Research character count: {stats}\")\n","        print(\"=\" * 50)\n","\n","    except Exception as e:\n","        print(f\"Research stats query failed: {e}\")\n","\n","\n","async def query_research_result(client: Client, workflow_id: str) -> None:\n","    handle = client.get_workflow_handle(workflow_id)\n","    try:\n","        result = await handle.query(GenerateReportWorkflow.get_research_result)\n","\n","        if result:\n","            print(\"\\n\" + \"=\" * 50)\n","            print(\"Current Research Result:\")\n","            print(\"=\" * 50)\n","            print(result)\n","            print(\"=\" * 50)\n","        else:\n","            print(\"Research not available yet.\")\n","\n","    except Exception as e:\n","        print(f\"Research result query failed: {e}\")\n","\n","\n","async def send_user_decision_signal(client: Client, workflow_id: str) -> None:\n","    loop = asyncio.get_running_loop()\n","\n","    handle = client.get_workflow_handle(workflow_id)\n","\n","    while True:\n","        print(\"\\n\" + \"=\" * 50)\n","        print(\"Research is complete!\")\n","        print(\"1. Type 'keep' to approve the research and create PDF\")\n","        print(\"2. Type 'edit' to modify the research\")\n","        print(\"3. Type 'query' to query for research result.\")\n","        print(\"4. Type 'stats' to view research statistics\")\n","        print(\"=\" * 50)\n","\n","        # When running input in async code, run in an executor to not block the event loop\n","        decision = await loop.run_in_executor(None, input, \"Your decision (keep/edit/query/stats): \")\n","        decision = decision.strip().lower()\n","\n","        if decision in {\"keep\", \"1\"}:\n","            signal_data = UserDecisionSignal(decision=UserDecision.KEEP)\n","            await handle.signal(\"user_decision_signal\", signal_data)\n","            print(\"Signal sent to keep research and create PDF\")\n","            break\n","        elif decision in {\"edit\", \"2\"}:\n","            additional_prompt_input = await loop.run_in_executor(None, input, \"Enter additional instructions for the research (optional): \")\n","            additional_prompt = additional_prompt_input.strip() if additional_prompt_input else \"\"\n","            signal_data = UserDecisionSignal(decision=UserDecision.EDIT, additional_prompt=additional_prompt)\n","            await handle.signal(\"user_decision_signal\", signal_data)\n","            print(\"Signal sent to regenerate research\")\n","        elif decision in {\"query\", \"3\"}:\n","            await query_research_result(client, workflow_id)\n","        elif decision in {\"stats\", \"4\"}:\n","            await query_research_stats(client, workflow_id)\n","        else:\n","            print(\"Please enter 'keep', 'edit', 'query', or 'stats'\")\n","\n","client = await Client.connect(\"localhost:7233\")\n","\n","print(\"Welcome to the Research Report Generator!\")\n","prompt = input(\"Enter your research topic or question: \").strip()\n","\n","if not prompt:\n","    prompt = \"Give me 5 fun and fascinating facts about tardigrades. Make them interesting and educational!\"\n","    print(f\"No prompt entered. Using default: {prompt}\")\n","\n","handle = await client.start_workflow(\n","    GenerateReportWorkflow.run,\n","    GenerateReportInput(prompt=prompt, llm_api_key=LLM_API_KEY),\n","    id=f\"generate-research-report-workflow-{uuid.uuid4()}\",\n","    task_queue=\"research\",\n",")\n","\n","print(f\"Started workflow. Workflow ID: {handle.id}, RunID {handle.result_run_id}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"ExpoxUE3n1te"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","==================================================\n","Research is complete!\n","1. Type 'keep' to approve the research and create PDF\n","2. Type 'edit' to modify the research\n","3. Type 'query' to query for research result.\n","4. Type 'stats' to view research statistics\n","==================================================\n","\n","==================================================\n","Current Research Result:\n","==================================================\n","Certainly! Here are two interesting facts about geese:\n","\n","1. **Migration Patterns**: Many species of geese are known for their long migration patterns. They often travel thousands of miles between their breeding and wintering grounds. The V-formation, or wedge-shaped flight pattern, they use during migration helps conserve energy, as the air resistance is reduced for the birds flying behind the lead bird. This allows geese to fly long distances with greater efficiency.\n","\n","2. **Strong Family Bonds**: Geese are known for their strong family bonds and social structures. Goose families typically stay together during migration and throughout the winter. The young, known as goslings, are cared for by both parents, who are vigilant in protecting them from predators. These strong familial relationships can sometimes last even beyond the goslings' first year, as some young geese remain with their parents to help with the next season's brood.\n","==================================================\n","\n","==================================================\n","Research is complete!\n","1. Type 'keep' to approve the research and create PDF\n","2. Type 'edit' to modify the research\n","3. Type 'query' to query for research result.\n","4. Type 'stats' to view research statistics\n","==================================================\n","\n","==================================================\n","Research character count: 942\n","==================================================\n","\n","==================================================\n","Research is complete!\n","1. Type 'keep' to approve the research and create PDF\n","2. Type 'edit' to modify the research\n","3. Type 'query' to query for research result.\n","4. Type 'stats' to view research statistics\n","==================================================\n"]}],"source":["signal_task = asyncio.create_task(send_user_decision_signal(client, handle.id))\n","\n","try:\n","    result = await handle.result()\n","    signal_task.cancel()\n","    print(f\"Result: {result}\")\n","except Exception as e:\n","    signal_task.cancel()\n","    print(f\"Workflow failed: {e}\")"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
